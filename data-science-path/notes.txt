“data drives all our decisions”
Some have estimated that data scientists spend 80% of their time cleaning and manipulating data, and only 20% of their time actually analyzing it or building models from it.

Інструменти аналізу даних і ML:
    - середовище розробки Jupyter Notebook (Google Colab, PyCharm)
        Google має ресурс, щоб створювати, запускати та ділитись Jupyter Notebooks colab.research.google.com;
    - інструменти аналізу даних Pandas, NumPy та SkiPy
    - інструменти ML, як-от SciKit-Learn і TensorFlow.
        - популярні фреймворки для Deep learning: TensorFlow, PyTorch та Keras.
        - робота з текстами та NLP, — рекурентні нейронні мережі (LSTM, GRU, attention models тощо)
        - сфера на выбор: NLP, Computer Vision, Reinforcement Learning, AutoML
    - інструменти візуалізації - Matplotlib, Tableau та ggplot
    - мови програмування - Python 3, R
        В окремих випадках - C++, якщо робота пов’язана з комп’ютерним зором, або Scala, якщо треба працювати з Big Data. Також йдеться про Matlab і Java.
    - Databases (SQL, NoSQL, GraphDBs)
    - Cloud Technologies (AWS, Azure, Google Cloud)
    - Серед обов’язкових розділів математики: матричний аналіз, диференціальне та інтегральне числення, методи оптимізації, тензорний аналіз.
    

-----------------------------

1. The Importance of Data and SQL Basics
    > Introduction to Data Science
        Data Engineer
        Data Analyst
        Visualization Developer

        Tools in the Data Science Path:
            - Data extraction with SQL
            - Programming basics with Python
            - Data analysis using pandas, a Python library
            - Data visualization using Matplotlib, a Python library
            - Machine Learning using scikit-learn, a Python library

        There are over 125,000 third-party Python libraries https://pypi.org/. 
        These libraries make Python more useful for specific purposes, from the traditional (e.g. web development, text processing) to the cutting edge (e.g. AI and machine learning).
        For example, a biologist might use the Biopython library https://biopython.org/ to aid their work in genetic sequencing.

        Machine Learning: 
            - Supervised Learning is when the data is labeled and the program learns to predict the output from the input data.
            - Unsupervised Learning is where the data is unlabeled and the program learns to recognize the inherent structure of the input data. 

    > Introduction to SQL   (check it in web-dev-path)
        SQL syntax may differ slightly depending on which RDBMS you are using.
        Popular Relational Database Management Systems:
            - MySQL         // most popular open source. It is typically used for web application development, and often accessed using PHP. Disadvantage: poor performance when scaling.
            - PostgreSQL    // open source. It is typically used for web application development. Disadvantage: it is slower in performance.
            - Oracle DB     // Oracle Corporation owns Oracle DB, and the code is not open sourced. Oracle DB is for large applications, particularly in the banking industry. Disadvantage: not free.
            - SQL Server    // Microsoft owns SQL Server. Like Oracle DB, the code is close sourced. Microsoft offers a free entry-level version called Express but can become very expensive as you scale your application.
            - SQLite        // open source. All of the data can be stored locally without having to connect your database to a server. Popular choice for databases in cellphones, PDAs, MP3 players, set-top boxes, and other electronic gadgets. The SQL courses on Codecademy use SQLite.

************************************************************************************************
************************************************************************************************
************************************************************************************************

2. SQL: Basics  
    > Writing Queries   (check it in web-dev-path)
        Cheat sheet https://s3.amazonaws.com/codecademy-content/courses/sql-intensive/reference-guide-queries.pdf

************************************************************************************************
************************************************************************************************
************************************************************************************************

3. SQL: Intermediate
    > Aggregate Functions (check it in web-dev-path)
    > Working with Multiple Tables (check it in web-dev-path)

************************************************************************************************
************************************************************************************************
************************************************************************************************

4. Go Off-Platform with SQL (check it in web-dev-path)

************************************************************************************************
************************************************************************************************
************************************************************************************************

5. Analyze Real Data with SQL
    > Usage Funnels
        A funnel is a marketing model which illustrates the theoretical customer journey towards the purchase of a product or service. Oftentimes, we want to track how many users complete a series of steps and know which steps have the most number of users giving up.
        Generally, we want to know the total number of users in each step of the funnel, as well as the percent of users who complete each step.

        Example: https://s3.amazonaws.com/codecademy-content/courses/sql-intensive/funnels.png
        - Build a Funnel From a Single Table: Count the number of distinct user_id who answered each question_text.
            Mattresses and More users were asked to answer a five-question survey:
            - “How likely are you to recommend Mattresses and More to a friend?”
            - “Which Mattresses and More location do you shop at?”
            - “How old are you?”
            - “What is your gender?”
            - “What is your annual household income?”
            However, not every user finished the survey! We want to build a funnel to analyze if certain questions prompted users to stop working on the survey.
            SELECT question_text, COUNT(DISTINCT user_id) FROM survey_responses GROUP BY 1;
        - Survey Result
            If we (manually with a calculator or in a spreadsheet program like Microsoft Excel or Google Sheets) divide the number of people completing each step by the number of people completing the previous step: 
                Question	Percent Completed this Question
                1	        100%
                2	        95%
                3	        82%
                4	        95%
                5	        74%
            We see that Questions 2 and 4 have high completion rates, but Questions 3 and 5 have lower rates. This suggests that age and household income are more sensitive questions that people might be reluctant to answer!
        - Compare Funnels For A/B Tests
            The Product team at Mattresses and More has created a new design for the pop-ups that they believe will lead more users to complete the workflow.
            They’ve set up an A/B test where:
                50% of users view the original 'control' version of the pop-ups
                50% of users view the new 'variant' version of the pop-ups
            Eventually, we’ll want to answer the question: How is the funnel different between the two groups?
            SELECT modal_text, COUNT(DISTINCT CASE WHEN ab_group = 'control' THEN user_id END) AS 'control_clicks', COUNT(DISTINCT CASE WHEN ab_group = 'variant' THEN user_id END) AS 'variant_clicks' FROM onboarding_modals GROUP BY 1 ORDER BY 1;
        - A/B Tests Results
            After some quick math:
            Modal	Control Percent	    Variant Percent
            1	    100%	            100%
            2	    60%	                79%
            3	    80%	                85%
            4	    80%	                80%
            5	    85%	                85%
            During Modal 2, variant has a 79% completion rate compared to control‘s 60%
            During Modal 3, variant has a 85% completion rate compared to control‘s 80%
            All other steps have the same level of completion
            This result tells us that the variant has greater completion!
        - Build a Funnel from Multiple Tables
            Scenario: Mattresses and More sells bedding essentials from their e-commerce store. Their purchase funnel is:
                - The user browses products and adds them to their cart
                - The user proceeds to the checkout page
                - The user enters credit card information and makes a purchase
            As a sales analyst, you want to examine data from the shopping days before Christmas. As Christmas approaches, you suspect that customers become more likely to purchase items in their cart (i.e., they move from window shopping to buying presents).
                SELECT * FROM browse LIMIT 5;
                SELECT * FROM checkout LIMIT 5;
                SELECT * FROM purchase LIMIT 5;
            First, we want to combine the information from the three tables (browse, checkout, purchase) into one table with the following schema:
                SELECT DISTINCT b.browse_date, b.user_id, c.user_id IS NOT NULL AS 'is_checkout', p.user_id IS NOT NULL AS 'is_purchase' FROM browse AS 'b' LEFT JOIN checkout 'c' ON c.user_id = b.user_id LEFT JOIN purchase 'p' ON p.user_id = c.user_id LIMIT 50;
            Let’s put the whole thing in a WITH statement so that we can continue on building our query.
                
                WITH funnels AS (
                    SELECT DISTINCT b.browse_date,
                        b.user_id,
                        c.user_id IS NOT NULL AS 'is_checkout',
                        p.user_id IS NOT NULL AS 'is_purchase'
                    FROM browse AS 'b'
                    LEFT JOIN checkout AS 'c'
                        ON c.user_id = b.user_id
                    LEFT JOIN purchase AS 'p'
                        ON p.user_id = c.user_id)
                SELECT COUNT(*) AS 'num_browse',
                    SUM(is_checkout) AS 'num_checkout',
                    SUM(is_purchase) AS 'num_purchase',
                    1.0 * SUM(is_checkout) / COUNT(user_id) AS 'browse_to_checkout',
                    1.0 * SUM(is_purchase) / SUM(is_checkout) AS 'checkout_to_purchase'
                FROM funnels
                GROUP BY browse_date
                ORDER BY browse_date;
            
            The management team suspects that conversion from checkout to purchase changes as the browse_date gets closer to Christmas Day.
            We can make a few edits to this code to calculate the funnel for each browse_date using GROUP BY.
        - Results
            Overall conversion rates:
                browse	checkout	purchase	browse_to_checkout	checkout_to_purchase
                775	    183	        163	        0.236	            0.890
            How conversion rates change as we get closer to Christmas:
                browse_date	browse	checkout	purchase	browse_to_checkout	checkout_to_purchase
                2017-12-20	100	    20	        16	        0.2	                0.8
                2017-12-21	150	    33	        28	        0.22	            0.84
                2017-12-22	250	    62	        55	        0.24	            0.88
                2017-12-23	275	    68	        64	        0.24	            0.94
            Oh wow, look at the steady increase in sales (increasing checkout_to_purchase percentage) as we inch closer to Christmas Eve!
    
    > User Churn
        Churn rate = cancellations / total subscribers
            e.g. SELECT 100. / 2000;     // 0.05
        Churn rate is the percent of subscribers that have canceled within a certain period, usually a month. For a user base to grow, the churn rate must be less than the new subscriber rate for the same period.
        A common revenue model for SaaS (Software as a service) companies is to charge a monthly subscription fee for access to their product. Frequently, these companies aim to continually increase the number of users paying for their product. 
        Example:
            SELECT 1.0 * 
            (
                SELECT COUNT(*)
                FROM subscriptions
                WHERE subscription_start < '2016-12-01'
                AND (subscription_end BETWEEN '2016-12-01' AND '2016-12-31')
            ) / (
                SELECT COUNT(*) 
                FROM subscriptions 
                WHERE subscription_start < '2016-12-01'
                AND ((subscription_end >= '2016-12-01') OR (subscription_end IS NULL))
            ) 
            AS result;

    > Marketing Attribution
        sources (sometimes called channels or touchpoints). 
        UTM parameters
            These parameters capture when and how a user finds the site. Site owners use special links containing UTM parameters in their ads, blog posts, and other sources. When a user clicks one, a row is added to a database describing their page visit.
        First-touch attribution only considers the first utm_source for each customer.
        Last-touch attribution only considers the last utm_source for each customer.


************************************************************************************************
************************************************************************************************
************************************************************************************************

6. Python Functions and Logic
    > Introduction to Python
        Syntax
            Comments                # comment
            Print                   print() function
            Strings                 "Hello" or 'Hello'
            Variables               message_string = "Hello"
            Errors                  Two common errors that we encounter while writing Python are 'SyntaxError' and 'NameError'.
            Numbers                 numeric data types: integer (int), floating-point number (float)
            Calculations            +, -, *, /  Python converts all ints to floats before performing division.
            Exponents               print(2 ** 10)      # 2 to the 10th power, or 1024
            Modulo                  remainder of a division calculation
            Concatenation           combining two strings
            Plus Equals             += operator
            Multi-line Strings      By using three quote-marks (""" or ''') instead of one, we tell the program that the string doesn’t end until the next triple-quote. 

    > Python Functions
        Example:
            def greet_customer():
                print("Welcome to Engrossing Grocers.")         # In Python, the amount of whitespace tells the computer what is part of a function and what is not part of that function. 2 spaces / 4 spaces / tab
                print("Our special is mandarin oranges.")
                print("Have fun shopping!")
            greet_customer()

        Arguments:
            - positional arguments      def greet_customer(grocery_store, special_item)
            - keyword arguments         greet_customer(special_item="chips and salsa", grocery_store="Stu's Staples")
            - default arguments         def greet_customer(special_item, grocery_store="Engrossing Grocers")     

        Returns
            When there is a result from a function that can be stored in a variable, it is called a returned function value. 
                def divide_by_four(input_number):
                    return input_number/4
        
        Remember, to concatenate a number to a string object, you’ll first have to cast numeric variable to a string using str(). Otherwise, you’ll get a TypeError.

        Multiple Return Values: 
            def square_point(x, y):
                x_2 = x * x
                y_2 = y * y
                return x_2, y_2
            x_squared, y_squared = square_point(1, 3)
            print(x_squared)
            print(y_squared)

        Scope

    > Python Control Flow
        print(type(my_var))

        if 2 == 4 - 2: 
            print("apple")
        
        def graduation_reqs(gpa, credits):
          if (gpa >= 2.0) and (credits >= 120):
            return "good"
          if (gpa >= 2.0) and not (credits >= 120):     //   elif donation >= 500: 
            return "so so"
          if not (gpa >= 2.0) and (credits >= 120):
            return "very bad"
          else:
            return "no graduation."
        
        Try and Except Statements
            def divides(a,b):
                try:
                    result = a / b
                    print (result)
                except ZeroDivisionError:
                    print ("Can't divide by zero!")

************************************************************************************************
************************************************************************************************
************************************************************************************************

7. Python Lists and Loops
    > Python Lists
        A list is an ordered set of objects in Python.
            heights = [61, 70, 67, 64]
        List of Lists
            heights = [['Jenny', 61], ['Alexus', 70], ['Sam', 67], ['Grace', 64]]
        zip
            zip takes two (or more) lists as inputs and returns an object that contains a list of pairs. Each pair contains one element from each of the inputs. 
                names = ['Jenny', 'Alexus', 'Sam', 'Grace']
                heights = [61, 70, 67, 65]
                names_and_heights = zip(names, heights)
                print(names_and_heights)    # will return the location of this object in memory, e.g. <zip object at 0x7f1631e86b48>
            list
                print(list(names_and_heights))  # convert the zip object to a list, returns: [('Jenny', 61), ('Alexus', 70), ('Sam', 67), ('Grace', 65)]
        
        To add a single element to a list:      .append()
            empty_list = []
            empty_list.append(1)        # [1]
        To add multiple items to a list:        use +, We can only use + with other lists. 
            items_sold = ['cake', 'cookie', 'bread']
            items_sold_new = items_sold + ['biscuit', 'tart']
            print(items_sold_new)       # ['cake', 'cookie', 'bread', 'biscuit', 'tart']
        
        function 'range'   - takes a single input, and generates numbers starting at 0 and ending at the number before the input.
            my_range = range(10)    # instead of my_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
            print(my_range)         # range(0, 10) - returns an object that we can convert into a list
            print(list(my_range))   # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

            If we call range with two arguments, we can create a list that starts at a different number. 
                my_list = range(2, 9)
                print(list(my_list))    # [2, 3, 4, 5, 6, 7, 8]
            If we use a third argument, we can create a list that “skips” numbers. 
                my_range2 = range(2, 9, 2)
                print(list(my_range2))  # [2, 4, 6, 8]
        
        Operations on Lists:
            - length of a list: len() 
                my_list = [1, 2, 3, 4, 5]
                print(len(my_list))     # 5
            - Selecting List Elements
                calls = ['Ali', 'Bob', 'Cam', 'Doug', 'Ellie']
                print(calls[2])     # 'Cam'

                list1 = ['a', 'b', 'c', 'd', 'e']
                print(list1[-1])    # 'e', use the index -1 to select the last item of a list, even when we don’t know how many elements are in a list.
                print(list1[4])     # 'e'
            - Slicing Lists (Select subsets of a list),     syntax: letters[start:end]
                letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g']
                sublist = letters[1:6]
                print(sublist)      # ['b', 'c', 'd', 'e', 'f']
                print(letters[:3])  # When starting at the beginning of the list, it is also valid to omit the 0:
                print(letters[3:])  # We can omit the final index when selecting the final elements from a list.
                print(letters[-3:]) # If we want to select the last 3 elements
            - Count the number of times that an element appears in a list
                letters = ['m', 'i', 's', 's', 'i', 's', 's', 'i', 'p', 'p', 'i']
                num_i = letters.count('i')
                print(num_i)    # 4
            - Sort a list of items: .sort(), sorted()
                names = ['Xander', 'Buffy', 'Angel', 'Willow', 'Giles']
                names.sort()
                print(names)    # ['Angel', 'Buffy', 'Giles', 'Willow', 'Xander']

                sorted_names = sorted(names)
                print(sorted_names) # ['Angel', 'Buffy', 'Giles', 'Willow', 'Xander']
            
            - Tuple - immutable list.
                my_info = ("Mike", 24, "Programmer")
                // unpacking a tuple
                name, age, occupation = my_info
                name        # "Mike"
                age         # 24
                occupation  # "Programmer"

                one_element_tuple = (4,)    # I have to put comma!

    > Python Loops
        - for loops             - Loops that let us move through each item in a list

            dog_breeds = ['french_bulldog', 'dalmatian', 'shihtzu', 'poodle', 'collie']
            for breed in dog_breeds:
                print(breed)

            promise = "I will not chew gum in class"
            for i in range(0,5):
                print(promise)

            infinite loop: The 'control + c' solution will work
            Break
            Continue    # e.g. Every time there was a negative number, the continue keyword moved the index to the next value in the list, without executing the code in the rest of the for loop.

            Nested Loops
                project_teams = [["Ava", "Samantha", "James"], ["Lucille", "Zed"], ["Edgar", "Gabriel"]]
                for team in project_teams:
                    for student in team:
                        print(student)

        - while loops           - Loops that keep going until we tell them to stop

            dog_breeds = ['bulldog', 'dalmation', 'shihtzu', 'poodle', 'collie']
            index = 0
            while index < len(dog_breeds):
                print(dog_breeds[index])
                index += 1

        - list comprehensions   - Loops that create new lists

            words = ["@coolguy35", "#nofilter", "@kewldawg54", "reply", "timestamp", "@matchamom", "follow", "#updog"]
            usernames = []
            for word in words:
              if word[0] == '@':
                usernames.append(word)
            print(usernames)    # ["@coolguy35", "@kewldawg54", "@matchamom"]

            Instead, use shorthand to create lists like this with one line: 
                usernames = [word for word in words if word[0] == '@']
                messages = [user + " please follow me!" for user in usernames]

************************************************************************************************
************************************************************************************************
************************************************************************************************

8. Advanced Python
    > List Comprehension in Python  
    > Lambda Functions in Python
        In Python, a lambda function is a one-line shorthand for function. 
            add_two = lambda my_input: my_input + 2
            print(add_two(3))   # 5

            // you can check if a string contains a substring by using the keyword 'in'. 
            is_substring = lambda my_string: my_string in "This is the master string"
            print(is_substring('the'))  # True

            check_if_A_grade = lambda grade: 'Got an A!' if grade >= 90 else 'Did not get an A...'

        random.randint(a,b) will return an integer between a and b (inclusive). Add 'import random'
        
        Syntax: lambda x: [OUTCOME IF TRUE] if [CONDITIONAL] else [OUTCOME IF FALSE]
        Example:
            Before:
                def myfunction(x):
                    if x > 40:
                        return 40 + (x - 40) * 1.50
                    else:
                        return x
            After: 
                myfunction = lambda x: 40 + (x - 40) * 1.50 if x > 40 else x

************************************************************************************************
************************************************************************************************
************************************************************************************************

9. Python Cumulative Project // 1_Reggie's+Linear+Regression
    Install Python environment
        The standard Python distribution is released on https://www.python.org/ and includes the Python standard library as well as the package manager pip.
        Though the list of Python distributions is always growing, two other popular distributions are Anaconda and Miniconda. 
            - Anaconda is an open source Python distribution that is purpose built for data science, machine learning, and large-scale data processing. 
              It includes the core Python language, over 1,500 data science packages, a package management system called conda, IPython (an interactive Python interpreter), and much more. 
              While it is a very comprehensive distribution, it is also quite large and therefore can take a while to download and consumes a lot of disk space.
            - Miniconda on the other hand, is a slimmed down version of Anaconda and includes all of the same components except for the pre-installed 1,500 data science packages. 
              Instead, we can simply install these packages individually as needed using conda (the Anaconda/Miniconda package manager). 
              We essentially get all the benefits of Anaconda, but without the hassle and burden of the large size. 
              Feel free to use either distribution, however Miniconda is probably the better choice for getting set up with the least amount of fuss.
        
        To install Python:
        Most modern versions of MacOS come pre-installed with Python 2, however Python 3 is now the standard and should be installed as well. 
        Python 3 can be installed using the official Python 3 installer.
        - Python Releases for Mac OS X https://www.python.org/downloads/mac-osx/ -> download the latest stable release macOS 64-bit/32-bit installer.
        - After the download is complete, run the installer and click through the setup steps leaving all the pre-selected installation defaults.
        - Open a Terminal and enter the command 'python3 --version'. The latest Python 3.7 version number should print to the Terminal.

        > python --version      // Python 2.7.16
        > python3 --version     // Python 3.7.4
        Since our system now has both Python 2 (which came pre-installed) and Python 3, we must remember to use the python3 command (instead of just python) when running scripts.
        
        To test: 
        - Create a new file called mycode.py -> paste code  print("I'm running Python code on my own environment!") -> save the file.
        - run the script 'python mycode.py'

        Install the Jupyter Notebook package.
            https://jupyter.readthedocs.io/en/latest/ 
            Jupyter Notebook is an open-source web application that allows us to create and share documents that contain live code, equations, visualizations and narrative text. 
            - USING STANDARD PYTHON
                // use 'pip3' in python3.
                - Run 'pip install jupyter' to download and install the Jupyter Notebook package.
                - Run 'jupyter notebook' from a Terminal. This will startup the Jupyter Notebook server, print out some information about the notebook server in the console, and open up a new browser tab at http://localhost:8888.
            - USING MINICONDA
                - Run 'conda install jupyter' to download and install the Jupyter Notebook package.
                - Run 'jupyter notebook'

        To install Miniconda:
        - Miniconda Download https://docs.conda.io/en/latest/miniconda.html -> download the Python 3.7 Mac OS X 64-bit .pkg installer.
        - After the download is complete, run the installer and click through the setup steps leaving all the pre-selected installation defaults.
        - Open a Terminal and enter the command 'conda list'. This will print a list of packages installed by Miniconda.

************************************************************************************************
************************************************************************************************
************************************************************************************************

10. Data Analysis with Pandas
    > Introduction to Pandas
        Pandas is a Python module for working with tabular data (i.e., data in a table with rows and columns). 
        Tabular data has a lot of the same functionality as SQL or Excel, but Pandas adds the power of Python.
        In order to get access to the Pandas module, we’ll need to install the module and then import it into a Python file.     
            import pandas as pd
        
        - Create a DataFrame // in other words, table
            A DataFrame is an object that stores data as rows and columns. 
            Each column has a name, which is a string. Each row has an index, which is an integer. DataFrames can contain many different data types: strings, ints, floats, tuples, etc.
            You can pass in a dictionary to pd.DataFrame(). Each key is a column name and each value is a list of column values. The columns must all be the same length or you will get an error. 
                df1 = pd.DataFrame({
                    'name': ['John Smith', 'Jane Doe', 'Joe Schmo'],
                    'address': ['123 Main St.', '456 Maple Ave.', '789 Broadway'],
                    'age': [34, 28, 51]
                })

                // add data using lists.
                df2 = pd.DataFrame([
                    ['John Smith', '123 Main St.', 34],
                    ['Jane Doe', '456 Maple Ave.', 28],
                    ['Joe Schmo', '789 Broadway', 51]
                    ],
                    columns=['name', 'address', 'age'])
                print(df2)

        - CVS
            We now know how to create our own DataFrame. However, most of the time, we’ll be working with datasets that already exist. 
            One of the most common formats for big datasets is the CSV.
            You can find CSVs in lots of places:
                - Online datasets (here’s an example from data.gov)
                - Export from Excel or Google Sheets
                - Export from SQL

                // file.cvs
                column1,column2,column3
                value1,value2,value3

            To load data in a CSV into a DataFrame in Pandas use .read_csv():
                df = pd.read_csv('my-csv-file.csv')
            To save data to a CSV, use .to_csv():
                df.to_csv('new-csv-file.csv')

            Inspect a DataFrame
                If it’s a small DataFrame, you can display it by typing print(df).
                If it’s a larger DataFrame, it’s helpful to be able to inspect a few items without having to look at the entire DataFrame.
                    The method .head() gives the first 5 rows of a DataFrame. If you want to see more rows, you can pass in the positional argument n.
                        e.g. df.head(10)    // would show the first 10 rows 
                    The method df.info() gives some statistics for each column.
            Select Columns:  
                my_var = df['column_name']
                print(type(df))         // <class 'pandas.core.frame.DataFrame'>
                print(type(my_var))     // <class 'pandas.core.series.Series'>

                new_df = df[['last_name', 'email']]     // To select two or more columns from a DataFrame
                print(type(new_df))     // <class 'pandas.core.frame.DataFrame'>
            Select Rows:
                march = df.iloc[2]
                orders.iloc[3:7]    // Selecting Multiple Rows

                df[df.MyColumnName == desired_column_value]     // Select Rows with Logic
                df[df.age == 30]
                df[(df.age < 30) | (df.name == 'Martha Jones')]     // In Python, | means “or” and & means “and”.
                df[df.name.isin(['Martha Jones', 'Rose Tyler', 'Amy Pond'])]    // use the isin command to check that df.name is one of a list of values
                
                df3 = df2.reset_index()     // .reset_index() to reset index of each row
                print(df3)
                df2.reset_index(inplace = True, drop = True)
                print(df2)

        Example
            import pandas as pd

            #Part 1: reading the csv
            orders = pd.read_csv('shoefly.csv')

            #Part 2: inspecting the first five lines of data
            print(orders.head(5))

            #Part 3: selecting the column 'email'
            emails = orders.email

            #Part 4: the Frances Palmer incident
            frances_palmer = orders[(orders.first_name == 'Frances') & (orders.last_name == 'Palmer')]

            #Part 5: Comfy feet means more time on the street
            comfy_shoes = orders[orders.shoe_type.isin(['clogs', 'boots', 'ballet flats'])]

        Modifying DataFrames
            - Adding a Column
                df['Quantity'] = [100, 150, 50, 35]
                df['In Stock?'] = True                  // add a new column that is the same for all rows in the DataFrame. 
                df['Sales Tax'] = df.Price * 0.075      // add a new column by performing a function on the existing columns. ('Price' is name of another column) 
            - Performing Column Operations
                from string import upper
                df['Name'] = df.Name.apply(upper)
            - Applying a Lambda to a Column
                df['Email Provider'] = df.Email.apply(lambda x: x.split('@')[-1])
            - Applying a Lambda to a Row
                df['Price with Tax'] = df.apply(lambda row:
                     row['Price'] * 1.075                   // To access particular values of the row, we use the syntax row.column_name or row[‘column_name’].
                     if row['Is taxed?'] == 'Yes'
                     else row['Price'],
                     axis=1
                )
            - Renaming Columns
                df = pd.DataFrame({
                    'name': ['John', 'Jane', 'Sue', 'Fred'],
                    'age': [23, 29, 21, 18]
                })
                df.columns = ['First Name', 'Age']
                df.rename(columns={             // to rename individual columns by using the .rename method.
                    'name': 'First Name',
                    'age': 'Age'},
                    inplace=True)               // Using inplace=True lets us edit the original DataFrame.(Using rename with only the columns keyword will create a new DataFrame, leaving your original DataFrame unchanged.)

    > Aggregates in Pandas
        An aggregate statistic is a way of creating a single number that describes a group of numbers. 
        Common aggregate statistics include mean, median, or standard deviation.
        
        - Calculating Column Statistics
            Syntax: df.column_name.command()
            e.g. print(customers.age.median())
                - mean      Average of all values in column
                - std       Standard deviation
                - median    Median
                - max       Maximum value in column
                - min       Minimum value in column
                - count     Number of values in column
                - nunique   Number of unique values in column
                - unique    List of unique values in column
        - Calculating Aggregate Functions: method .groupby
            Syntax: df.groupby('column1').column2.measurement()
            e.g. grades = df.groupby('student').grade.mean()
            print(type(grades))   // <class 'pandas.core.series.Series'>

            To transform our Series into a DataFrame:
                df.groupby('column1').column2.measurement().reset_index()
                high_earners = df.groupby('category').wage.apply(lambda x: np.percentile(x, 75)).reset_index()
                df.groupby(['Location', 'Day of Week'])['Total Sales'].mean().reset_index()     // to group by more than one column.
        - Pivot Tables
            Reorganizing a table in a specific way is called pivoting. 
                df.pivot(columns='ColumnToPivot', index='ColumnToBeRows', values='ColumnToBeValues')
            Example:
                # First use the groupby statement:
                unpivoted = df.groupby(['Location', 'Day of Week'])['Total Sales'].mean().reset_index()
                # Now pivot the table
                pivoted = unpivoted.pivot( columns='Day of Week', index='Location', values='Total Sales')
        - A/B Testing, e.g. to check how the two ads are performing on different platforms on each day of the week. 

    > Multiple Tables in Pandas
        .merge method   - looks for columns that are common between two DataFrames and then looks for rows where those column’s values are the same. It then combines the matching rows into a single row in a new table.
                        e.g. new_df = pd.merge(orders, customers)
                             new_df = orders.merge(customers)
                             big_df = orders.merge(customers).merge(products)   // chaining .merge()
                             
                             pd.merge(orders, customers.rename(columns={'id': 'customer_id'}))
                             pd.merge(orders, customers, left_on='customer_id', right_on='id')
                             pd.merge(orders, customers, left_on='customer_id', right_on='id', suffixes=['_order', '_customer'])
                         when we merge two DataFrames whose rows don’t match perfectly, we lose the unmatched rows.
        Types of merge:
            - inner merge - where we only include matching rows.
            - outer merge - would include all rows from both tables, even if they don’t match. Any missing values are filled in with None or nan
                pd.merge(company_a, company_b, how='outer')
            - Left Merge - includes all rows from the first (left) table, but only rows from the second (right) table that match the first table.
                pd.merge(company_a, company_b, how='left')
            - Right Merge - the exact opposite of left merge.
                pd.merge(company_a, company_b, how="right")
        
        Concatenate DataFrames
            To combine two DataFrames:
                pd.concat([df1, df2, df2, ...])
                
    Project: Page Visits Funnel
**********************************************************************************************
************************************************************************************************
************************************************************************************************

11. Data Visualization
    > Introduction to Matplotlib
        Matplotlib is a Python library used to create charts and graphs.
            from matplotlib import pyplot as plt
        - Basic Line Plot
            Some possible data that would be displayed with a line graph:
                - average prices of gasoline over the past decade
                - weight of an individual over the past couple of months
                - average temperature along a line of longitude over different latitudes
            
            To create a simple line graph using .plot() and display it using .show():
                x_values = [0, 1, 2, 3, 4]
                y_values = [0, 1, 4, 9, 16]
                plt.plot(x_values, y_values)
                plt.show()
            To have multiple line plots displayed on the same set of axes:
                days = [0, 1, 2, 3, 4, 5, 6]
                money_spent = [10, 12, 12, 10, 14, 22, 24]      // Your Money
                money_spent_2 = [11, 14, 15, 15, 22, 21, 12]    // Your Friend's Money:
                plt.plot(days, money_spent)                     // Matplotlib will automatically place the two lines on the same axes and give them different colors if you call plt.plot() twice. By default, the first line is always blue, and the second line is always orange. 
                plt.plot(days, money_spent_2)
                plt.show()
        - Linestyles
            To specify a different color for a line:
                plt.plot(days, money_spent, color='green')
                plt.plot(days, money_spent_2, color='#AAAAAA')
            To make a line dotted or dashed:
                plt.plot(x_values, y_values, linestyle='--')    # Dashed
                plt.plot(x_values, y_values, linestyle=':')     # Dotted
                plt.plot(x_values, y_values, linestyle='')      # No line
            To add a marker:
                plt.plot(x_values, y_values, marker='o')        # A circle
                plt.plot(x_values, y_values, marker='s')        # A square
                plt.plot(x_values, y_values, marker='*')        # A star
            To see all of the possible options: https://matplotlib.org/api/lines_api.html
        - Axis and Labels
            To zoom (use plt.axis()):
                x = [0, 1, 2, 3, 4]
                y = [0, 1, 4, 9, 16]
                plt.plot(x, y)
                plt.axis([0, 3, 2, 5])  // [min x-value, max x-value, min y-value, max y-value]
                plt.show()
        - Labeling the Axes
            plt.plot(hours, happiness)
            plt.xlabel('Time of day')
            plt.ylabel('Happiness Rating (out of 10)')
            plt.title('My Self-Reported Happiness While Awake')
            plt.show()
        - Subplots
            Subplot - each set of axes when we have multiple axes in the same picture. 
            Figure - the picture or object that contains all of the subplots.
                
                plt.subplot(2, 3, 4), arguments to be passed into it:
                    - The number of rows of subplots
                    - The number of columns of subplots
                    - The index of the subplot we want to create
            
            Example: 
                x = [1, 2, 3, 4]                        # Data sets
                y = [1, 2, 3, 4]
                plt.subplot(1, 2, 1)                    # First Subplot
                plt.plot(x, y, color='green')
                plt.title('First Subplot')
                plt.subplot(1, 2, 2)                    # Second Subplot
                plt.plot(x, y, color='steelblue')
                plt.title('Second Subplot')
                plt.show()                              # Display both subplots
            
            To customize the spacing between subplots:
                e.g. plt.subplots_adjust(bottom=0.2)
                Arguments:
                    left    - left-side margin, default=0.125. 
                    right   - right-side margin, default=0.9. 
                    bottom  - bottom margin, default=0.1.
                    top     - top margin, default=0.9.
                    wspace  - horizontal space between adjacent subplots, default=0.2.
                    hspace  - vertical space between adjacent subplots, default=0.2.
        - Legends
            e.g. plt.legend(['parabola', 'cubic'], loc=6)   // loc - location - accepts 0-10
            To label each line as we create it:
                plt.plot(x, y, label="parabola")
                plt.legend()    # Still need this command!
        - Modify Ticks
            ax.set_xticks([1, 2, 4])
            ax.set_yticks()         // to modify the y-ticks
            ax.set_yticklabels(['10%', '60%', '80%'])
        - Figures
            plt.close('all')                // to clear all existing plots before you plot a new one.
            # Figure 2
            plt.figure(figsize=(4, 10))     // figsize=(width, height)
            plt.plot(x, parabola)
            plt.savefig('tall_and_narrow.png')  //  to save out to many different file formats, such as png, svg, or pdf. 

        - DIFFERENT PLOT TYPES
            - Simple Bar Chart, plt.bar()
                days_in_year = [88, 225, 365, 687, 4333, 10756, 30687, 60190, 90553]
                plt.bar(range(len(days_in_year)), days_in_year)
                plt.show()

                To customize the tick marks on the x-axis:
                    - Create an axes object: 
                        ax = plt.subplot()
                    - Set the x-tick positions using a list of numbers: 
                        ax.set_xticks([0, 1, 2, 3, 4, 5, 6, 7, 8])
                    - Set the x-tick labels using a list of strings:
                        // we can label the x-axis (plt.xlabel) and y-axis (plt.ylabel) as well.
                        ax.set_xticklabels(['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune', 'Pluto'])
                    - If your labels are particularly long, you can use the rotation keyword to rotate your labels by a specified number of degrees:
                        ax.set_xticklabels(['Mercury', 'Venus', 'Earth', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune', 'Pluto'], rotation=30)
            - Side-By-Side Bars
                Some examples of data that side-by-side bars could be useful for include:
                    - the populations of two countries over time
                    - prices for different foods at two different restaurants
                    - enrollments in different classes for males and females
                Example:
                    # China Data (blue bars)
                        n = 1  # This is our first dataset (out of 2)
                        t = 2 # Number of datasets
                        d = 7 # Number of sets of bars
                        w = 0.8 # Width of each bar
                        x_values1 = [t*element + w*n for element in range(d)]
                    # US Data (orange bars)
                        n = 2  # This is our second dataset (out of 2)
                        t = 2 # Number of datasets
                        d = 7 # Number of sets of bars
                        w = 0.8 # Width of each bar
                        x_values2 = [t*element + w*n for element in range(d)]
            - Stacked Bars
                If we want to compare two sets of data while preserving knowledge of the total between them.
                Example:
                    video_game_hours = [1, 2, 2, 1, 2]
                    plt.bar(range(len(video_game_hours)), video_game_hours) 
                    book_hours = [2, 3, 4, 2, 1]
                    plt.bar(range(len(book_hours)), book_hours, bottom=video_game_hours)
            - Error Bars
                We often use error bars to show where each bar could be, taking errors into account.
                Each of the black lines is called an 'error bar'. 
                The horizontal lines at the top and bottom are called 'caps'. They make it easier to read the error bars.
                If we wanted to show an error of +/- 2, we would add the keyword yerr=2 to our plt.bar command. To make the caps wide and easy to read, we would add the keyword capsize=10.
                Example:
                    values = [10, 13, 11, 15, 20]
                    yerr = 2    // or yerr = [1, 3, 0.5, 2, 4] - If we want a different amount of error for each bar, we can make yerr equal to a list rather than a single number.
                    plt.bar(range(len(values)), values, yerr=yerr, capsize=10)
                    plt.show()
                
                Fill Between
                    In Matplotlib, we can use plt.fill_between to shade error, using plt.fill_between().
                    Having to calculate y_lower and y_upper by hand is time-consuming. In order to correctly add or subtract from a list, we need to use list comprehension:
                        y_lower = [i - 2 for i in y_values]
                    Example:
                        x_values = range(10)
                        y_values = [10, 12, 13, 13, 15, 19, 20, 22, 23, 29]
                        y_lower = [8, 10, 11, 11, 13, 17, 18, 20, 21, 27]
                        y_upper = [12, 14, 15, 15, 17, 21, 22, 24, 25, 31]
                        plt.fill_between(x_values, y_lower, y_upper, alpha=0.2) #this is the shaded error
                        plt.plot(x_values, y_values) #this is the line itself
                        plt.show()
            - Pie Chart
                Pie charts are helpful for displaying data like:
                    - Different ethnicities that make up a school district
                    - Different macronutrients (carbohydrates, fat, protein) that make up a meal
                    - Different responses to an online poll
                Example:
                    budget_data = [500, 1000, 750, 300, 100]
                    budget_categories = ['marketing', 'payroll', 'engineering', 'design', 'misc']
                    plt.pie(budget_data)
                    plt.legend(budget_categories)                                       # option 1. Pie Chart Labeling
                    plt.pie(budget_data, labels=budget_categories, autopct='%0.1f%%')   # option 2. Plus, adding the percentage of the total that each slice occupies.
                    // plt.axis('equal')    # to set the axes to be equal
                    plt.show()
            - Histogram
                To make a histogram in Matplotlib, we use the command plt.hist. plt.hist finds the minimum and the maximum values in your dataset and creates 10 equally-spaced bins between those values.
                Example:
                    plt.hist(dataset, range=(66,69), bins=40) 
                    plt.show()
                
                Multiple Histograms
                    Normalizing is dividing the height of each column by a constant such that the area under the curve sums to 1.
                    Example: 
                        plt.hist(a, range=(55, 75), bins=20, alpha=0.5, normed=True)    # normed is optional
                        plt.hist(b, range=(55, 75), bins=20, alpha=0.5, normed=True)
                        // or
                        plt.hist(a, range=(55, 75), bins=20, histtype='step')
                        plt.hist(b, range=(55, 75), bins=20, histtype='step')
                        // or
                        a = normal(loc=64, scale=2, size=10000)
                        b = normal(loc=70, scale=2, size=100000)
                        plt.hist(a, range=(55, 75), bins=20)
                        plt.hist(b, range=(55, 75), bins=20)
                        plt.show()
        
        - How to Select a Meaningful Visualization    
            The three steps in the data visualization process are preparing, visualizing, and styling data. 
            Chart categories:
                - Composition charts: 
                    - changing over time,   e.g. stacked bars
                    - static,               e.g. pie
                    Focusing Question: What are the parts of some whole? What is the data made of?
                    Datasets that work well: Data pertaining to probabilities, proportions, and percentages can be visualized as with the graphs in this composition category. Charts in this category illustrate the different data components and their percentages as part of a whole.
                - Distribution Charts:
                    - one variable,         e.g. histogram
                    - two variables,        e.g. multiple histogram, Side-By-Side Bars
                    - three or more variables
                    Datasets that work well: Data in large quantities and/or with an array of attributes works well for these types of charts. Visualizations in this category will allow you to see patterns, re-occurrences, and a clustering of data points.
                - Relationship Charts:
                    - two variables
                    - three variables
                    Focusing Question: How do variables relate to each other?
                    Datasets that work well: Data with two or more variables can be displayed in these charts. These charts typically illustrate a correlation between two or more variables.
                - Comparison Charts
                    - among items,          e.g. line, simple bar charts
                    - over time,            e.g. line, error bars
                    Focusing Question: How do variables compare to each other?
                    Datasets that work well: Data must have multiple variables, and the visualizations in this category allow readers to compare those items against the others.
        
        - RECREATE GRAPHS USING MATPLOTLIB!

    > Graphing Cumulative Project: Orion Constellation
        Matplotlib Cheat Sheet - https://s3.amazonaws.com/codecademy-content/courses/matplotlib/data_vis_matplotlib_cheatsheet_v1_revisons.pdf
        Matplotlib Tutorials - https://matplotlib.org/tutorials/index.html

        1. Set-Up
            %matplotlib notebook                    // to rotate visualization in jupyter notebook.
            from matplotlib import pyplot as plt
            from mpl_toolkits.mplot3d import Axes3D // to see 3D visualization
        2. Get familiar with real data
            x = [-0.41, 0.57, 0.07, 0.00, -0.29, -0.32,-0.50,-0.23, -0.23]
            y = [4.12, 7.71, 2.36, 9.10, 13.35, 8.13, 7.19, 13.25,13.43]
            z = [2.06, 0.84, 1.56, 2.07, 2.36, 1.72, 0.66, 1.25,1.38]
        3. Create a 2D Visualization
            fig = plt.figure()
            plt.subplot(1,1,1)      # fig.add_subplot(1,1,1)
            plt.scatter(x,y)
            plt.show()
        4. Create a 3D Visualization
            fig_3d = plt.figure()
            fig_3d.add_subplot(1,1,1,projection='3d')
            constellation3d = plt.scatter(x,y,z)
            plt.show()
        5. Rotate and explore

    > Introduction to Seaborn
        Seaborn is a Python data visualization library that provides simple code to create elegant visualizations for statistical exploration and insight. Seaborn is based on Matplotlib, but improves on Matplotlib in several ways:
            - Seaborn provides a more visually appealing plotting style and concise syntax.
            - ! Seaborn natively understands Pandas DataFrames, making it easier to plot data directly from CSVs.
            - Seaborn can easily summarize Pandas DataFrames with many rows of data into aggregated charts.

            import seaborn as sns
            df = pd.read_csv('survey.csv')
            sns.barplot(x='Age Range', y='Response', hue='Gender', data=df)
            plt.show()

            The Seaborn function sns.barplot(), takes at least three keyword arguments:
                - data: a Pandas DataFrame that contains the data (in this example, data=df)
                - x: a string that tells Seaborn which column in the DataFrame contains otheur x-labels (in this case, x="Gender")
                - y: a string that tells Seaborn which column in the DataFrame contains the heights we want to plot for each bar (in this case y="Mean Satisfaction")

        Aggregates - compute aggregates using Numpy.    
            import numpy as np
            gradebook = pd.read_csv("gradebook.csv")    // print(gradebook)
            assignment1 = gradebook[gradebook.assignment_name == "Assignment 1"]    // print(assignment1)
            asn1_median = np.median(assignment1.grade)  // print(asn1_median)

        Plotting Aggregates
            sns.barplot(data=df, x="student", y="grade")
        Modifying Error Bars
            sns.barplot(data=gradebook, x="name", y="grade", ci="sd")
        Calculating Different Aggregates
            sns.barplot(data=df, x="x-values", y="y-values", estimator=np.median)
            sns.barplot(data=df, x="Patient ID", y="Response", estimator=len)
        Aggregating by Multiple Columns
            sns.barplot(data=df, x="Gender", y="Response", hue="Age Range")
        
        SEABORN: DISTRIBUTIONS
            Distributions provide us with more information about our data — how spread out it is, its range, etc.
            Plots used to visualize distributions:
                - KDE Plots (instead of histogram) - Kernel density estimator; shows a smoothed version of dataset. Use sns.kdeplot().
                - Box plot - A classic statistical model that shows the median, interquartile range, and outliers. Use sns.boxplot().
                    (also known as a box-and-whisker plot) can’t tell us about how our dataset is distributed, like a KDE plot. But it shows us the range of our dataset, gives us an idea about where a significant portion of our data lies, and whether or not any outliers are present.
                    Interpretation of a box plot:
                        - The 'box' represents the interquartile range
                        - The 'line in the middle' of the box is the median
                        - The 'end lines' are the first and third quartiles
                        - The 'diamonds' show outliers
                - Violin plots - A combination of a KDE and a box plot. Good for showing multiple distributions at a time. Use sns.violinplot().
                    Provide more information than box plots because instead of mapping each individual data point, we get an estimation of the dataset thanks to the KDE.
                    Violin plots are less familiar and trickier to read, so let’s break down the different parts:
                        - There are two KDE plots that are symmetrical along the center line.
                        - A white dot represents the median.
                        - The thick black line in the center of each violin represents the interquartile range.
                        - The lines that extend from the center are the confidence intervals - just as we saw on the bar plots, a violin plot also displays the 95% confidence interval.

            Example:
                // read in three datasets. In order to plot them in Seaborn, we'll combine them using NumPy's .concatenate() function into a Pandas DataFrame.
                n = 500
                dataset1 = np.genfromtxt("dataset1.csv", delimiter=",")
                dataset2 = np.genfromtxt("dataset2.csv", delimiter=",")
                dataset3 = np.genfromtxt("dataset3.csv", delimiter=",")
                df = pd.DataFrame({
                    "label": ["set_one"] * n + ["set_two"] * n + ["set_three"] * n,
                    "value": np.concatenate([dataset1, dataset2, dataset3])
                })
                sns.set()
                // plot each dataset as bar charts.
                sns.barplot(data=df, x='label', y='value')
                plt.show()
                // use a KDE plot - to find out more about the distribution
                sns.kdeplot(dataset1, shade=True, label="dataset1")
                sns.kdeplot(dataset2, shade=True, label="dataset2")
                sns.kdeplot(dataset3, shade=True, label="dataset3")
                plt.legend()
                plt.show()
                // Box Plots - to make it easier for us to compare distributions
                sns.boxplot(data=df, x='label', y='value')
                plt.show()
                // Violin Plots - to determine the shape of the data
                sns.violinplot(data=df, x="label", y="value")
                plt.show()

        Seaborn Styling, Part 1: Figure Style and Scale
            Seaborn has five built-in themes to style its plots: 'darkgrid', 'whitegrid', 'dark', 'white', and 'ticks'. 
            Seaborn defaults to using the darkgrid theme for its plots, but you can change this styling to better suit your presentation needs.
                sns.set_style("darkgrid")
                sns.stripplot(x="day", y="total_bill", data=tips)
            Despine
                In addition to changing the color background, you can also define the usage of spines. Spines are the borders of the figure that contain the visualization. By default, an image has four spines.
                    sns.set_style("white")
                    sns.stripplot(x="day", y="total_bill", data=tips)
                    sns.despine()
                    // or 
                    sns.despine(left=True, bottom=True)
            Scaling Plots
                Seaborn has four presets which set the size of the plot and allow you to customize your figure depending on how it will be presented.
                In order of relative size they are: 'paper', 'notebook', 'talk', and 'poster'. 
                    sns.set_style("ticks")
                    # Smallest context:
                    sns.set_context("paper")
                    sns.stripplot(x="day", y="total_bill", data=tips)
            Scaling Fonts and Line Widths
                You are also able to change the size of the text using the font_scale parameter for sns.set_context()
                    sns.set_context("poster", font_scale = .5, rc={"grid.linewidth": 0.6})
                    sns.stripplot(x="day", y="total_bill", data=tips)
        
        Seaborn Styling, Part 2: Color 
            doc https://seaborn.pydata.org/tutorial/color_palettes.html?highlight=color
            How to set a palette
                palette = sns.color_palette("bright")   # Save a palette to a variable
                sns.palplot(palette)                    # Use palplot and pass in the variable
            Seaborn default and built-in color palettes
                sns.set()
                for col in 'xy':
                  plt.hist(data[col], normed=True, alpha=0.5)
                
                Seaborn has six variations of its default color palette: deep, muted, pastel, bright, dark, and colorblind.
                sns.set_palette("pastel")                           # Set the palette to the "pastel" default palette
                sns.stripplot(x="day", y="total_bill", data=tips)   # plot using the "pastel" palette
            Color Brewer Palettes https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3
                custom_palette = sns.color_palette("Paired", 9)
                sns.palplot(custom_palette)
            Selecting palettes for your dataset
                - Qualitative Palettes for Categorical Datasets
                    qualitative_colors = sns.color_palette("Set3", 10)
                    sns.palplot(qualitative_colors)
                - Sequential Palettes
                    sequential_colors = sns.color_palette("RdPu", 10)
                    sns.palplot(sequential_colors)
                - Diverging Palettes
                    diverging_colors = sns.color_palette("RdBu", 10)
                    sns.palplot(diverging_colors)
        
        Project: Visualizing World Cup Data With Seaborn
            Kaggle https://www.kaggle.com/ to find all the code & data you need to do your data science work. Use over 19,000 public datasets and 200,000 public notebooks to conquer any analysis in no time.

************************************************************************************************
************************************************************************************************
************************************************************************************************

12. Visualization Cumulative Projects
    > Twitch Project
        The project is broken down into two parts:
            Part 1: Analyze Data with SQL
                'stream' table    - Stream viewing data
                'chat' table      - Chat usage data
            Part 2: Visualize Data with Matplotlib
                Take your findings from the SQL queries and visualize them using Python and Matplotlib, in the forms of:
                    - Bar Graph: Featured Games
                    - Pie Chart: Stream Viewers’ Locations
                    - Line Graph: Time Series Analysis
    > Metrics for FoodWheel
    > Kiva Visualization Project
        https://www.kiva.org/
        Explore the average loan amount by country using aggregated bar charts, and visualize the distribution of loan amount by project type and gender using box plots and violin plots.

************************************************************************************************
************************************************************************************************
************************************************************************************************

13. Data Visualization Capstone Projects
    OPTION 1: LIFE EXPECTANCY AND GDP IN FIVE NATIONS
        Explore data from the World Health Organization and the World Bank to visualize the relationship between Gross Domestic Product (GDP) and Life Expectancy. This project will combine material from Matplotlib, Pandas, and Seaborn.
        CREATE A SLIDE DECK
            Create a slide deck using Google Drive , Microsoft Powerpoint, or some other presentation software. Your presentation should include the following:
                - A title slide
                - A list of your visualizations and your role in their creation for the “Stock Profile” team
                - A visualization of the distribution of the stock prices for Netflix in 2017
                - A visualization and a summary of Netflix stock and revenue for the past four quarters and a summary
                - A visualization and a brief summary of their earned versus actual earnings per share
                - A visualization of Netflix stock against the Dow Jones stock (to get a sense of the market) in 2017
                If you like, you can also record a video of yourself giving the presentation and upload it to YouTube.
    OPTION 2: VISUALIZING THE NETFLIX STOCK PROFILE
        Explore data from the publicly traded company, Netflix, to visualize the performance of the stock during the year of 2017.
        CREATE A BLOG POST
            Once you’ve performed your analysis on your computer, you’re ready to create your blog post.
            The Guardian’s Datablog(https://www.theguardian.com/data) is a good resource for example blog posts about data visualizations.
            Create a blog post using Google Drive, Medium, or some other blogging platform. Your blog post should include the following:
            - A compelling title about your findings
            - An introduction to the data research
            - A section sharing the background info (definition of GDP for example) and sources for your data as well as any further research you conducted
            - An accompanying paragraph describing the following visualizations:
                - The violin plot of the life expectancy distribution by country
                - The facet grid of scatter graphs mapping GDP as a function of Life Expectancy by country
                - The facet grid of line graphs mapping GDP by country
                - The facet grid of line graphs mapping Life Expectancy by country
            - Any other graphs that you created in the Jupyter notebook
            - A conclusion touching on the limitations of the data and further research

    Save your Jupyter notebook as a “.py” file using File > Download as > Python (.py)
    
    Another popular programming language used in Data Science is R. 
    R is a statistical programming language that works especially well with data and is loved by users in academia and industry.

************************************************************************************************
************************************************************************************************
************************************************************************************************

14. Learn Statistics With Python
    > Mean, Median, and Mode
        Calculating Mean
            // Manually finding the mean of a dataset
            The mean, often referred to as the average, is a way to measure the center of a dataset.
            data = [4, 6, 2, 8]
                Calculate the total                     4 + 6 + 2 + 8 = 204+6+2+8=20
                Divide by the number of observations    20​/4=5     // mean
            
            // Using Python’s NumPy library to find the mean of a dataset
            NumPy Average   - syntax:  np.average(my_array)
                example_array = np.array([24, 16, 30, 10, 12, 28, 38, 2, 4, 36])
                example_average = np.average(example_array)
        Median  (sort -> choose number in the middle of array)
            example_array = np.array([24, 16, 30, 10, 12, 28, 38, 2, 4, 36, 42])
            example_median = np.median(example_array)
            print(example_median)
        Mode
            from scipy import stats
            example_array = np.array([24, 16, 12, 10, 12, 28, 38, 12, 28, 24])
            example_mode = stats.mode(example_array)

        What does our data tell us?
            Make inferences from our data (example):
                It looks like the average cost of one-bedroom apartments in Manhattan is the most, and in Queens is the least. This pattern holds for the median and mode values as well.
                While the mode is not the most important indicator of centrality, the fact that mean, median, and mode are within a few hundred dollars for each borough indicates the data is centered around:
                $3,300 for Brooklyn, $3,900 for Manhattan, $2,300 for Queens
            Assumptions (example):
                We assumed that the data from Streeteasy is representative of housing prices for the entire borough. Given that Streeteasy is only used by a subset of property owners, this is not a fair assumption. A quick search on rentcafe.com will tell you the averages are more like:
                $2,695 for Brooklyn one-bedroom apartments
                $4,188 for Manhattan one-bedroom apartments
                $2,178 for Queens one-bedroom apartments
                This is an interesting finding. Why may the cost from rentcafe.com be higher in Manhattan than in Brooklyn or Queens?
                Although we don’t have the answer to this question, it’s worth thinking about the possible differences between our Streeteasy data and where rentcafe is pulling their data.
            Draw histograms.

    > Variance and Standard Deviation   // Variance - отклонение
        Variance (is a measure of spread) is a descriptive statistic that describes how spread out the points in a data set are.
        
        Example:
            import numpy as np
            grades = [88, 82, 85, 84, 90]
            mean = np.mean(grades)
            difference_one = (88 - mean) ** 2
            difference_two = (82 - mean) ** 2
            difference_three = (85 - mean) ** 2
            difference_four = (84 - mean) ** 2
            difference_five = (90 - mean) ** 2
            difference_sum = difference_one + difference_two + difference_three + difference_four + difference_five     # print("The sum of the squared differences is " + str(difference_sum))
            variance = difference_sum / 5       # print("The variance is " + str(variance))

        Variance In NumPy
            import numpy as np
            dataset = [3, 5, -2, 49, 10]
            variance = np.var(dataset)

        STANDARD DEVIATION
            Standard deviation is computed by taking the square root of the variance. sigma is the symbol commonly used for standard deviation. 
                num = 25
                num_square_root = num ** 0.5
            Standard Deviation in NumPy
                import numpy as np
                dataset = [4, 8, 15, 16, 23, 42]
                standard_deviation = np.std()

    > Histograms
        Range  
            exercise_ages = np.array([22, 27, 45, 62, 34, 52, 42, 22, 34, 26]) 
            min_age = np.amin(exercise_ages) # Answer is 22
            max_age = np.amax(exercise_ages) # Answer is 62
            age_range = max_age - min_age
        Bins and Count 
            The two key features of a histogram are bins and counts.
            A bin is a sub-range of values that falls within the range of a dataset. All bins in a histogram must be the same width.
            A count is the number of values that fall within a bin’s range.
                exercise_ages = np.array([22, 27, 45, 62, 34, 52, 42, 22, 34, 26, 24, 65, 34, 25, 45, 23, 45, 33, 52, 55])
                np.histogram(exercise_ages, range = (20, 70), bins = 5)
        Plotting a Histogram
            from matplotlib import pyplot as plt
            plt.hist(exercise_ages, range = (20, 70), bins = 5, edgecolor='black')
            plt.title("Decade Frequency")
            plt.xlabel("Ages")
            plt.ylabel("Count")
            plt.show()

    > Describe a Histogram
        Interpret a distribution using the following five features of a dataset:
            Center
            Spread
            Skew
            Modality
            Outliers
    > Quartiles, Quantiles, and Interquartile Range
        QUARTILES
            A common way to communicate a high-level overview of a dataset is to find the values that split the data into four groups of equal size.
            By doing this, we can then say whether a new datapoint falls in the first, second, third, or fourth quarter of the data.
            Q2 happens to be exactly the median. 
            To find Q1, we take all of the data points smaller than Q2 and find the median of those points. 
            To find Q3, do the same process using the points that are larger than Q2.
            The first step in finding the quartiles of a dataset is to sort the data from smallest to largest.

            import numpy as np
            dataset = [50, 10, 4, -3, 4, -20, 2]
            first_quartile = np.quantile(dataset, 0.25)
            second_quartile = np.quantile(dataset, 0.5)
            third_quartile = np.quantile(dataset, 0.75)     # Q3
        QUANTILES
            Quantiles are points that split a dataset into groups of equal size. 

            import numpy as np
            dataset = [5, 10, -20, 42, -9, 10]
            ten_percent = np.quantile(dataset, 0.10)   
            # or
            ten_percent = np.quantile(dataset, [0.2, 0.4, 0.6, 0.8])    # Many Quantiles

            Common Quantiles
                - 2-quantile. This value splits the data into two groups of equal size. Half the data will be above this value, and half the data will be below it. This is also known as the median!
                - 4-quantiles, or the quartiles, split the data into four groups of equal size.
                - percentiles, or the values that split the data into 100 groups, are commonly used to compare new data points to the dataset. 
        INTERQUARTILE RANGE
            The interquartile range (IQR) ignores the tails of the dataset, so you know the range around-which your data is centered.
            IQR = Q3 - Q1

            From scipy.stats import iqr
            dataset = [4, 10, 38, 85, 193]
            interquartile_range = iqr(dataset)

    > Boxplots
        Boxplot by hand:
            Median: When making a box plot, the easiest place to start is the line that is inside the box. This line is the median of the dataset. Half of the data falls above that line and half falls below it. // The box of a boxplot visualizes the median, first quartile, and third quartile of a dataset.
            Interquartile Range: The box extends to the first and third quartile of the dataset.    // The length of the box in a boxplot visualizes the interquartile range.
            Whiskers: display information related to the spead of the dataset.  // extend from the box 1.5 times the size of the interquartile range.
            Outliers: is a point in the dataset that falls outside of the whiskers. Outliers are usually represented with a dot or an asterisk.
        Boxplots in Matplotlib: 
            import matplotlib.pyplot as plt
            data = [1, 2, 3, 4, 5]
            plt.boxplot(data)
            plt.show()

************************************************************************************************
************************************************************************************************
************************************************************************************************

15. Introduction to Statistics with NumPy
    > Introduction to NumPy
        Importing NumPy
            import numpy as np                          
        NumPy Arrays
            my_array = np.array([1, 2, 3, 4, 5, 6])     
            # or
            my_list = [1, 2, 3, 4, 5, 6]
            my_array = np.array(my_list)
        Creating an Array from a CSV
            csv_array = np.genfromtxt('sample.csv', delimiter=',')      # Note that in this case, our file sample.csv has values separated by commas, so we use delimiter=',', but sometimes you’ll find files with other delimiters, the most common being tabs or colons.
        Operations with NumPy Arrays
            NumPy arrays are more efficient than lists. 
            To add a number to each value in a python list:     // The same is true for subtraction, multiplication, and division.
                # With a list
                l = [1, 2, 3, 4, 5]
                l_plus_3 = []
                for i in range(len(l)):
                    l_plus_3.append(l[i] + 3)
                # With an array
                a = np.array(l)
                a_plus_3 = a + 3
            To find the squares or square roots of each value:
                a ** 2
            To take the square root of each value:
                np.sqrt(a)
            Adding or subtracting arrays:
                a = np.array([1, 2, 3, 4, 5])
                b = np.array([6, 7, 8, 9, 10])
                a + b                               // array([ 7,  9, 11, 13, 15])
        Two-Dimensional Arrays
        Selecting Elements from a 1-D Array
            a = np.array([5, 2, 7, 0, 11])
            a[0]        // 5
            a[-1]       // 11
            a[-2]       // 0
            a[1:3]      // array([2, 7])        - to select multiple elements in the array, e.g. will select all the elements from a[1] to a[3], including a[1] but excluding a[3].
            a[:3]       // array([5, 2, 7])     - to select all elements before a[3]
            a[-3:]      // array([7, 0, 11])    - to select the last 3 elements in an array
        Selecting Elements from a 2-D Array
            a = np.array([[32, 15, 6, 9, 14], [12, 10, 5, 23, 1], [2, 16, 13, 40, 37]])
            a[2,1]      // 16
            a[:,0]      // array([32, 12,  2])  - selects the first column
            a[1,:]      // array([12, 10,  5, 23,  1])  - selects the second row
            a[0,0:3]    //array([32, 15,  6])   - selects the first three elements of the first row
        Logical Operations with Arrays
            a = np.array([10, 2, 2, 4, 5, 3, 9, 8, 9, 7])
            a > 5                   // array([True, False, False, False, False, False, True, True, True, True], dtype=bool)
            a[a > 5]                // array([10, 9, 8, 9, 7])
            a[(a > 5) | (a < 2)]    // array([10, 9, 8, 9, 7])

    > Statistics in NumPy
        Mean
            np.mean()
            np.mean(survey_array > 8)   // Mean and Logical Operations
            np.mean(ring_toss)  // Calculating the Mean of 2D Arrays
                np.mean(ring_toss, axis=1)  // axis 1 (the “rows”)
                np.mean(ring_toss, axis=0)  // axis 0 (the “columns”)
        Median
            np.median(my_array)
        Percentiles
            np.percentile(my_array, 40)
        Interquartile Range
        Outliers
        Standard Deviation
            While the mean and median can tell us about the center of our data, they do not reflect the range of the data. That’s where standard deviation comes in.
            Similar to the interquartile range, the standard deviation tells us the spread of the data. The larger the standard deviation, the more spread out our data is from the center. The smaller the standard deviation, the more the data is clustered around the mean.
                np.std(my_array)

        Histograms
        Different Types of Distributions
            One way to classify a dataset is by counting the number of distinct peaks present in the graph. Peaks represent concentrations of data. 
            - A unimodal dataset has only one distinct peak.histogram
                - A symmetric dataset has equal amounts of data on both sides of the peak. Both sides should look about the same.histogram
                    a = np.random.normal(0, 1, size=100000) // np.random.normal(loc=0, scale=1, size=100000)
                - A skew-right dataset has a long tail on the right of the peak, but most of the data is on the left.histogram
                - A skew-left dataset has a long tail on the left of the peak, but most of the data is on the right.
            - A bimodal dataset has two distinct peaks. This often happens when the data contains two different populations.histogram
                The binomial distribution tells us how likely it is for a certain number of “successes” to happen, given a probability of success and a number of trials.
                    # Let's generate 10,000 "experiments"
                    # N = 10 shots
                    # P = 0.30 (30% he'll get a free throw)
                    a = np.random.binomial(10, 0.30, size=10000)
            - A multimodal dataset has more than two peaks.histogram
            - A uniform dataset doesn’t have any distinct peaks.
        
************************************************************************************************
************************************************************************************************
************************************************************************************************

16. Hypothesis Testing with SciPy
    > Hypothesis Testing
        Statistical Concepts
            The individual measurements on Monday, Tuesday, and Wednesday are called samples. 
            A sample is a subset of the entire population. The mean of each sample is the sample mean and it is an estimate of the population mean.

            'Central Limit Theorem' - states that if we have a large enough sample size, all of our sample means will be sufficiently close to the population mean.

            'Hypothesis testing' is a mathematical way of determining whether we can be confident that the null hypothesis is false.
            A 'null hypothesis' is a statement that the observed difference is the result of chance.

            In statistical hypothesis testing, wo types of error:
                - Type I error, is finding a correlation between things that are not related. This error is sometimes called a “false positive” and occurs when the null hypothesis is rejected even though it is true.
                    e.g. type_i_errors = intersect(experimental_positive, actual_negative)
                - Type II error, is failing to find a correlation between things that are actually related. This error is referred to as a “false negative” and occurs when the null hypothesis is accepted even though it is false.
                    e.g. type_ii_errors = intersect(experimental_negative, actual_positive)
            
            'P-value' is the probability that we yield the observed statistics under the assumption that the null hypothesis is true.
                A p-value of 0.05 would mean that if we assume the null hypothesis is true, there is a 5% chance that the data results in what was observed due only to random sampling error. This generally means there is a 5% chance that there is no difference between the two population means.
                Generally, we want a p-value of less than 0.05, meaning that there is less than a 5% chance that our results are due to random chance.
            
        Hypothesis Testing
            Types of Hypothesis Test:
                For numerical data:
                    - One Sample T-Tests
                        A 'univariate T-test' compares a sample mean to a hypothetical population mean. It answers the question “What is the probability that the sample came from a distribution with the desired mean?”
                        SciPy has a function called ttest_1samp, which performs a 1 Sample T-Test:
                            tstat, pval = ttest_1samp(example_distribution, expected_mean)
                            print pval
                        It returns two outputs: 
                            - t-statistic 
                            - p-value — telling us how confident we can be that the sample of values came from a distribution with the mean specified.
                        
                    - Two Sample T-Tests
                        Use SciPy’s ttest_ind function.
                            tstat, pval = ttest_ind(week1, week2)
                            # or
                            a_b_pval = ttest_ind(a, b).pvalue
                    - ANOVA
                        When comparing more than two numerical datasets, the best way to preserve a Type I error probability of 0.05 is to use ANOVA. 
                        ANOVA (Analysis of Variance) tests the null hypothesis that all of the datasets have the same mean. If we reject the null hypothesis with ANOVA, we’re saying that at least one of the sets has a different mean; however, it does not tell us which datasets are different.
                        Use the SciPy function f_oneway to perform ANOVA on multiple datasets. 
                            fstat, pval = f_oneway(scores_mathematicians, scores_writers, scores_psychologists)
                    - Tukey Tests
                        Example:
                            movie_scores = np.concatenate([drama_scores, comedy_scores, documentary_scores])
                            labels = ['drama'] * len(drama_scores) + ['comedy'] * len(comedy_scores) + ['documentary'] * len(documentary_scores)
                            tukey_results = pairwise_tukeyhsd(movie_scores, labels, 0.05)
                For categorical data:
                    - Binomial Tests
                        How do we begin comparing, if there’s no mean or standard deviation that we can use? Example: The data is divided into two discrete categories, “made a purchase” and “did not make a purchase”.
                        Examples:
                            - Comparing the actual percent of emails that were opened to the quarterly goals
                            - Comparing the actual percentage of respondents who gave a certain survey response to the expected survey response
                            - Comparing the actual number of heads from 1000 coin flips of a weighted coin to the expected number of heads
                        SciPy has a function called binom_test.
                        binom_test requires three inputs, the number of observed successes, the number of total trials, and an expected probability of success. 
                            pval = binom_test(525, n=1000, p=0.5)
                    - Chi Square
                        If we have two or more categorical datasets that we want to compare, we should use a Chi Square test.
                        Examples:
                            - An A/B test where half of users were shown a green submit button and the other half were shown a purple submit button. Was one group more likely to click the submit button?
                            - Men and women were both given a survey asking “Which of the following three products is your favorite?” Did the men and women have significantly different preferences?
                        In SciPy use the function chi2_contingency.
                            X = [[30, 10], [35, 5], [28, 12], [20, 20]]
                            chi2, pval, dof, expected = chi2_contingency(X)
                            print pval

                
                Table for determining which significance test to use:
                                                    Numerical               Categorical
                    Sample vs. Known Quantity       1 Sample T-Test         Binomial Test
                    2 Samples                       2 Sample T-Test         Chi Square
                    More than 2 Samples             ANOVA and/or Tukey      Chi Square 

    > Sample Size Determination
        In order to determine the sample size necessary for an A/B test, a sample size calculator requires three numbers:
            - The Baseline conversion rate      
                e.g. In order to get our baseline, we need to first know how many users visited the site. 
                baseline_percent = (paying_visitor_count * 100.0) / total_visitor_count
            - The Minimum detectable effect
                Example:
                    payment_history = noshmishmosh.money_spent
                    average_payment = np.mean(payment_history)
                    new_customers_needed = np.ceil(1240 / average_payment)
                    percentage_point_increase = (new_customers_needed * 100.0) / total_visitor_count
                    minimum_detectable_effect = 100.0 * percentage_point_increase / baseline_percent
                    print(minimum_detectable_effect)
            - The Statistical significance
        A/B Testing: Understanding the Baseline
            number_of_site_visitors = 2000.0
            number_of_converted_visitors = 1300.0
            conversion_rate = number_of_converted_visitors / number_of_site_visitors
        A/B Testing: Determining Lift
            In order to choose a sample size, we need to know the smallest difference that we actually care to measure. This “smallest difference” is called lift.
            100 * (new - old) / old
        A/B Testing: Splitting a Test
            # How many users will see Option B each day?
            optionB_per_day = 350 * 0.20        #print optionB_per_day
            # In order to get 910 users, how many days will we need to run this experiment for?
            hamster_headline_experiment_length = 910 / optionB_per_day      # print hamster_headline_experiment_length
        Sample Size Calculator for Margin of Error
            Sample size calculators use 4 parameters:
                - Margin of error
                - Confidence level
                - Population size
                - Expected proportion

                Example:
                    import math
                    margin_of_error = 4
                    confidence_level = 95
                    likely_proportion = 40
                    population_size = 100000
                    sample_size = 573
                    weeks_of_survey = math.ceil(sample_size/120.0)

    > Analyze FarmBurg's A/B Test
************************************************************************************************
************************************************************************************************
************************************************************************************************

17. Practical Data Cleaning
    > Data Cleaning with Pandas
        regex
            Literals - The simplest text we can match with regular expressions are literals.
            Alternation - performed in regular expressions with the pipe symbol, |, allows us to match either the characters preceding the | OR the characters after the |.
                e.g. baboons|gorillas
            Character Sets - denoted by a pair of brackets [], let us match one character from a series of characters, allowing for matches with incorrect or different spellings.
                e.g. con[sc]en[sc]us will match consensus
                caret ^ symbol - negates the set, matching any character that is not stated. 
                    e.g. regex [^cat] will match any character that is not c, a, or t
            Wildcards - will match any single character (letter, number, symbol or whitespace) in a piece of text. 
                e.g. The regex ......... will completely match orangutan and marsupial! 
                Similarly, the regex I ate . bananas will completely match both I ate 3 bananas and I ate 8 bananas!
                the escape character, \, to escape the wildcard functionality of the . and match an actual period. 
            Ranges
                The - character allows us to specify that we are interested in matching a range of characters.
                e.g. [a-c], The regex I adopted [2-9] [b-h]ats will match the text I adopted 4 bats
                To match any single capital or lowercase alphabetical character, we can use the regex [A-Za-z].
            Shorthand Character Classes
                \w: the “word character” class represents the regex range [A-Za-z0-9_] - it matches a single uppercase character, lowercase character, digit or underscore
                \d: the “digit character” class represents the regex range [0-9], and it matches a single digit character
                \s: the “whitespace character” class represents the regex range [ \t\r\n\f\v], matching a single space, tab, carriage return, line break, form feed, or vertical tab
                e.g. \d\s\w\w\w\w\w\w\w matches '3 monkeys'
            Negated shorthand character classes - match any character that is NOT in the regular shorthand classes.
                \W: the “non-word character” class represents the regex range [^A-Za-z0-9_]
                \D: the “non-digit character” class represents the regex range [^0-9]
                \S: the “non-whitespace character” class represents the regex range [^ \t\r\n\f\v]
            Grouping    - denoted with the open parenthesis ( and the closing parenthesis ), lets us group parts of a regular expression together, and allows us to limit alternation to part of the regex.
                e.g. I love (baboons|gorillas) will match the text 'I love' and then match either 'baboons' or 'gorillas'
            Quantifiers - Fixed
                denoted with curly braces {}, let us indicate the exact quantity of a character we wish to match, or allow us to provide a quantity range to match on.
                instead of \w\w\w\w\w\w\s\w\w\w\w\w\w, use Quantifiers.
                \w{3} will match exactly 3 word characters
                \w{4,7} will match at minimum 4 word characters and at maximum 7 word characters
                roa{3}r = roaaar
            Quantifiers - Optional
                indicated by the question mark ?, allow us to indicate a character in a regex is optional, or can appear either 0 times or 1 time. 
                humou?r = humour or humor
                (rotten)? = rotten or no word
            Quantifiers - 0 or More, 1 or More
                Kleene star, denoted with the asterisk *, is also a quantifier, and matches the preceding character 0 or more times. This means that the character doesn’t need to appear, can appear once, or can appear many many times.
                    e.g. meo*w = mew | meow | meoow | meoooooooooooow
                Kleene plus, denoted by the plus +, which matches the preceding character 1 or more times.
                    e.g. meo+w = meow | meoow | meoooooooooooow
            Anchors
                The anchors hat ^ and dollar sign $ are used to match text at the start and the end of a string, respectively.
                e.g. '^Monkeys: my mortal enemy$' will completely match the text 'Monkeys: my mortal enemy'
        DATA CLEANING WITH PANDAS
            When we receive raw data, we have to do a number of things before we’re ready to analyze it, possibly including:
                - diagnosing the “tidiness” of the data — how much data cleaning we will have to do
                    For data to be tidy, it must have:
                        - Each variable as a separate column
                        - Each row as a separate observation
                    The first step of diagnosing whether or not a dataset is tidy is using 'pandas' functions to explore and probe the dataset:
                        .head() — display the first 5 rows of the table
                        .info() — display a summary of the table
                        .describe() — display the summary statistics of the table
                        .columns — display the column names of the table
                        .value_counts() — display the distinct values for a column
                - reshaping the data — getting right rows and columns for effective analysis
                    Use pd.melt() to reshape a table. .melt() takes in a DataFrame, and the columns to unpack:
                    pd.melt(frame=df, id_vars='name', value_vars=['Checking','Savings'], value_name="Amount", var_name="Account Type")
                        The parameters are:
                            frame: the DataFrame you want to melt
                            id_vars: the column(s) of the old DataFrame to preserve
                            value_vars: the column(s) of the old DataFrame that you want to turn into variables
                            value_name: what to call the column of the new DataFrame that stores the values
                            var_name: what to call the column of the new DataFrame that stores the variables
                    Use .columns() to rename the columns after melting:
                        df.columns(["Account", "Account Type", "Amount"])

                    e.g. 
                        Account     Checking        Savings
                        123456      80              90

                        Transformed into table below:

                        Account     Account Type    Amount
                        123456      Checking        80
                        123456      Savings         90

                - combining multiple files
                    glob, a Python library for working with files - can open multiple files by using regex matching to get the filenames:
                        import glob
                        files = glob.glob("file*.csv")
                        df_list = []
                        for filename in files:
                          data = pd.read_csv(filename)
                          df_list.append(data)
                        df = pd.concat(df_list)
                        print(files)
                - changing the types of values — how we fix a column where numerical values are stored as strings, for example
                    Splitting by Index  (e.g from MMDDYYYY format)
                        df['month'] = df.birthday.str[0:2]      # Create the 'month' column
                        df['day'] = df.birthday.str[2:4]        # Create the 'day' column
                        df['year'] = df.birthday.str[4:]        # Create the 'year' column

                        e.g. 
                            birthday    month   day     year
                            '11011993'  '11'    '01'    '1993'
                    Splitting by Character  (e.g. "admin_US" -> user type (with values like “admin”), country (with values like “US”))
                        df['str_split'] = df.type.str.split('_')        # Create the 'str_split' column
                        df['usertype'] = df.str_split.str.get(0)        # Create the 'usertype' column
                        df['country'] = df.str_split.str.get(1)         # Create the 'country' column
                    Looking at Types
                        To see the types of each column of a DataFrame:     print(df.dtypes)

                - dropping or filling missing values - how we deal with data that is incomplete or missing
                    The missing elements normally show up as NaN (or Not a Number) values.
                    Some calculations we do will just skip the NaN values, but some calculations or visualizations we try to perform will break when a NaN is encountered.
                    Methods to deal with missing values:
                        - Method 1: drop all of the rows with a missing value
                            bill_df = bill_df.dropna()
                            bill_df = bill_df.dropna(subset=['num_guests'])     # to remove every row with a NaN value in the num_guests column only
                        - Method 2: fill the missing values with the mean of the column, or with some other aggregate value.
                            bill_df = bill_df.fillna(value={"bill":bill_df.bill.mean(), "num_guests":bill_df.num_guests.mean()})
                            # or df.fillna(100)

                - manipulating strings to represent the data better     (e.g. '$4' -> 4)
                    Use regex to get rid of all of the dollar signs:
                        fruit.price = fruit['price'].replace('[\$,]', '', regex=True)
                    Use the pandas function .to_numeric() to convert strings containing numerical values to integers or floats:
                        fruit.price = pd.to_numeric(fruit.price)
                    
                    To extract numerical data from the strings, use regex and pandas’ .str.split():
                        split_df = df['exerciseDescription'].str.split('(\d+)', expand=True)

                - (additional)Dealing with Duplicates
                    Use the pandas function .duplicated(), which will return a Series telling us which rows are duplicate rows.
                    Use the pandas .drop_duplicates() function to remove all rows that are duplicates of another row.
                    e.g. table_name.drop_duplicates()
                    e.g. fruits = fruits.drop_duplicates(subset=['item'])   # If we wanted to remove every row with a duplicate value in the item column, we could specify a subset

************************************************************************************************
************************************************************************************************
************************************************************************************************

18. Data Analysis Capstone Projects
    Capstone Project: MuscleHub A/B Test 
        Your presentation should include the following:
            - A title slide
            - A description of what happened in this A/B test
            - A summary of your dataset and any information you think would be helpful background
            - The results of the three hypothesis tests that you ran, including an explanation of the type of test that you used and why it was appropriate
            - A summary of the qualitative data
            - A recommendation for MuscleHub
    Capstone Project: Biodiversity in National Parks    - analyze, clean up, and plot data.
        Your presentation should include the following:
            - A title slide
            - A section describing the data in species_info.csv. Be sure to include some (or all) of what you noticed while working through the notebook.
            - A section describing the significance calculations that you did for endangered status between different categories of species.
            - A recommendation for conservationists concerned about endangered species, based on your significance calculations
            - A section describing the sample size determination that you did for the foot and mouth disease study
            - All of the graphs that you created in the notebook

************************************************************************************************
************************************************************************************************
************************************************************************************************

19. Learn Web Scraping with Beautiful Soup
    > Beautiful Soup
        But how do we get data? - If it’s provided to us in a well-organized csv or json file, we’re lucky! Most of the time, we need to go out and search for it ourselves.
        Often times you’ll find the perfect website that has all the data you need, but there’s no way to download it.
        BeautifulSoup - Python library - allows us to easily and quickly take information from a website and put it into a DataFrame.
        Always check a website’s Terms and Conditions before scraping. Read the statement on the legal use of data. Usually, the data you scrape should not be used for commercial purposes.
        Do not spam the website with a ton of requests. A large number of requests can break a website that is unprepared for that level of traffic. As a general rule of good practice, make one request to one webpage per second.
        Requests
            In order to get the HTML of the website, we need to make a request to get the content of the webpage.
            Python has a 'requests' library 
            Example:
                import requests
                webpage = requests.get('https://www.codecademy.com/articles/http-requests')
                print(webpage.text)     # or print(webpage.content)
        
        The BeautifulSoup Object
            from bs4 import BeautifulSoup
            soup = BeautifulSoup("rainbow.html", "html.parser")     # options: "html.parser", "lxml", "html5lib"
            # or soup = BeautifulSoup(webpage.content, "html.parser") 
        Object Types
            Tags
                soup = BeautifulSoup('<div id="example">An example div</div><p>An example p tag</p>')
                print(soup.div)
                print(soup.div.name)
                print(soup.div.attrs)
            NavigableStrings
                print(soup.div.string)      # Print out the string associated with 'div' tag 
        Navigating by Tags
            for child in soup.ul.children:      # to get the children of a tag by accessing the .children attribute
                print(child)
            for parent in soup.li.parents:      # to navigate up the tree of a tag by accessing the .parents attribute
                print(parent)
        Website Structure
            Chrome DevTools (inspector)
        Find All
            If we want to find all of the occurrences of a tag, instead of just the first one, we can use .find_all().
                print(soup.find_all("h1"))

            Using Regex
                import re
                soup.find_all(re.compile("[ou]l"))      # if we want every <ol> and every <ul> that the page contains
                soup.find_all(re.compile("h[1-9]"))     # if we want all of the h1 - h9 tags that the page contains
            Using Lists
                soup.find_all(['h1', 'a', 'p'])         # to specify all of the elements we want to find
            Using Attributes
                soup.find_all(attrs={'class':'banner'})
                soup.find_all(attrs={'class':'banner', 'id':'jumbotron'})
            Using A Function
                def has_banner_class_and_hello_world(tag):
                    return tag.attr('class') == "banner" and tag.string == "Hello world"
                soup.find_all(has_banner_class_and_hello_world)
            Select for CSS Selectors
                soup.select(".recipeLink")
                soup.select("#selected")
                for link in soup.select(".recipeLink > a"):
                    webpage = requests.get(link)
                    new_soup = BeautifulSoup(webpage)
            Reading Text
                soup.get_text()

************************************************************************************************
************************************************************************************************
************************************************************************************************

20. Machine Learning: Supervised Learning
    > Introduction to ML
        ML can be branched out into categories:
            - Supervised Learning       - is where the data is labeled and the program learns to predict the output from the input data. 
                e.g. a supervised learning algorithm for credit card fraud detection would take as input a set of recorded transactions. For each transaction, the program would predict if it is fraudulent or not.
                Supervised learning problems can be further grouped into regression and classification problems.
                Regression:
                    In regression problems, we are trying to predict a continuous-valued output. Examples are:
                        What is the housing price in Neo York?
                        What is the value of cryptocurrencies?
                Classification:
                    In classification problems, we are trying to predict a discrete number of values. Examples are:
                        Is this a picture of a human or a picture of an AI?
                        Is this email spam?

                Examples:
                    - image classification  - To do this, we normally show a program thousands of examples of pictures, with labels that describe them. 
                        e.g. Captcha
                    
            - Unsupervised Learning     - is a type of ML where the program learns the inherent structure of the data based on unlabeled examples.
                Clustering is a common unsupervised ML approach that finds patterns and structures in unlabeled data by grouping them into clusters.
                Some examples:
                    Social networks clustering topics in their news feed
                    Consumer sites clustering users for recommendations
                    Search engines to group similar objects in one cluster
            
        Scikit-learn https://scikit-learn.org/stable/
            Scikit-learn is a library in Python that provides many unsupervised and supervised learning algorithms.

            See sklearn commands below:
            Linear Regression
                https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html
                Import and create the model:
                    from sklearn.linear_model import LinearRegression
                    your_model = LinearRegression()
                Fit:
                    your_model.fit(x_training_data, y_training_data)
                        .coef_: contains the coefficients
                        .intercept_: contains the intercept
                Predict:
                    predictions = your_model.predict(your_x_data)
                        .score(): returns the coefficient of determination R²
            Naive Bayes 
                https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)
                Import and create the model:
                    from sklearn.naive_bayes import MultinomialNB
                    your_model = MultinomialNB()
                Fit:
                    your_model.fit(x_training_data, y_training_data)
                Predict:
                    # Returns a list of predicted classes - one prediction for every data point
                    predictions = your_model.predict(your_x_data)
                    # For every data point, returns a list of probabilities of each class
                    probabilities = your_model.predict_proba(your_x_data)
            K-Nearest Neighbors
                https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier
                Import and create the model:
                    from sklearn.neigbors import KNeighborsClassifier
                    your_model = KNeighborsClassifier()
                Fit:
                    your_model.fit(x_training_data, y_training_data)
                Predict:
                    # Returns a list of predicted classes - one prediction for every data point
                    predictions = your_model.predict(your_x_data)
                    # For every data point, returns a list of probabilities of each class
                    probabilities = your_model.predict_proba(your_x_data)
            K-Means
                https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
                Import and create the model:
                    from sklearn.cluster import KMeans
                    your_model = KMeans(n_clusters=4, init='random')
                        n_clusters: number of clusters to form and number of centroids to generate
                        init: method for initialization
                            k-means++: K-Means++ [default]
                            random: K-Means
                        random_state: the seed used by the random number generator [optional]
                Fit:
                    your_model.fit(x_training_data)
                Predict:
                    predictions = your_model.predict(your_x_data)
            Validating the Model
                https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics
                Import and print accuracy, recall, precision, and F1 score:
                    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
                    print(accuracy_score(true_labels, guesses))
                    print(recall_score(true_labels, guesses))
                    print(precision_score(true_labels, guesses))
                    print(f1_score(true_labels, guesses))
                Import and print the confusion matrix:
                    from sklearn.metrics import confusion_matrix
                    print(confusion_matrix(true_labels, guesses))
            Training Sets and Test Sets
                https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
                    # train_test_split returns four values in the following order:
                        - The training set
                        - The validation set
                        - The training labels
                        - The validation labels

                    from sklearn.model_selection import train_test_split
                    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2)
                        train_size: the proportion of the dataset to include in the train split
                        test_size: the proportion of the dataset to include in the test split
                        random_state: the seed used by the random number generator [optional]

    > Linear Regression
        The simplest model that we can fit to data is a line. A line is a rough approximation, but it allows us the ability to explain and predict variables that have a linear relationship with each other.
        Points and Lines: y=mx+b,     where m - slope, b - intercept.
        Loss:   For each data point, we calculate loss, a number that measures how bad the model’s (in this case, the line’s) prediction was.
                We can think about loss as the squared distance from the point to the line. 
                The line that produces the lowest total loss will be the line of better fit.

                Example:
                    x = [1, 2, 3]
                    y = [5, 1, 3]

                    #y = x
                    m1 = 1
                    b1 = 0
                    y_predicted1 = [m1*x_val + b1 for x_val in x]

                    #y = 0.5x + 1
                    m2 = 0.5
                    b2 = 1
                    y_predicted2 = [m2*x_val + b2 for x_val in x]

                    total_loss1 = 0
                    for i in range(len(y)):
                      total_loss1 += (y[i] - y_predicted1[i]) ** 2

                    total_loss2 = 0
                    for i in range(len(y)):
                      total_loss2 += (y[i] - y_predicted2[i]) ** 2

                    print(total_loss1)
                    print(total_loss2)
                    better_fit = 2
        Minimizing Loss
        Gradient Descent for Intercept:  Gradient refers to the slope of the curve at any point.
            To find the gradient of loss as intercept changes, the formula comes out to be:
                we find the sum of y_value - (m*x_value + b) for all the y_values and x_values we have
                and then we multiply the sum by a factor of -2/N. N is the number of points we have.
            Example:
                def get_gradient_at_b(x,y,m,b):
                    diff = 0
                    N = len(x)
                    for i in range(0, len(x)):
                        y_val = y[i]
                        x_val = x[i]
                        diff += (y_val - ((m * x_val) + b))
                    b_gradient = -2/N * diff
                    return b_gradient
        Gradient Descent for Slope:
            Example:
                def get_gradient_at_m(x, y, m, b):
                    diff = 0
                    N = len(x)
                    for i in range(N):
                        y_val = y[i]
                        x_val = x[i]
                        diff += x_val*(y_val - ((m * x_val) + b))
                    m_gradient = -2/N * diff
                    return m_gradient
        Put it Together            
            We can scale the size of the step by multiplying the gradient by a learning rate.
            To find a new b value:  new_b = current_b - (learning_rate * b_gradient)
            Example:
                def step_gradient(x, y, b_current, m_current):          # add learning_rate as the last parameter
                    b_gradient = get_gradient_at_b(x,y,b_current,m_current)
                    m_gradient = get_gradient_at_m(x,y,b_current,m_current)
                    b = b_current - (0.01 * b_gradient)     # replace 0.01 with learning_rate
                    m = m_current - (0.01 * m_gradient)     # replace 0.01 with learning_rate
                    return [b, m]

                months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
                revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]
                b = 0       # current intercept guess
                m = 0       # current slope guess

                b, m = step_gradient(months, revenue, b, m)     # Call your function here to update b and m
                print(b, m)
        Convergence
            Convergence is when the loss stops changing (or changes very slowly) when parameters are changed.
        Learning Rate
        Put it Together II
            Example:
                def gradient_descent(x, y, learning_rate, num_iterations):
                    b = 0
                    m = 0
                    for i in range(num_iterations):
                        b, m = step_gradient(b, m, x, y, learning_rate)
                    return [b,m]  

                months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
                revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]

                b, m = gradient_descent(months, revenue, 0.01, 1000)
                y = [m*x + b for x in months]

                plt.plot(months, revenue, "o")
                plt.plot(months, y)
                plt.show()
        Use Your Functions on Real Data
            Example:
                from gradient_descent_funcs import gradient_descent
                import pandas as pd
                import matplotlib.pyplot as plt

                df = pd.read_csv("heights.csv")
                X = df["height"]
                y = df["weight"]
                plt.plot(X, y, 'o')

                b, m = gradient_descent(X, y, num_iterations=1000, learning_rate=0.0001)
                y_predictions = [m*x + b for x in X]

                plt.plot(X, y_predictions)
                plt.show()
        Scikit-Learn
            You’ve now built a linear regression algorithm from scratch.
            Luckily, we don’t have to do this every time we want to use linear regression. We can use Python’s scikit-learn library.
                
                from sklearn.linear_model import LinearRegression
                line_fitter = LinearRegression()
                line_fitter.fit(X, y)
                y_predicted = line_fitter.predict(X)        # use the .predict() function to pass in x-values and receive the y-values that this line would predict

    > Multiple Linear Regression
        There are two different types of linear regression models: 
            - simple linear regression
                useful when we want to predict the values of a variable from its relationship with other variables. 
            - multiple linear regression
                e.g. In predicting the price of a home, one factor to consider is the size of the home. The relationship between those two variables, price and size, is important, but there are other variables that factor in to pricing a home: location, air quality, demographics, parking, and more.
        Multiple Linear Regression uses two or more independent variables to predict the values of the dependent variable.
        Training Set vs. Test Set
            As with most machine learning algorithms, we have to split our dataset into:
                - Training set(80% of your data): the data used to fit the model
                - Test set(20% of your data): the data partitioned away at the very start of the experiment (to provide an unbiased evaluation of the model)
            
            from sklearn.model_selection import train_test_split
            x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2)
                # parameters:
                    train_size: the proportion of the dataset to include in the train split (between 0.0 and 1.0)
                    test_size: the proportion of the dataset to include in the test split (between 0.0 and 1.0)
                    random_state: the seed used by the random number generator [optional]
            
            Example:
                import pandas as pd
                from sklearn.model_selection import train_test_split

                streeteasy = pd.read_csv("https://raw.githubusercontent.com/sonnynomnom/Codecademy-Machine-Learning-Fundamentals/master/StreetEasy/manhattan.csv")
                df = pd.DataFrame(streeteasy)
                
                # Create a DataFrame x that selects the following columns from the main df
                x = df[['bedrooms', 'bathrooms', 'size_sqft', 'min_to_subway', 'floor', 'building_age_yrs', 'no_fee', 'has_roofdeck', 'has_washer_dryer', 'has_doorman', 'has_elevator', 'has_dishwasher', 'has_patio', 'has_gym']]
                # Create a DataFrame y that selects the 'rent' column from the main df DataFrame.
                y = df[['rent']]    

                # split x into 80% training set and 20% testing set and generate:
                x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2, random_state=6)

                # The .shape attribute for NumPy arrays returns the dimensions of the array. If array has n rows × m columns, then array.shape returns (n, m).
                print(x_train.shape)    # For the Manhattan data, this results in: (2831, 14)
                print(x_test.shape)     # For the Manhattan data, this results in: (708, 14)
                print(y_train.shape)    # For the Manhattan data, this results in: (2831, 1)
                print(y_test.shape)     # For the Manhattan data, this results in: (708, 1)
        Multiple Linear Regression: Scikit-Learn
            from sklearn.linear_model import LinearRegression
            mlr = LinearRegression()
            mlr.fit(x_train, y_train) 
            y_predicted = mlr.predict(x_test)
        Visualizing Results with Matplotlib
            To create 2D graphs:
                plt.scatter(x, y, alpha=0.4)                # Create a scatter plot
                plt.xlabel("the x-axis label")              # Create x-axis label and y-axis label
                plt.ylabel("the y-axis label")
                plt.title("title!")                         # Create a title                
                plt.show()                                  # Show the plot
        Multiple Linear Regression Equation
            y = b + m1​x1​ + m2​x2                  # multiple linear regression that uses two independent variables
            y = b + m1​x1​ + m2​x2 + m3x3           # multiple linear regression that uses three independent variables
            y = b + m1​x1​ + m2​x2 + ... + mnxn     # m1, m2, m3, … mn refer to the coefficients, and b refers to the intercept that you want to find.
            The .fit() method gives the model two variables that are useful to us:
                .coef_, which contains the coefficients     # e.g. print(mlr.coef_)
                .intercept_, which contains the intercept
        Correlations
            Graphically, when you see a downward trend, it means a negative linear relationship exists. 
            When you find an upward trend, it indicates a positive linear relationship.
        Evaluating the Model's Accuracy
            Residual Analysis - is used to evaluate the regression model’s accuracy. In other words, it’s used to see if the model has learned the coefficients correctly.
                The difference between the actual value y, and the predicted value ŷ is the residual e. 
                The TSS tells you how much variation there is in the y variable.
                R² is the percentage variation in y explained by all the x variables together.
                    R² = 1 - u / v
                        u = ((y - y_predict) ** 2).sum()    # u is the residual sum of squares
                        v = ((y - y.mean()) ** 2).sum()     # v is the total sum of squares (TSS)
                If R² = 0 - it would indicate that the model fails to accurately model the data at all.

                To graph a scatter plot of residuals vs. predicted_y values:
                    residuals = y_predict - y_test
                    plt.scatter(y_predict, residuals, alpha=0.4)
                    plt.title('Residual Analysis')
                    plt.show()
        Rebuild the Model
            # remove some of the features that don’t have strong correlations and see if your scores improved!

    > Yelp Regression Project
    > Classification Vs Regression
        In terms of output, two main types of ML models: 
            - those for regression 
                Regression is used to predict outputs that are continuous. The outputs are quantities that can be flexibly determined based on the inputs of the model rather than being confined to a set of possible labels.
                For example:
                    - Predict the height of a potted plant from the amount of rainfall
                    - Predict salary based on someone’s age and availability of high-speed internet
                    - Predict a car’s MPG (miles per gallon) based on size and model year
            - those for classification
                Classification is used to predict a discrete label. The outputs fall under a finite set of possible outcomes. 
                > binary classification (True/False, 0 or 1, Hotdog / not Hotdog) - only two possible outcomes.
                    For example:
                        - Predict whether an email is spam or not
                        - Predict whether it will rain or not
                        - Predict whether a user is a power user or a casual user
                > multi-label classification is when there are multiple possible outcomes. It is useful for customer segmentation, image categorization, and sentiment analysis for understanding text. 
                    To perform these classifications, we use models like: 
                        - Naive Bayes
                        - K-Nearest Neighbors
                        - SVMs.
        ML Process
            - Formulating a Question
                What is it that we want to find out? How will we reach the success criteria that we set?
            - Finding and Understanding the Data
                Finding the relevant data to help answer your question, and getting it into the format necessary for performing predictive analysis.
                Examine the summary statistics:
                    - Calculate means and medians to understand the distribution
                    - Calculate percentiles
                    - Find correlations that indicate relationships
                You may also want to visualize the data, perhaps using:
                    - box plots to identify outliers, 
                    - histograms to show the basic structure of the data, and 
                    - scatter plots to examine relationships between variables.
            - Cleaning the Data and Feature Engineering
                Feature Engineering refers to the process by which we choose the important features (or columns) to look at, and make the appropriate transformations to prepare our data for our model.
                We might try:
                    - Normalizing or standardizing the data
                    - Augmenting the data by adding new columns
                    - Removing unnecessary columns
            - Choosing a Model
                > Use a regression algorithm - If we are attempting to find a continuous output, like predicting the number of minutes someone should wait for their order.
                > Use a classification algorithm - If we are attempting to classify an input, like determining if an order will take under 5 minutes or over 10 mins.
            - Tuning and Evaluating
                We often want to set a metric of success, so that we know the model we’ve chosen is good enough. 
                Are we looking for accuracy? Precision? Some combination of the two? We discuss this in our lesson on Precision and Accuracy.
            - Using the Model and Presenting Results

    > Classification: K-Nearest Neighbors
        DISTANCE FORMULA
            Representing Points
                pt1 = [5, 8]                # two dimensions point
                pt2 = [4, 8, 15, 16, 23]    # five-dimensional point
                Ways to define the distance between two points:
                    - Euclidean Distance    - the most commonly used distance formula
                        Euclidean distance = calculate the squared distance between each dimension -> add up all of these squared differences -> take the square root.
                            d = root of (a1​	− b1​)**2 + (a2​ − b2​)**2
                        Example:
                            def euclidean_distance(pt1,pt2):
                                distance = 0
                                for i in range(len(pt1)):
                                    distance += (pt1[i] - pt2[i]) ** 2

                                return distance ** 0.5
                            print(euclidean_distance([1,2], [4,0]))
                            print(euclidean_distance([5,4,3], [1,7,9]))
                    - Manhattan Distance    - extremely similar to Euclidean distance.
                        Manhattan distance = sum the absolute value of the difference between each dimension
                            d = |a1-b1| + |a2-b2| + ... + |an-bn|
                        Note that Manhattan distance will always be greater than or equal to Euclidean distance. 
                        Example:
                            def manhattan_distance(pt1,pt2):
                                distance = 0
                                for i in range(len(pt1)):
                                    distance += abs(pt1[i] - pt2[i])
                                return distance
                            print(manhattan_distance([1, 2], [4, 0]))
                            print(manhattan_distance([5, 4, 3], [1, 7, 9]))
                    - Hamming Distance  - another slightly different variation on the distance formula.
                        Hamming distance only cares about whether the dimensions are exactly equal. When finding the Hamming distance between two points, add one for every dimension that has different values.
                        Example:
                            def hamming_distance(pt1, pt2):
                                distance = 0
                                for i in range(len(pt1)):
                                    if(pt1[i] != pt2[i]):
                                    distance += 1
                                return distance
                            print(hamming_distance([1,2], [1,100]))
                            print(hamming_distance([5,4,9], [1,7,9]))
                
            SciPy Distances:
                Euclidean Distance .euclidean()     # e.g. print(distance.euclidean([1, 2], [4, 0]))
                Manhattan Distance .cityblock()
                Hamming Distance .hamming()         # the scipy implementation of Hamming distance will always return a number between 0 an 1. Rather than summing the number of differences in dimensions, this implementation sums those differences and then divides by the total number of dimensions. 
            
            Normalization
                > Min-max normalization is one of the most common ways to normalize data. For every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1.
                    Guarantees all features will have the exact same scale but does not handle outliers well.
                    Formula: (value - min) / (max - min)
                    Min-max normalization has one fairly significant downside: it does not handle outliers very well. 
                > Z-score normalization is a strategy of normalizing data that avoids this outlier issue.
                    Handles outliers, but does not produce normalized data with the exact same scale.
                    Formula: (value - μ) / σ    # where μ is the mean value of the feature and σ is the standard deviation of the feature.
                    If a value is exactly equal to the mean of all the values of the feature, it will be normalized to 0. 
                    If it is below the mean, it will be a negative number, and if it is above the mean it will be a positive number. 
                    The size of those negative and positive numbers is determined by the standard deviation of the original feature. 
                    If the unnormalized data had a large standard deviation, the normalized values will be closer to 0.
            Training Set vs Validation Set vs Test Set
                In order to test the effectiveness of your algorithm, we’ll split this data into:
                    - training set      - is the data that the algorithm will learn from. 
                    - validation set
                    - test set
                Validation error might not be the only metric we’re interested in. A better way of judging the effectiveness of a ML algorithm is to compute its precision, recall, and F1 score.
                How to Split - Figuring out how much of your data should be split into your validation set is a tricky question. If your training set is too small, then your algorithm might not have enough data to effectively learn. On the other hand, if your validation set is too small, then your accuracy, precision, recall, and F1 score could have a large variance. You might happen to get a really lucky or a really unlucky split!
                N-Fold Cross-Validation - Sometimes your dataset is so small, that splitting it 80/20 will still result in a large amount of variance. One solution to this is to perform N-Fold Cross-Validation. The central idea here is that we’re going to do this entire process N times and average the accuracy. 
                Changing The Model / Test Set 
        
        K-NEAREST NEIGHBORS (KNN)  
            KNN Classifier
                The central idea of KNN is that data points with similar attributes tend to fall into similar categories.
                If you have a dataset of points where the class of each point is known, you can take a new point with an unknown class, find it’s nearest neighbors, and classify it.
            Distance Between Points - 2D
                Example:
                    star_wars = [125, 1977]
                    raiders = [115, 1981]
                    mean_girls = [97, 2004]

                    def distance(movie1,movie2):
                        distance =0 
                        length_difference = (movie1[0] - movie2[0]) ** 2
                        year_difference = (movie1[1] - movie2[1]) ** 2
                        distance = (length_difference + year_difference) ** 0.5
                        return distance

                    print(distance(star_wars, raiders))
                    print(distance(star_wars, mean_girls))

            Distance Between Points - 3D
                Example:
                    star_wars = [125, 1977, 11000000]
                    raiders = [115, 1981, 18000000]
                    mean_girls = [97, 2004, 17000000]

                    def distance(movie1, movie2):
                        squared_difference = 0
                        for i in range(len(movie1)):
                            squared_difference += (movie1[i] - movie2[i]) ** 2
                        final_distance = squared_difference ** 0.5
                        return final_distance

                    print(distance(star_wars, raiders))
                    print(distance(star_wars, mean_girls))
            
            The K-Nearest Neighbor Algorithm:
                0 - Normalize the data
                1 - Find the k nearest neighbors
                2 - Classify the new point based on those neighbors

            0 - Data with Different Scales: Normalization
                Example:
                    release_dates = [1897.0, 1998.0, 2000.0, 1948.0, 1962.0, 1950.0, 1975.0, 1960.0, 2017.0, 1937.0, 1968.0, 1996.0, 1944.0, 1891.0, 1995.0, 1948.0, 2011.0, 1965.0, 1891.0, 1978.0]
                    def min_max_normalize(lst):
                        minimum = min(lst)
                        maximum = max(lst)
                        normalized = []
                        for value in lst:
                            normalized_num = (value - minimum) / (maximum - minimum)
                            normalized.append(normalized_num)
                        return normalized
                    print(min_max_normalize(release_dates))
            
            1 - Finding the Nearest Neighbors  
                Example:
                    from movies import movie_dataset, movie_labels
                    #print(movie_dataset['Bruce Almighty'])
                    #print(movie_labels['Bruce Almighty'])

                    def distance(movie1, movie2):
                        squared_difference = 0
                        for i in range(len(movie1)):
                            squared_difference += (movie1[i] - movie2[i]) ** 2
                        final_distance = squared_difference ** 0.5
                        return final_distance

                    def classify(unknown, dataset, k):
                        distances = []
                        for title in dataset:                               #Looping through all points in the dataset
                            movie = dataset[title]
                            distance_to_point = distance(movie, unknown)
                            distances.append([distance_to_point, title])    #Adding the distance and point associated with that distance
                        distances.sort()
                        neighbors = distances[0:k]                          #Taking only the k closest points
                        return neighbors

                    print(classify([.4, .2, .9], movie_dataset, 5))         # Take a look at the 5 nearest neighbors. 
            
            3 - Count Neighbors
                Example: (modify classify fn from above)
                    def classify(unknown, dataset, labels, k):          # Add a parameter named labels
                        distances = []
                        #Looping through all points in the dataset
                        for title in dataset:
                            movie = dataset[title]
                            distance_to_point = distance(movie, unknown)
                            #Adding the distance and point associated with that distance
                            distances.append([distance_to_point, title])
                        distances.sort()
                        #Taking only the k closest points
                        neighbors = distances[0:k]
                        num_good = 0                                    # Create two variables named num_good and num_bad and set them each at 0.
                        num_bad = 0
                        for neighbor in neighbors:                      # Use a for loop to loop through every movie in neighbors.
                            title = neighbor[1]
                            if labels[title] == 0:
                            num_bad += 1
                            elif labels[title] == 1:
                            num_good += 1
                        if num_good > num_bad:
                            return 1
                        else:
                            return 0
                    print(classify([.4, .2, .9], movie_dataset, movie_labels, 5))
            
            Classify Your Favorite Movie
                Example: (include fns from above)
                    print("Call Me By Your Name" in movie_dataset)
                    my_movie = [3500000, 132, 2017]
                    normalized_my_movie = normalize_point(my_movie)     # from movies import movie_dataset, movie_labels, normalize_point
                    print(normalized_my_movie)
                    print(classify(normalized_my_movie, movie_dataset, movie_labels, 5))

            Training and Validation Sets
                Example:
                    print(validation_set["Bee Movie"])          # from movies import training_set, training_labels, validation_set, validation_labels
                    print(validation_labels["Bee Movie"])

                    guess=classify(validation_set["Bee Movie"], training_set, training_labels, 5)
                    print(guess)
                    if guess == validation_labels["Bee Movie"]:
                        print("Correct!")
                    else:
                        print("Wrong!")

            Choosing K
                Overfitting occurs when you rely too heavily on your training data; you assume that data in the real world will always behave exactly like your training data. In the case of K-Nearest Neighbors, overfitting happens when you don’t consider enough neighbors. A single outlier could drastically determine the label of an unknown point. 
                On the other hand, if k is very large, our classifier will suffer from underfitting. Underfitting occurs when your classifier doesn’t pay enough attention to the small quirks in the training set.
                Example:
                    # from movies import training_set, training_labels, validation_set, validation_labels
                    def find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, k):
                        num_correct = 0.0
                        for title in validation_set:
                            guess = classify(validation_set[title], training_set, training_labels, k)
                            if guess == validation_labels[title]:
                            num_correct += 1
                        return num_correct / len(validation_set)
                    print(find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, 3))
            
            Graph of K
            Using sklearn
                Rather than writing your own classifier every time, you can use Python’s sklearn library.
                
                classifier = KNeighborsClassifier(n_neighbors = 3)  # create a KNeighborsClassifier object, with one parameter - k, e.g. k=3.
                # train our classifier
                training_points = [ [0.5, 0.2, 0.1], [0.9, 0.7, 0.3], [0.4, 0.5, 0.7] ]
                training_labels = [0, 1, 1]
                classifier.fit(training_points, training_labels)
                # classify new points
                unknown_points = [[0.2, 0.1, 0.7],[0.4, 0.7, 0.6],[0.5, 0.8, 0.1]]
                guesses = classifier.predict(unknown_points)
        Project: Breast Cancer Classifier

    > K Nearest Neighbor Regression
        The K-Nearest Neighbor algorithm can be used for regression. Rather than returning a classification, it returns a number.
        Regression
            Instead of classifying a new movie as either good or bad, we are now going to predict its IMDb rating as a real number.
            This process is almost identical to classification, except for the final step. 
            Example:
                from movies import movie_dataset, movie_ratings
                def distance(movie1, movie2):
                    squared_difference = 0
                    for i in range(len(movie1)):
                        squared_difference += (movie1[i] - movie2[i]) ** 2
                    final_distance = squared_difference ** 0.5
                    return final_distance
                def predict(unknown, dataset, movie_ratings, k):
                    distances = []
                    for title in dataset:                               #Looping through all points in the dataset
                        movie = dataset[title]
                        distance_to_point = distance(movie, unknown)
                        distances.append([distance_to_point, title])    #Adding the distance and point associated with that distance
                    distances.sort()
                    neighbors = distances[0:k]                          #Taking only the k closest points
                    sum = 0
                    for neighbor in neighbors:
                        title = neighbor[1]
                        sum += movie_ratings[title]
                    return sum/len(neighbors)
                print(movie_dataset["Life of Pi"])
                print(movie_ratings["Life of Pi"])
                print(predict([0.016, 0.300, 1.022], movie_dataset, movie_ratings, 5))
        
        Weighted Regression
            # add code to predict fn after 'neighbors = distances[0:k]' row
                numerator = 0
                denominator = 0
                for neighbor in neighbors:
                    rating = movie_ratings[neighbor[1]]
                    distance_to_neighbor = neighbor[0]
                    numerator += rating / distance_to_neighbor
                    denominator += 1 / distance_to_neighbor
                return numerator / denominator
            print(predict([0.016, 0.300, 1.022], movie_dataset, movie_ratings, 5))
        
        Scikit-learn
            regressor = KNeighborsRegressor(n_neighbors = 3, weights = "distance")     # If weights equals "uniform", all neighbors will be considered equally in the average. If weights equals "distance", then a weighted average is used.
            # fit the model to our training data
            training_points = [[0.5, 0.2, 0.1],[0.9, 0.7, 0.3],[0.4, 0.5, 0.7]]
            training_labels = [5.0, 6.8, 9.0]
            regressor.fit(training_points, training_labels)
            # make predictions on new data points
            unknown_points = [[0.2, 0.1, 0.7],[0.4, 0.7, 0.6],[0.5, 0.8, 0.1]]
            guesses = regressor.predict(unknown_points)

    > Accuracy, Recall, and Precision
        Accuracy
            The simplest way of reporting the effectiveness of an algorithm is by calculating its accuracy. 
            Accuracy is calculated by finding the total number of correctly classified points and dividing by the total number of points.
            Accuracy = (True Positives + True Negatives) / (True Positives + True Negatives + False Positives + False Negatives)
                True Positive: The algorithm predicted you would get above a B, and you did.
                True Negative: The algorithm predicted you would get below a B, and you did.
                False Positive: The algorithm predicted you would get above a B, and you didn’t.
                False Negative: The algorithm predicted you would get below a B, and you didn’t.
            Example:
                labels = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]     # labels represents the true labels of your dataset. Each 1 represents a test that you got above a B on, and each 0 represents a test that was below a B.
                guesses = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]    # guesses represents the classifications a machine learning algorithm might return. For every test, the classifier guessed whether your grade was above or below a B.

                true_positives = 0
                true_negatives = 0
                false_positives = 0
                false_negatives = 0

                for i in range(len(guesses)):
                    #True Positives
                    if labels[i] == 1 and guesses[i] == 1:
                        true_positives += 1
                    #True Negatives
                    if labels[i] == 0 and guesses[i] == 0:
                        true_negatives += 1
                    #False Positives
                    if labels[i] == 0 and guesses[i] == 1:
                        false_positives += 1
                    #False Negatives
                    if labels[i] == 1 and guesses[i] == 0:
                        false_negatives += 1

                accuracy = (true_positives + true_negatives) / len(guesses)     # len(guesses) is the same as (True Positives + True Negatives + False Positives + False Negatives)
                print(accuracy)

        Recall
            Recall measures the percentage of relevant items that your classifier found.
            Recall = True Positives / (True Positives + False Negatives)
        
        Precision
            Precision = True Positives / (True Positives + False Positives)
            Precision and recall are statistics that are on opposite ends of a scale. If one goes down, the other will go up.

        F1 Score
            We still don’t have one number that can sufficiently describe how effective our algorithm is. This is the job of the F1 score.
            F1 score is the harmonic mean of precision and recall. 
            F1 = 2 * (Precision * Recall)/(Precision + Recall)
        
        Python library scikit-learn fns:
            Example:
                from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
                labels = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]
                guesses = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]
                
                print(accuracy_score(labels, guesses))
                print(recall_score(labels, guesses))
                print(precision_score(labels, guesses))
                print(f1_score(labels, guesses))

        The Dangers of Overfitting
            Overfitting occurs when we have fit our model’s parameters too closely to the training data.
            When we overfit, we are assuming that everything we see in the training data is exactly how it will appear in the real world. 
            Instead, we want to be modeling trends that show us the general behavior of a variable.
            Some people say that ML can be a GIGO process — Garbage In, Garbage Out.
            Example: We can imagine an example where an ad agency is creating an algorithm to display the right job recommendations to the right people. If they use a training set of the kinds of people who have high paying jobs to determine which people to show ads for high paying jobs to, the model will probably learn to make decisions that leave out historically underrepresented groups of people.
            How do we tackle this problem?
                Inspect Training Data First
                Collect Data Thoughtfully
                Try to Augment the Training Data
                Try Restricting the Featureset
                Reflection

    > Logistic Regression
        Logistic Regression is a supervised machine learning algorithm that uses regression to predict the continuous probability, ranging from 0 to 1, of a data sample belonging to a specific category, or class. Then, based on that probability, the sample is classified as belonging to the more probable class, ultimately making Logistic Regression a classification algorithm.
        Examples: 
            - Spam filtering
                We would call spam the 'positive class', with the label 1, since the positive class is the class our model is looking to detect. 
                If the predicted probability is less than 0.5, the email is classified as ham (a real email). We would call ham the 'negative class', with the label 0. 
                This act of deciding which of two classes a data sample belongs to is called 'binary classification'.
            - Disease survival —Will a patient, 5 years after treatment for a disease, still be alive?
            - Customer conversion —Will a customer arriving on a sign-up page enroll in a service?
        
        Linear Regression Approach
            # The output of a Linear Regression model does not provide the probabilities we need to predict whether a student passes the final exam.
        Logistic Regression
            To predict the probability of a data sample belonging to a class, we:
                - initialize all feature coefficients and intercept to 0
                - multiply each of the feature coefficients by their respective feature value to get what is known as the log-odds
                - place the log-odds into the sigmoid function to link the output to the range [0,1], giving us a probability
            By comparing the predicted probabilities to the actual classes of our data points, we can evaluate how well our model makes predictions and use gradient descent to update the coefficients and find the best ones for our model.
            To then make a final classification, we use a classification threshold to determine whether the data sample belongs to the positive class or the negative class.
        Log-Odds
            In Linear Regression we multiply the coefficients of our features by their respective feature values and add the intercept, resulting in our prediction, which can range from -∞ to +∞. 
            In Logistic Regression, we make the same multiplication of feature coefficients and feature values and add the intercept, but instead of the prediction, we get what is called the log-odds.

            The log-odds are another way of expressing the probability of a sample belonging to the positive class, or a student passing the exam.
            Odds = P(event occuring) / P(event not occuring)
            E.g.  If a student will pass the exam with probability 0.7, they will fail with probability 1 - 0.7 = 0.3. 
                Odds for passing = 0.7 / 0.3 = 2.33
                Log odds of passing = log(2.33) = 0.847

            log_odds = np.dot(features, coefficients) + intercept

            Example:
                import numpy as np
                from exam import hours_studied, calculated_coefficients, intercept
                
                def log_odds(features,coefficients,intercept):          # Create log_odds() fn
                    return np.dot(features,coefficients) + intercept
                
                calculated_log_odds = log_odds(hours_studied,calculated_coefficients,intercept)     # Calculate the log-odds for the Codecademy University data
                print(calculated_log_odds)
                print(hours_studied)

        Sigmoid Function
            The Sigmoid Function is a special case of the more general Logistic Function, where Logistic Regression gets its name. Why is the Sigmoid Function so important? By plugging the log-odds into the Sigmoid Function, defined below, we map the log-odds z to the range [0,1].
                h(z) = 1 / (1 + e^(-z))     # e^(-z) - written in numpy as np.exp(-z)
            Example:
                import numpy as np
                from exam import calculated_log_odds
                
                def sigmoid(z):                         # Create sigmoid fn
                    denominator = 1 + np.exp(-z)
                    return 1/denominator
                
                probabilities = sigmoid(calculated_log_odds)        # Calculate the sigmoid of the log-odds
                print(probabilities)

        Log-Loss
            The function used to evaluate the performance of a ML model is called a 'loss function', or a 'cost function'. 
            To evaluate how “good a fit” a model is, we calculate the loss for each data sample (how wrong the model’s prediction was) and then average the loss across all samples. 
            The loss function for Logistic Regression, known as 'Log Loss'
                - 1 / m (sum of [y^i*log(h*z^i + (1-y^i)log(1-h(z^i)))])
                    m       - total number of data samples
                    y_i     - class of data sample i
                    z_i     - log-odds of sample i
                    h(z_i)  - sigmoid of the log-odds of sample i, which is the probability of sample i belonging to the positive class
            Example:
                import numpy as np
                from exam import passed_exam, probabilities, probabilities_2
                
                def log_loss(probabilities,actual_class):           # Function to calculate log-loss
                    return np.sum(-(1/actual_class.shape[0])*(actual_class*np.log(probabilities) + (1-actual_class)*np.log(1-probabilities)))
                print(passed_exam)                              # Print passed_exam
                
                loss_1 = log_loss(probabilities,passed_exam)    # Calculate and print loss_1
                print(loss_1)

                loss_2 = log_loss(probabilities_2,passed_exam)  # Calculate and print loss_2
                print(loss_2)

        Classification Thresholding
            The default threshold for many algorithms is 0.5. 
            If the predicted probability of an observation belonging to the positive class is greater than or equal to the threshold, 0.5, the classification of the sample is the positive class. 
            If the predicted probability of an observation belonging to the positive class is less than the threshold, 0.5, the classification of the sample is the negative class.
            Example:
                import numpy as np
                from exam import hours_studied, calculated_coefficients, intercept

                def log_odds(features, coefficients,intercept):
                  return np.dot(features,coefficients) + intercept

                def sigmoid(z):
                    denominator = 1 + np.exp(-z)
                    return 1/denominator

                def predict_class(features,coefficients,intercept,threshold):       # Create predict_class() fn
                  calculated_log_odds = log_odds(features,coefficients,intercept)
                  probabilities = sigmoid(calculated_log_odds)
                  return np.where(probabilities >= threshold, 1, 0)

                final_results = predict_class(hours_studied, calculated_coefficients, intercept, 0.5)   # Make final classifications on Codecademy University data
                print(final_results)

        Scikit-Learn
            model = LogisticRegression()        # creating the object
            model.fit(features, labels)         # We train — or fit — the model using the .fit() method, which takes two parameters. The first is a matrix of features, and the second is a matrix of class labels.
            # access a few useful attributes of the LogisticRegression object.
            #   model.coef_ is a vector of the coefficients of each feature
            #   model.intercept_ is the intercept b_0
            model.predict(features)             # .predict() takes a matrix of features as a parameter and returns a vector of labels 1 or 0 for each sample. In making its predictions, sklearn uses a classification threshold of 0.5.
            model.predict_proba(features)       # If we are more interested in the predicted probability of the data samples belonging to the positive class than the actual class. predict_proba() also takes a matrix of features as a parameter and returns a vector of probabilities, ranging from 0 to 1, for each sample.
            # Note: sklearn‘s Logistic Regression implementation requires feature data to be normalized.
            # sklearn‘s Logistic Regression requires normalized feature data due to a technique called Regularization that it uses under the hood.

            Example:
                import numpy as np
                from sklearn.linear_model import LogisticRegression
                from exam import hours_studied_scaled, passed_exam, exam_features_scaled_train, exam_features_scaled_test, passed_exam_2_train, passed_exam_2_test, guessed_hours_scaled

                # Create and fit logistic regression model here
                model = LogisticRegression()
                model.fit(hours_studied_scaled,passed_exam)

                # Save the model coefficients and intercept here
                calculated_coefficients = model.coef_
                intercept = model.intercept_

                print(calculated_coefficients)
                print(intercept)

                # Predict the probabilities of passing for next semester's students here
                passed_predictions = model.predict_proba(guessed_hours_scaled)

                # Create a new model on the training data with two features here
                model_2 = LogisticRegression()
                model_2.fit(exam_features_scaled_train,passed_exam_2_train)

                # Predict whether the students will pass here
                passed_predictions_2 = model_2.predict(exam_features_scaled_test)
                print(passed_predictions_2)
                print(passed_exam_2_test)
        
        Feature Importance
            Features with larger, positive coefficients will increase the probability of a data sample belonging to the positive class
            Features with larger, negative coefficients will decrease the probability of a data sample belonging to the positive class
            Features with small, positive or negative coefficients have minimal impact on the probability of a data sample belonging to the positive class

            Example:
                import numpy as np
                import matplotlib.pyplot as plt
                from sklearn.linear_model import LogisticRegression
                from exam import exam_features_scaled, passed_exam_2

                # Train a sklearn logistic regression model on the normalized exam data
                model_2 = LogisticRegression()
                model_2.fit(exam_features_scaled,passed_exam_2)

                # Assign and update coefficients
                coefficients = model_2.coef_
                coefficients = coefficients.tolist()[0]

                # Plot bar graph
                plt.bar([1,2],coefficients)
                plt.xticks([1,2],['hours studied','math courses taken'])
                plt.xlabel('feature')
                plt.ylabel('coefficient')

                plt.show()
        Project: Predict Titanic Survival

    > Support Vector Machines
        A Support Vector Machine (SVM) is a powerful supervised machine learning model used for classification. 
        An SVM makes classifications by defining a decision boundary and then seeing what side of the boundary an unclassified point falls on.
            Decision boundaries are easiest to wrap your head around when the data has two features. In this case, the decision boundary is a line.
            If there are three features, the decision boundary is now a plane rather than a line.
            As the number of dimensions grows past 3, it becomes very difficult to visualize these points in space. Nonetheless, SVMs can still find a decision boundary. However, rather than being a separating line, or a separating plane, the decision boundary is called a separating hyperplane.
        
        Optimal Decision Boundaries
            Move decision boundaries if they are too close to the training data!    e.g. change intercept_two value and/or slope_two.
            Example:
                import matplotlib.pyplot as plt
                import numpy as np
                from graph import ax, x_1, y_1, x_2, y_2
                
                #Top graph intercept and slope
                intercept_one = 98
                slope_one = -20
                
                x_vals = np.array(ax.get_xlim())
                y_vals = intercept_one + slope_one * x_vals
                plt.plot(x_vals, y_vals, '-')
                
                #Bottom graph
                ax = plt.subplot(2, 1, 2)
                plt.title('Good Decision Boundary')
                ax.set_xlim(0, 10)
                ax.set_ylim(0, 10)
                
                plt.scatter(x_1, y_1, color = "b")
                plt.scatter(x_2, y_2, color = "r")
                
                #Bottom graph intercept and slope
                intercept_two = 8
                slope_two = -0.5
                
                x_vals = np.array(ax.get_xlim())
                y_vals = intercept_two + slope_two * x_vals
                plt.plot(x_vals, y_vals, '-')
                
                plt.tight_layout()
                plt.show()
        
        Support Vectors and Margins
            The support vectors are the points in the training set closest to the decision boundary. In fact, these vectors are what define the decision boundary.
            If you are using n features, there are at least n+1 support vectors.
            The distance between a support vector and the decision boundary is called the margin. We want to make the margin as large as possible. 
            SVMs are fast because they only use the support vectors!
        
        scikit-learn
            To use scikit-learn’s SVM we first need to create an SVC object. It is called an SVC because scikit-learn is calling the model a Support Vector Classifier rather than a Support Vector Machine.
                classifier = SVC(kernel = 'linear')
                # now the model needs to be trained on a list of data points and a list of labels associated with those data points. 
                training_points = [[1, 2], [1, 5], [2, 2], [7, 5], [9, 4], [8, 2]]
                labels = [1, 1, 1, 0, 0, 0]
                classifier.fit(training_points, labels)     # Calling .fit() creates the line between the points.
                print(classifier.predict([[3, 2]]))
                print(classifier.support_vectors_)          # to see which points from the training set are the support vectors.
        
        Outliers
            SVMs have a parameter C that determines how much error the SVM will allow for. 
                If C is large, then the SVM has a hard margin — it won’t allow for many misclassifications, and as a result, the margin could be fairly small. If C is too large, the model runs the risk of overfitting. It relies too heavily on the training data, including the outliers.
                If C is small, the SVM has a soft margin. Some points might fall on the wrong side of the line, but the margin will be large. This is resistant to outliers, but if C gets too small, you run the risk of underfitting. The SVM will allow for so much error that the training data won’t be represented.
            When using scikit-learn’s SVM, you can set the value of C when you create the object:
                classifier = SVC(C = 0.01)
            The optimal value of C will depend on your data. 
                Don’t always maximize margin size at the expense of error. 
                Don’t always minimize error at the expense of margin size. 
                The best strategy is to validate your model by testing many different values for C.
        
        Kernels
            Example:
                from sklearn.svm import SVC
                from graph import points, labels
                from sklearn.model_selection import train_test_split

                training_data, validation_data, training_labels, validation_labels = train_test_split(points, labels, train_size = 0.8, test_size = 0.2, random_state = 100)

                classifier = SVC(kernel = "poly", degree = 2)
                classifier.fit(training_data, training_labels)
                print(classifier.score(validation_data, validation_labels))

        Polynomial Kernel
            Example:
                from sklearn.datasets import make_circles
                from sklearn.svm import SVC
                from sklearn.model_selection import train_test_split

                #Makes concentric circles
                points, labels = make_circles(n_samples=300, factor=.2, noise=.05, random_state = 1)

                #Makes training set and validation set.
                training_data, validation_data, training_labels, validation_labels = train_test_split(points, labels, train_size = 0.8, test_size = 0.2, random_state = 100)

                classifier = SVC(kernel = "linear", random_state = 1)
                classifier.fit(training_data, training_labels)
                print(classifier.score(validation_data, validation_labels))
                print(training_data[0])

                # Let's transform the data into three dimensions!
                new_training = [[2 ** 0.5 * pt[0] * pt[1], pt[0] ** 2, pt[1] ** 2] for pt in training_data]
                new_validation = [[2 ** 0.5 * pt[0] * pt[1], pt[0] ** 2, pt[1] ** 2] for pt in validation_data]

                # Retrain classifier
                classifier.fit(new_training, training_labels)
                print(classifier.score(new_validation, validation_labels))
        
        Radial Bias Function Kernel
            The most commonly used kernel in SVMs is a radial basis function (rbf) kernel. 
            This is the default kernel used in scikit-learn’s SVC object. If you don’t specifically set the kernel to "linear", "poly" the SVC object will use an rbf kernel. If you want to be explicit, you can set kernel = "rbf", although that is redundant.
            The polynomial kernel we used transformed two-dimensional points into three-dimensional points. 
            An rbf kernel transforms two-dimensional points into points with an infinite number of dimensions!
                classifier = SVC(kernel = "rbf", gamma = 0.5, C = 2)
            gamma is similar to the C parameter. You can essentially tune the model to be more or less sensitive to the training data. A higher gamma, say 100, will put more importance on the training data and could result in overfitting. Conversely, A lower gamma like 0.01 makes the points in the training data less relevant and can result in underfitting.

            Example:
                from data import points, labels
                from sklearn.model_selection import train_test_split
                from sklearn.svm import SVC

                training_data, validation_data, training_labels, validation_labels = train_test_split(points, labels, train_size = 0.8, test_size = 0.2, random_state = 100)

                classifier = SVC(kernel = "rbf", gamma = 0.1)
                classifier.fit(training_data, training_labels)
                print(classifier.score(validation_data, validation_labels))
        Project: Sports Vector Machine

    > Decision Trees
        Decision trees are ML models that try to find patterns in the features of data points.
        Making Decision Trees
            Decision trees are supervised ML models, which means that they’re created from a training set of labeled data. Creating the tree is where the learning in machine learning happens.
        Cars
            When considering buying a car, what factors go into making that decision?
            Each car can fall into four different classes which represent how satisfied someone would be with purchasing the car — unacc (unacceptable), acc (acceptable), good, vgood.
            Each car has 6 features:
                - The price of the car which can be "vhigh", "high", "med", or "low".
                - The cost of maintaining the car which can be "vhigh", "high", "med", or "low".
                - The number of doors which can be "2", "3", "4", "5more".
                - The number of people the car can hold which can be "2", "4", or "more".
                - The size of the trunk which can be "small", "med", or "big".
                - The safety rating of the car which can be "low", "med", or "high".
            
            from tree import tree, classify, data
            car = ["low", "low", "4", "4", "big", "high"]
            print(classify(car, tree))
        
        Gini Impurity
            To find the Gini impurity, start at 1 and subtract the squared percentage of each label in the set. 
            e.g, if a data set had three items of class A and one item of class B, the Gini impurity of the set would be
                1 - (3/4)^2 - (1/4)^2 = 0.375
            If a data set has only one class, you’d end up with a Gini impurity of 0. The lower the impurity, the better the decision tree!
            Example:
                from collections import Counter
                labels = ["unacc", "unacc", "acc", "acc", "good", "good"]
                #labels = ["unacc","unacc","unacc", "good", "vgood", "vgood"]
                #labels = ["unacc", "unacc", "unacc", "unacc", "unacc", "unacc"]

                impurity = 1
                label_counts = Counter(labels)
                for label in label_counts:
                  probability_of_label = label_counts[label]/len(labels)
                  impurity -= probability_of_label ** 2

                print(impurity)
        
        Information Gain
            Information gain measures difference in the impurity of the data before and after the split. 
            e.g. 
                let’s say you had a dataset with an impurity of 0.5. 
                After splitting the data based on a feature, you end up with three groups with impurities 0, 0.375, and 0. 
                The information gain of splitting the data in that way is 0.5 - 0 - 0.375 - 0 = 0.125.
            The higher the information gain the better — if information gain is 0, then splitting the data on that feature was useless!
        
            Example:
                from collections import Counter

                unsplit_labels = ["unacc", "unacc", "unacc", "unacc", "unacc", "unacc", "good", "good", "good", "good", "vgood", "vgood", "vgood"]

                split_labels_1 = [
                  ["unacc", "unacc", "unacc", "unacc", "unacc", "unacc", "good", "good", "vgood"], 
                  [ "good", "good"], 
                  ["vgood", "vgood"]
                ]

                split_labels_2 = [
                  ["unacc", "unacc", "unacc", "unacc","unacc", "unacc", "good", "good", "good", "good"], 
                  ["vgood", "vgood", "vgood"]
                ]

                def gini(dataset):
                  impurity = 1
                  label_counts = Counter(dataset)
                  for label in label_counts:
                    prob_of_label = label_counts[label] / len(dataset)
                    impurity -= prob_of_label ** 2
                  return impurity

                info_gain = gini(unsplit_labels)
                for subset in split_labels_1:
                  info_gain -= gini(subset)

                print(info_gain)

        Weighted Information Gain
            Let’s modify the formula for information gain to reflect the fact that the size of the set is relevant. 
            Instead of simply subtracting the impurity of each set, we’ll subtract the weighted impurity of each of the split sets. 
            If the data before the split contained 20 items and one of the resulting splits contained 2 items, then the weighted impurity of that subset would be 2/20 * impurity. 
            We’re lowering the importance of the impurity of sets with few elements.
        
        Recursive Tree Building
            We’ll stop the recursion when we can no longer find a feature that results in any information gain. 
            In other words, we want to create a leaf of the tree when we can’t find a way to split the data that makes purer subsets.
            Example:
                from tree import *

                car_data = [['med', 'low', '3', '4', 'med', 'med'], ['med', 'vhigh', '4', 'more', 'small', 'high'], ['high', 'med', '3', '2', 'med', 'low'], ['med', 'low', '4', '4', 'med', 'low'], ['med', 'low', '5more', '2', 'big', 'med'], ['med', 'med', '2', 'more', 'big', 'high'], ['med', 'med', '2', 'more', 'med', 'med'], ['vhigh', 'vhigh', '2', '2', 'med', 'low'], ['high', 'med', '4', '2', 'big', 'low'], ['low', 'low', '2', '4', 'big', 'med']]
                car_labels = ['acc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'unacc', 'unacc', 'good']

                def find_best_split(dataset, labels):
                    best_gain = 0
                    best_feature = 0
                    for feature in range(len(dataset[0])):
                        data_subsets, label_subsets = split(dataset, labels, feature)
                        gain = information_gain(labels, label_subsets)
                        if gain > best_gain:
                            best_gain, best_feature = gain, feature
                    return best_feature, best_gain

                def build_tree(data, labels):
                  best_feature, best_gain = find_best_split(data, labels)
                  if best_gain == 0:
                    return Counter(labels)
                  data_subsets, label_subsets = split(data, labels, best_feature)
                  branches = []
                  for i in range(len(data_subsets)):
                    branch = build_tree(data_subsets[i], label_subsets[i])
                    branches.append(branch)
                  return branches

                tree = build_tree(car_data, car_labels)
                print_tree(tree) 

        Classifying New Data
            We’ve slightly changed the way our build_tree() function works. 
            Instead of returning a list of branches or a Counter object, the build_tree() function now returns a Leaf object or an Internal_Node object.
            Example:
                from tree import *
                import operator

                test_point = ['vhigh', 'low', '3', '4', 'med', 'med']

                def classify(datapoint, tree):
                    if isinstance(tree, Leaf):
                        return max(tree.labels.items(), key=operator.itemgetter(1))[0]
                    
                    value = datapoint[tree.feature]
                    for branch in tree.branches:
                        if branch.value == value:
                        return classify(datapoint, branch)

                print(classify(test_point, tree))

        Decision Trees in scikit-learn
            classifier = DecisionTreeClassifier()               # create a DecisionTreeClassifier object
            classifier.fit(training_data, training_labels)      # create the tree based on our training data
            predictions = classifier.predict(test_data)
            print(classifier.score(test_data, test_labels))     # find the accuracy of the model by calling the .score() method
                                                                # .score() returns the percentage of data points from the test set that it classified correctly.
        Decision Tree Limitations
            Problems with trees:
            - trees aren’t always globablly optimal.  -> there might be a better tree out there somewhere that produces better results.
                Our current strategy of creating trees is greedy. We assume that the best way to create a tree is to find the feature that will result in the largest information gain right now and split on that feature. We never consider the ramifications of that split further down the tree. It’s possible that if we split on a suboptimal feature right now, we would find even better splits later on. Unfortunately, finding a globally optimal tree is an extremely difficult task, and finding a tree using our greedy approach is a reasonable substitute.
            - trees is that they potentially overfit the data. -> the structure of the tree is too dependent on the training data and doesn’t accurately represent the way the data in the real world looks like. In general, larger trees tend to overfit the data more. As the tree gets bigger, it becomes more tuned to the training data and it loses a more generalized understanding of the real world data.
                One way to solve this problem is to prune the tree. The goal of pruning is to shrink the size of the tree. There are a few different pruning strategies, and we won’t go into the details of them here. scikit-learn currently doesn’t prune the tree by default, however we can dig into the code a bit to prune it ourselves.

        RANDOM FORESTS
            We’ve seen that decision trees can be powerful supervised machine learning models. However, they’re not without their weaknesses — decision trees are often prone to overfitting.
            We’ve discussed some strategies to minimize this problem, like pruning, but sometimes that isn’t enough. We need to find another way to generalize our trees. This is where the concept of a random forest comes in handy.
            A random forest is an ensemble ML technique — a random forest contains many decision trees that all work together to classify new points. When a random forest is asked to classify a new point, the random forest gives that point to each of the decision trees. Each of those trees reports their classification and the random forest returns the most popular classification. It’s like every tree gets a vote, and the most popular classification wins.

            Bagging
                Random forests create different trees using a process known as bagging. Every time a decision tree is made, it is created using a different subset of the points in the training set. 
                For example, if our training set had 1000 rows in it, we could make a decision tree by picking 100 of those rows at random to build the tree. 
                Example:
                    from tree import build_tree, print_tree, car_data, car_labels
                    import random
                    random.seed(4)
                    
                    #tree = build_tree(car_data, car_labels)
                    #print_tree(tree)
                    
                    # implement bagging:
                    indices = [random.randint(0, 999) for i in range(1000)]
                    
                    data_subset = [car_data[index] for index in indices]
                    labels_subset = [car_labels[index] for index in indices]
                    
                    # Create a tree
                    subset_tree = build_tree(data_subset, labels_subset)
                    print_tree(subset_tree)

                Syntax - for loop:
                    data_subset = []
                    labels_subset = []
                    for index in indices:
                        data_subset.append(car_data[index])
                        labels_subset.append(_____)
                Syntax - list comprehension:
                    data_subset = [car_data[index] for index in indices]
                    labels_subset = [_____]

            Bagging Features
                Example:
                    from tree import car_data, car_labels, split, information_gain
                    import random
                    import numpy as np
                    np.random.seed(1)
                    random.seed(4)

                    def find_best_split(dataset, labels):
                        best_gain = 0
                        best_feature = 0
                        #Create features here
                        features = np.random.choice(len(dataset[0]), 3, replace=False)
                        for feature in features:
                            data_subsets, label_subsets = split(dataset, labels, feature)
                            gain = information_gain(labels, label_subsets)
                            if gain > best_gain:
                                best_gain, best_feature = gain, feature
                        return best_gain, best_feature

                    indices = [random.randint(0, 999) for i in range(1000)]

                    data_subset = [car_data[index] for index in indices]
                    labels_subset = [car_labels[index] for index in indices]
                    print(find_best_split(data_subset, labels_subset))

            Classify
                Example:
                    from tree import build_tree, print_tree, car_data, car_labels, classify
                    from collections import Counter
                    import random
                    random.seed(4)

                    # The features are the price of the car, the cost of maintenance, the number of doors, the number of people the car can hold, the size of the trunk, and the safety rating
                    unlabeled_point = ['high', 'vhigh', '3', 'more', 'med', 'med']

                    predictions = []
                    for i in range(20):
                        indices = [random.randint(0, 999) for i in range(1000)]
                        data_subset = [car_data[index] for index in indices]
                        labels_subset = [car_labels[index] for index in indices]
                        subset_tree = build_tree(data_subset, labels_subset)
                        predictions.append(classify(unlabeled_point, subset_tree))
                    final_prediction = max(predictions, key=predictions.count)
                    print(final_prediction)
            
            Test Set
                Example:
                    from tree import training_data, training_labels, testing_data, testing_labels, make_random_forest, make_single_tree, classify
                    import numpy as np
                    import random
                    np.random.seed(1)
                    random.seed(1)
                    from collections import Counter

                    tree = make_single_tree(training_data, training_labels)
                    single_tree_correct = 0

                    forest = make_random_forest(40, training_data, training_labels)
                    forest_correct = 0

                    for i in range(len(testing_data)):
                        prediction = classify(testing_data[i], tree)
                        if prediction == testing_labels[i]:
                            single_tree_correct += 1
                        predictions = []
                        for forest_tree in forest:
                            predictions.append(classify(testing_data[i], forest_tree))
                        forest_prediction = max(predictions,key=predictions.count)
                        if forest_prediction == testing_labels[i]:
                            forest_correct += 1

                    print(single_tree_correct/len(testing_data))
                    print(forest_correct/len(testing_data))
            
            Random Forest in Scikit-learn
                classifier = RandomForestClassifier(n_estimators = 100)
                Example:
                    def warn(*args, **kwargs):
                        pass
                    import warnings
                    warnings.warn = warn
                    from cars import training_points, training_labels, testing_points, testing_labels
                    import warnings
                    from sklearn.ensemble import RandomForestClassifier

                    classifier = RandomForestClassifier(n_estimators = 2000, random_state = 0)  # If you don’t include 'n_estimators' parameter, the default is 10. 'n_estimators' represents the number of trees in the forest.
                    classifier.fit(training_points, training_labels)        # Train the forest using the training data
                    print(classifier.score(testing_points, testing_labels)) # Test the random forest on the testing set and print the results.
            
    > Classification: Naive Bayes
        BAYES' THEOREM
            Bayes’ Theorem is the basis of a branch of statistics called Bayesian Statistics.
            British mathematician Alan Turing used it to crack the German Enigma code during WWII. And now it is used in: ML, Statistical Modeling, A/B Testing, Robotics.

            Independent Events
                If two events are independent, then the occurrence of one event does not affect the probability of the other event.
                If two events are dependent, then when one event occurs, the probability of the other event occurring changes in a predictable way.
            Conditional Probability
                Conditional probability is the probability that two events happen. It’s easiest to calculate conditional probability when the two events are independent.
                If the probability of event A is P(A) and the probability of event B is P(B) and the two events are independent, then the probability of both events occurring is the product of the probabilities:
                    P(A∩B)=P(A)×P(B)    # The symbol ∩ just means “and”, so P(A ∩ B) means the probability that both A and B happen.
            Testing for a Rare Disease
            Bayes' Theorem
                That extra information about how we expect the world to work is called a 'prior'.
                When we only use the first piece of information (the result of the test), it’s called a 'Frequentist Approach' to statistics. 
                When we incorporate our prior, it’s called a 'Bayesian Approach'.
                P(A∣B)= P(B∣A)⋅P(A)​ / P(B)     # P(B|A). This is the probability that event B will happen given that event A has already happened. This is very different from P(A|B), which is the probability we are trying to solve for. The order matters!
                    P(A|B) means The probability of event A given that B is true.
            Spam Filters
                Example:
                    import numpy as np

                    a = 'spam'
                    b = 'enhancement'

                    p_spam = 0.2
                    p_enhancement_given_spam = 0.05
                    p_enhancement = 0.05 * 0.2 + 0.001 * (1 - 0.2)
                    p_spam_enhancement = p_enhancement_given_spam * p_spam / p_enhancement

                    print p_spam_enhancement

        NAIVE BAYES CLASSIFIER
            A Naive Bayes classifier is a supervised ML algorithm that leverages Bayes’ Theorem to make predictions and classifications. 
            P(A ∣ B) is finding the probability of A given B. But can be turned into a classifier if we replace B with a data point and A with a class. 
            For example, let’s say we’re trying to classify an email as either spam or not spam. We could calculate P(spam | email) and P(not spam | email). Whichever probability is higher will be the classifier’s prediction. Naive Bayes classifiers are often used for text classification.

            Investigate the Data
                e.g. create a Naive Bayes classifier that can predict whether a review for a product is positive or negative. 
            Bayes Theorem
                Example:
                    # Formula: P(positive ∣ review)= P(review | positive)⋅P(positive) / P(review)
                    from reviews import neg_list, pos_list, neg_counter, pos_counter

                    total_reviews = len(pos_list) + len(neg_list)
                    percent_pos = len(pos_list) / total_reviews
                    percent_neg = len(neg_list) / total_reviews

                    print(percent_pos)
                    print(percent_neg)
                --------------------------------------
                    # Formula: P(positive | review) = P(review | positive)⋅P(positive) / P(review)
                    from reviews import neg_counter, pos_counter

                    review = "This crib was amazing"

                    percent_pos = 0.5
                    percent_neg = 0.5

                    total_pos = sum(pos_counter.values())
                    total_neg = sum(neg_counter.values())

                    pos_probability = 1
                    neg_probability = 1

                    review_words = review.split()

                    for word in review_words:
                      word_in_pos = pos_counter[word]
                      word_in_neg = neg_counter[word]

                      pos_probability *= word_in_pos / total_pos    # Smoothing: pos_probability *= (word_in_pos + 1) / (total_pos + len(pos_counter))
                      neg_probability *= word_in_neg / total_neg    # Smoothing: neg_probability *= (word_in_neg + 1) / (total_neg + len(neg_counter))    

                    print(pos_probability)
                    print(neg_probability)
​	         Smoothing
                we smooth by adding 1 to the numerator of each probability and N to the denominator of each probability. N is the number of unique words in our review dataset.
                For example, P("crib" | positive) goes from this:
                    P("crib" | positive) = # of “crib" in positive / # of words in positive
                To this:
                    P("crib" | positive) = # of “crib" in positive+1 / # of words in positive+N
            Classify
                Example: (add code to the end of above code)
                    final_pos = pos_probability * percent_pos
                    final_neg = neg_probability * percent_neg

                    if final_pos > final_neg:
                      print("The review is positive")
                    else:
                      print("The review is negative")
            
            Formatting the Data for scikit-learn
                vectorizer = CountVectorizer()
                vectorizer.fit(["Training review one", "Second review"])    
                # print(vectorizer.vocabulary_)
                counts = vectorizer.transform(["one review two review"])
                # print(counts.toarray())
            Using scikit-learn
                Example:
                    from reviews import counter, training_counts
                    from sklearn.feature_extraction.text import CountVectorizer
                    from sklearn.naive_bayes import MultinomialNB

                    review = "This crib was great amazing and wonderful"
                    review_counts = counter.transform([review])

                    classifier = MultinomialNB()
                    training_labels = [0] * 1000 + [1] * 1000

                    classifier.fit(training_counts, training_labels)
                    print(classifier.predict(review_counts))
                    print(classifier.predict_proba(review_counts))
            Project: Email Similarity

    > Artificial Intelligence Decision Making: Minimax
        MINIMAX
            Minimax algorithm - concept of thinking ahead.
            The minimax algorithm is a decision-making algorithm that is used for finding the best move in a two player game. It’s a recursive algorithm — it calls itself. In order for us to determine if making move A is a good idea, we need to think about what our opponent would do if we made that move.
            e.g. game tree of a Tic-Tac-Toe

            Tic-Tac-Toe
                check code in ML/Tic-Tac-Toe folder.
            Detecting Tic-Tac-Toe Leaves
                evaluation function: a leaf where player "X" wins evaluates to a 1, a leaf where player "O" wins evaluates to a -1, and a leaf that is a tie evaluates to 0.
            Evaluating Leaves
            Copying Boards
                One of the central ideas behind the minimax algorithm is the idea of exploring future hypothetical board states.
            The Minimax Function
            Recursion In Minimax
            Which Move?
            Play a Game
        ADVANCED MINIMAX
            Connect Four    (more complicated game)
                There are games, like Chess, that have much larger trees. There are 20 different options for the first move in Chess, compared to 9 in Tic-Tac-Toe.
                We’ll investigate two techniques to solve this problem:
                    - The first technique puts a hard limit on how far down the tree you allow the algorithm to go. 
                    - The second technique uses a clever trick called pruning to avoid exploring parts of the tree that we know will be useless.
                Depth and Base Case
                    By adding the depth parameter to our function, we’ve prevented it from spending days trying to reach the end of the tree. 
                Evaluation Function
                    We need to rewrite our evaluation function.
                Alpha-Beta Pruning
                Implement Alpha-Beta Pruning
                    Alpha-beta pruning is accomplished by keeping track of two variables for each node — alpha and beta. 
                        alpha keeps track of the minimum score the maximizing player can possibly get. It starts at negative infinity and gets updated as that minimum score increases.
                        beta represents the maximum score the minimizing player can possibly get. It starts at positive infinity and will decrease as that maximum possible score decreases.
                    For any node, if alpha is greater than or equal to beta, that means that we can stop looking through that node’s children.


************************************************************************************************
************************************************************************************************
************************************************************************************************

21. Supervised Machine Learning Cumulative Project
        Twitter Classification Cumulative Project
            In this project, you will use real tweets to find patterns in the way people use social media.
            There are two parts to this project that can be done in either order:
            - part 1: you will make a system that predicts whether or not a tweet will go viral by using a K-Nearest Neighbor classifier. What features of a tweet do you think are the most important in determining its virality? Does the length of the tweet matter? What about the number of hashtags? Maybe information about the account that sent the tweet is most important. You’ll answer these questions while using DataFrames and Matplotlib visualizations to present your results!
            - part 2: you’ll test the power of Naive Bayes classifiers by creating a system that predicts whether a tweet was sent from New York City, London, or Paris. You will investigate how language is used differently in these three cities. Can the classifier automatically detect the difference between French and English? Can it learn local phrases or slang? Can you create tweets that trick the system?

************************************************************************************************
************************************************************************************************
************************************************************************************************

22. Machine Learning: Unsupervised Learning
    > Clustering: K-Means
        Unsupervised Learning is how we find patterns and structure in data.
        Clustering is the most well-known unsupervised learning technique. It finds structure in unlabeled data by identifying similar groups, or clusters. 
        Examples of clustering applications are:
            Recommendation engines: group products to personalize the user experience
            Search engines: group news topics and search results
            Market segmentation: group customers based on geography, demography, and behaviors
            Image segmentation: medical imaging or road scene segmentation on self-driving cars
        
        K-Means Clustering
            The goal of clustering is to separate data so that data similar to one another are in the same group, while data different from one another are in different groups. 
            So two questions arise:
                How many groups do we choose?
                How do we define similarity?
            K-Means is the most popular and well-known clustering algorithm, and it tries to address these two questions.
                The “K” refers to the number of clusters (groups) we expect to find in a dataset.
                The “Means” refers to the average distance of data to each cluster center, also known as the centroid, which we are trying to minimize.
            It is an iterative approach:
                - Place k random centroids for the initial clusters.
                - Assign data samples to the nearest centroid.
                - Update centroids based on the above-assigned data samples.
            Repeat Steps 2 and 3 until convergence (when points don’t move between clusters and centroids stabilize).
            Once we are happy with our clusters, we can take a new unlabeled datapoint and quickly assign it to the appropriate cluster.
        
        Iris Dataset
            The sklearn package embeds some datasets and sample images. One of them is the Iris dataset. https://en.wikipedia.org/wiki/Iris_flower_data_set
                from sklearn import datasets        # import the datasets module From sklearn library
                iris = datasets.load_iris()         # To load the Iris dataset
                
                print(iris.data)  
                print(iris.target)                  # The iris.target values give the ground truth for the Iris dataset. Ground truth, in this case, is the number corresponding to the flower that we are trying to learn.              
                print(iris.DESCR)                   #  to read the descriptions of the data

            The Iris dataset looks like:
                [[ 5.1  3.5  1.4  0.2 ]             # Each row is a plant!
                [ 4.9  3.   1.4  0.2 ]
                [ 4.7  3.2  1.3  0.2 ]
                [ 4.6  3.1  1.5  0.2 ]
                . . .
                [ 5.9  3.   5.1  1.8 ]]
            sample - each piece of data. For example, each flower is one sample.
            feature - Each characteristic we are interested in.
            
            The features of the dataset are:
                Column 0: Sepal length
                Column 1: Sepal width
                Column 2: Petal length
                Column 3: Petal width

        Visualize Before K-Means
            import matplotlib.pyplot as plt
            from sklearn import datasets
            iris = datasets.load_iris()
            samples = iris.data             # Store iris.data
            x = samples[:, 0]               # Create a list named x that contains the column 0 values of samples.
            y = samples[:, 1]               # Create a list named y that contains the column 1 values of samples.
            plt.scatter(x, y, alpha=0.5)    # Plot x and y
            plt.show()                      # Show the plot
        
        Implementing K-Means: Step 1    (1. Place k random centroids for the initial clusters.)
            import matplotlib.pyplot as plt
            import numpy as np
            from sklearn import datasets
            from copy import deepcopy
            
            iris = datasets.load_iris()
            samples = iris.data
            x = samples[:,0]
            y = samples[:,1]
            
            k = 3                                                   # Number of clusters
            centroids_x = np.random.uniform(min(x), max(x), k)      # Create x coordinates of k random centroids
            centroids_y = np.random.uniform(min(y), max(y), k)      # Create y coordinates of k random centroids
            centroids = np.array(list(zip(centroids_x, centroids_y)))   # Create centroids array
            print(centroids)
            plt.scatter(x, y, alpha=0.5)                            # Make a scatter plot of x, y
            plt.scatter(centroids_x, centroids_y)                   # Make a scatter plot of the centroids
            plt.xlabel('sepal length (cm)') 
            plt.ylabel('sepal width (cm)')
            plt.show()                                              # Display plot
        
        Implementing K-Means: Step 2    (2. Assign data samples to the nearest centroid.)
            def distance(a,b):                      # Distance formula
                one = (a[0] - b[0]) ** 2
                two = (a[1] - b[1]) ** 2
                distance = (one+two) ** 0.5
                return distance
            labels = np.zeros(len(samples))         # Cluster labels for each point (either 0, 1, or 2)
            distances = np.zeros(k)                 # Distances to each centroid
            for i in range(len(samples)):           # Assign to the closest centroid
                distances[0] = distance(sepal_length_width[i], centroids[0])
                distances[1] = distance(sepal_length_width[i], centroids[1])
                distances[2] = distance(sepal_length_width[i], centroids[2])
                cluster = np.argmin(distances)
                labels[i] = cluster
            print(labels)                           # Print labels
        
        Implementing K-Means: Step 3    (3. Update centroids based on the above-assigned data samples.)
            centroids_old = deepcopy(centroids)         # Save the old centroids value before updating.
            for i in range(k):
                points = [sepal_length_width[j] for j in range(len(sepal_length_width)) if labels[j] == i]
                centroids[i] = np.mean(points, axis=0)
            print(centroids_old)
            print("- - - - - - - - - - - - - -")
            print(centroids)

        Implementing K-Means: Step 4    (Repeat Steps 2 and 3 until convergence.)
            import matplotlib.pyplot as plt
            import numpy as np
            from sklearn import datasets
            from copy import deepcopy

            iris = datasets.load_iris()
            samples = iris.data
            x = samples[:,0]
            y = samples[:,1]
            sepal_length_width = np.array(list(zip(x, y)))

            # Step 1: Place K random centroids
            k = 3
            centroids_x = np.random.uniform(min(x), max(x), size=k)
            centroids_y = np.random.uniform(min(y), max(y), size=k)
            centroids = np.array(list(zip(centroids_x, centroids_y)))
            def distance(a, b):
                one = (a[0] - b[0]) ** 2
                two = (a[1] - b[1]) ** 2
                distance = (one + two) ** 0.5
                return distance
            # To store the value of centroids when it updates
            centroids_old = np.zeros(centroids.shape)
            # Cluster labeles (either 0, 1, or 2)
            labels = np.zeros(len(samples))
            distances = np.zeros(3)

            # Initialize error:
            error = np.zeros(3)
            error[0] = distance(centroids[0], centroids_old[0])
            error[1] = distance(centroids[1], centroids_old[1])
            error[2] = distance(centroids[2], centroids_old[2])

            # Repeat Steps 2 and 3 until convergence:
            while error.all() != 0:
                # Step 2: Assign samples to nearest centroid
                for i in range(len(samples)):
                    distances[0] = distance(sepal_length_width[i], centroids[0])
                    distances[1] = distance(sepal_length_width[i], centroids[1])
                    distances[2] = distance(sepal_length_width[i], centroids[2])
                    cluster = np.argmin(distances)
                    labels[i] = cluster
                # Step 3: Update centroids
                centroids_old = deepcopy(centroids)
                for i in range(3):
                    points = [sepal_length_width[j] for j in range(len(sepal_length_width)) if labels[j] == i]
                    centroids[i] = np.mean(points, axis=0)
                error[0] = distance(centroids[0], centroids_old[0])
                error[1] = distance(centroids[1],   centroids_old[1])
                error[2] = distance(centroids[2], centroids_old[2])

            colors = ['r', 'g', 'b']
            for i in range(k):
                points = np.array([sepal_length_width[j] for j in range(len(samples)) if labels[j] == i])
                plt.scatter(points[:, 0], points[:, 1], c=colors[i], alpha=0.5)

            plt.scatter(centroids[:, 0], centroids[:, 1], marker='D', s=150)
            plt.xlabel('sepal length (cm)')
            plt.ylabel('sepal width (cm)')
            plt.show()

        Implementing K-Means: Scikit-Learn
            from sklearn.cluster import KMeans
            model = KMeans(n_clusters = k)          # For Step 1, use the KMeans() method to build a model that finds k clusters. 
            model.fit(X)                            # For Steps 2 and 3, use the .fit() method to compute K-Means clustering (X = iris.data).
            model.predict(X)                        # predict the closest cluster each sample in X belongs to. Use the .predict() method to compute cluster centers and predict cluster index for each sample

        New Data?
            new_samples = np.array([[5.7, 4.4, 1.5, 0.4],       # Store the new Iris measurements
               [6.5, 3. , 5.5, 0.4],
               [5.8, 2.7, 5.1, 1.9]])
            new_labels = model.predict(new_samples)             # Predict labels for the new_samples
            print(new_labels)
        
        Visualize After K-Means
            x = samples[:,0]                    # Make a scatter plot of x and y and using labels to define the colors
            y = samples[:,1]
            
            plt.scatter(x, y, c=labels, alpha=0.5)
            plt.xlabel('sepal length (cm)')
            plt.ylabel('sepal width (cm)')
            plt.show()
        
        Evaluation
            species = np.chararray(target.shape, itemsize=150)
            for i in range(len(samples)):
                if target[i] == 0:
                    species[i] = 'setosa'
                elif target[i] == 1:
                    species[i] = 'veriscolor'
                elif target[i] == 2: 
                    species[i] = 'virginica'
            df = pd.DataFrame({'labels': labels, 'species': species})
            ct = pd.crosstab(df['labels'], df['species'])
            print(ct)
        
        The Number of Clusters
            import matplotlib.pyplot as plt
            import numpy as np
            import pandas as pd
            from sklearn import datasets
            from sklearn.cluster import KMeans
            
            iris = datasets.load_iris()
            samples = iris.data
            
            num_clusters = list(range(1, 9))
            inertias = []
            for k in num_clusters:
                model = KMeans(n_clusters=k)
                model.fit(samples)
                inertias.append(model.inertia_)
            plt.plot(num_clusters, inertias, '-o')
            plt.xlabel('number of clusters (k)')
            plt.ylabel('inertia')
            plt.show()
    
    K-MEANS++ CLUSTERING
        There can be some problems with first step of the traditional K-Means algorithms, because the starting postitions of the centroids are intialized completely randomly. This can result in suboptimal clusters.
        We will go over another version of K-Means, known as the K-Means++ algorithm. K-Means++ changes the way centroids are initalized to try to fix this problem.
        Poor Clustering
        What is K-Means++?
            The K-Means++ algorithm replaces Step 1 of the K-Means algorithm and adds the following:
                1.1 The first cluster centroid is randomly picked from the data points.
                1.2 For each remaining data point, the distance from the point to its nearest cluster centroid is calculated.
                1.3 The next cluster centroid is picked according to a probability proportional to the distance of each point to its nearest cluster centroid. This makes it likely for the next cluster centroid to be far away from the already initialized centroids.
                Repeat 1.2 - 1.3 until k centroids are chosen.
        K-Means++ using Scikit-Learn
            K-Means:
                model = KMeans(n_clusters=6, init='random')     # initial centroids are chosen as random.
            K-Means++:
                test = KMeans(n_clusters=6, init='k-means++')   # init=k-means++ is actually default in scikit-learn.
                # or
                test = KMeans(n_clusters=6)
        
        Project: Handwriting Recognition using K-Means

************************************************************************************************
************************************************************************************************
************************************************************************************************

23. Unsupervised Machine Learning Cumulative Project
        Masculinity Survey
            In this project, you will use a survey conducted by 538 to find patterns in the way men view masculinity.

************************************************************************************************
************************************************************************************************
************************************************************************************************

24. Perceptrons and Neural Nets
    > Perceptrons
        An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain.
        A neural network is a programming model that simulates the human brain. 
        Humans are naturally wired to effortlessly recognize objects and patterns, something that computers find difficult.
        This juxtaposition brought up two important questions in the 1950s:
            “How can computers be better at solving problems that humans find effortless?”
            “How can computers solve such problems in the way a human brain does?”
        In 1957, Frank Rosenblatt explored the second question and invented the 'Perceptron algorithm' that allowed an artificial neuron to simulate a biological neuron! The artificial neuron could take in an input, process it based on some rules, and fire a result. But computers had been doing this for years — what was so remarkable?
        There was a final step in the Perceptron algorithm that would give rise to the incredibly mysterious world of Neural Networks — the artificial neuron could train itself based on its own results, and fire better results in the future. In other words, it could learn by trial and error, just like a biological neuron.
        It was found out that creating multiple layers of neurons — with one layer feeding its output to the next layer as input — could process a wide range of inputs, make complex decisions, and still produce meaningful results. With some tweaks, the algorithm became known as the Multilayer Perceptron, which led to the rise of Feedforward Neural Networks.
        Today, the applications of neural networks have become widespread — from simple tasks like speech recognition to more complicated tasks like self-driving vehicles.

        What is a Perceptron?
            Perceptrons are the building blocks of Neural Networks.
            The word “perceptron” is a combination of two words:
                - Perception (noun) the ability to sense something
                - Neuron (noun) a nerve cell in the human brain that turns sensory input into meaningful information
            Perceptron is an artificial neuron that simulates the task of a biological neuron to solve problems through its own “sense” of the world.
            Example: self-driving car. Perceptron could take the position of the obstacle as inputs, and produce a decision — left turn or right turn — based on those inputs.
        
        Representing a Perceptron
            The perceptron has three main components:
                - Inputs: Each input corresponds to a feature. For example, in the case of a person, features could be age, height, weight, college degree, etc.
                - Weights: Each input also has a weight which assigns a certain amount of importance to the input. If an input’s weight is large, it means this input plays a bigger role in determining the output. For example, a team’s skill level will have a bigger weight than the average age of players in determining the outcome of a match.
                - Output: Finally, the perceptron uses the inputs and weights to produce an output. The type of the output varies depending on the nature of the problem. For example, to predict whether or not it’s going to rain, the output has to be binary — 1 for Yes and 0 for No. However, to predict the temperature for the next day, the range of the output has to be larger — say a number from 70 to 90.
            Example:
                class Perceptron:
                    def __init__(self, num_inputs=2, weights=[1,1]):
                        self.num_inputs = num_inputs
                        self.weights = weights
                cool_perceptron = Perceptron()
                print(cool_perceptron)
        
        Step 1: Weighted Sum
            How are the inputs and weights magically turned into an output? This is a two-step process, and the first step is finding the weighted sum of the inputs.
            Weighted sum is just a number that gives a reasonable representation of the inputs.
                weighted sum = x1w1 + x2w2 + ... + xnwn     # The x‘s are the inputs and the w‘s are the weights.
            Example:
                class Perceptron:
                    def __init__(self, num_inputs=2, weights=[2,1]):
                        self.num_inputs = num_inputs
                        self.weights = weights
                    def weighted_sum(self, inputs):
                        weighted_sum = 0
                        for i in range(self.num_inputs):
                            weighted_sum += self.weights[i]*inputs[i]
                            #complete this loop
                        return weighted_sum
                cool_perceptron = Perceptron()
                print(cool_perceptron.weighted_sum([24, 55]))

        Step 2: Activation Function
            After finding the weighted sum, the second step is to constrain the weighted sum to produce a desired output.
            Example:
                class Perceptron:
                    # def __init__, def weighted_sum - from above
                    def activation(self, weighted_sum):
                        if weighted_sum >= 0:
                            return 1
                        if weighted_sum < 0:
                            return -1
                print(cool_perceptron.activation(52))
        
        Training the Perceptron
            We can train the perceptron to produce better and better results! In order to do this, we provide the perceptron a training set — a collection of random inputs with correctly predicted outputs.
            Example:
                import matplotlib.pyplot as plt
                import random

                def generate_training_set(num_points):
                	x_coordinates = [random.randint(0, 50) for i in range(num_points)]
                	y_coordinates = [random.randint(0, 50) for i in range(num_points)]
                	training_set = dict()
                	for x, y in zip(x_coordinates, y_coordinates):
                		if x <= 45-y:
                			training_set[(x,y)] = 1
                		elif x > 45-y:
                			training_set[(x,y)] = -1
                	return training_set

                training_set = generate_training_set(30)
                x_plus = []
                y_plus = []
                x_minus = []
                y_minus = []

                for data in training_set:
                	if training_set[data] == 1:
                		x_plus.append(data[0])
                		y_plus.append(data[1])
                	elif training_set[data] == -1:
                		x_minus.append(data[0])
                		y_minus.append(data[1])

                fig = plt.figure()
                ax = plt.axes(xlim=(-25, 75), ylim=(-25, 75))

                plt.scatter(x_plus, y_plus, marker = '+', c = 'green', s = 128, linewidth = 2)
                plt.scatter(x_minus, y_minus, marker = '_', c = 'red', s = 128, linewidth = 2)
                plt.title("Training Set")
                plt.show()

        Training Error
            Every time the output mismatches the expected label, we say that the perceptron has made a training error — a quantity that measures “how bad” the perceptron is performing.
                training error = actual label - predicted label
            For each point in the training set, the perceptron either produces a +1 or a -1 (as we are using the Sign Activation Function).
            Example:
                class Perceptron:
                    def __init__(self, num_inputs=2, weights=[1,1]):
                        self.num_inputs = num_inputs
                        self.weights = weights
                    def weighted_sum(self, inputs):
                        weighted_sum = 0
                        for i in range(self.num_inputs):
                            weighted_sum += self.weights[i]*inputs[i]
                        return weighted_sum
                    def activation(self, weighted_sum):
                        if weighted_sum >= 0:
                            return 1
                        if weighted_sum < 0:
                            return -1
                    def training(self, training_set):
                        for inputs in training_set:                   
                            prediction = self.activation(self.weighted_sum(inputs))
                            actual = training_set[inputs]
                            error = actual - prediction
                cool_perceptron = Perceptron()
                print(cool_perceptron.weighted_sum([24, 55]))
                print(cool_perceptron.activation(52))
        
        Tweaking the Weights
            What do we do once we have the errors for the perceptron? We slowly nudge the perceptron towards a better version of itself that eventually has zero error.
            The only way to do that is to change the parameters that define the perceptron. We can’t change the inputs so the only thing that can be tweaked are the weights. As we change the weights, the outputs change as well.
            The goal is to find the optimal combination of weights that will produce the correct output for as many points as possible in the dataset.
        
        The Perceptron Algorithm
            The most important part of the algorithm is the update rule where the weights get updated:
                weight = weight + (error * input)
                We keep on tweaking the weights until all possible labels are correctly predicted by the perceptron. This means that multiple passes might need to be made through the training_set before the Perceptron Algorithm comes to a halt.
                
                - If the algorithm doesn’t find an error, the perceptron must have correctly predicted the labels for all points.
                    if total_error == 0:        # under 'def training' -> at the end of 'while not foundLine:'
                       foundLine = True
                - In order to update the weight for each inputs, create another for loop (inside the existing for loop) that iterates a loop variable i through a range of self.num_inputs.
                    for i in range(self.num_inputs):
                        self.weights[i] += error*inputs[i]
                - Train cool_perceptron using small_training_set.
                    cool_perceptron = Perceptron()
                    small_training_set = {(0,3):1, (3,0):-1, (0,-3):-1, (-3,0):1}
                    cool_perceptron.training(small_training_set)
                    print(cool_perceptron.weights)
        
        The Bias Weight  
            weighted sum=x1w1​ + x2​w2​ + ... + xn​wn​ + 1wb
        Representing a Line
            The perceptron works as expected, but everything seems to be taking place behind the scenes. What if we could visualize the perceptron’s training process to gain a better understanding of what’s going on?
            The weights change throughout the training process so if only we could meaningfully visualize those weights …
            Turns out we can! In fact, it gets better. The weights can actually be used to represent a line! This greatly simplifies our visualization.
            You might know that a line can be represented using the slope-intercept form. A perceptron’s weights can be used to find the slope and intercept of the line that the perceptron represents.
                slope = -self.weights[0]/self.weights[1]
                intercept = -self.weights[2]/self.weights[1]
        
        Finding a Linear Classifier
        What's Next? Neural Networks
            By increasing the number of features and perceptrons, we can give rise to the Multilayer Perceptrons, also known as Neural Networks, which can solve much more complicated problems.

************************************************************************************************
************************************************************************************************
************************************************************************************************

25. Machine Learning Capstone Project
        Date-A-Scientist
            In this capstone, you will analyze some data from OKCupid, an app that focuses on using multiple choice and short answers to match users.
            
            Introduction
                The purpose of this capstone is to practice formulating questions and implementing Machine Learning techniques to answer those questions. 
                The dataset provided has the following columns of multiple-choice data:
                    body_type, diet, drinks, drugs, education, ethnicity, height, income, job, offspring, orientation, pets, religion, sex, sign, smokes, speaks, status
                And a set of open short-answer responses to :
                    essay0 - My self summary
                    essay1 - What I’m doing with my life
                    essay2 - I’m really good at
                    essay3 - The first thing people usually notice about me
                    essay4 - Favorite books, movies, show, music, and food
                    essay5 - The six things I could never do without
                    essay6 - I spend a lot of time thinking about
                    essay7 - On a typical Friday night I am
                    essay8 - The most private thing I am willing to admit
                    essay9 - You should message me if…
            Load in the DataFrame
                The data is stored in profiles.csv. We can start to work with it in dating.py by using Pandas, which we have imported for you with the line:
                    import pandas as pd
                and then loading the csv into a DataFrame:
                    df = pd.read_csv("profiles.csv")
            Explore the Data
                Let’s make sure we understand what these columns represent!
                Pick some columns and call .head() on them to see the first five rows of data. For example, we were curious about job, so we called:
                    df.job.head()
                You can also call value_counts() on a column to figure out what possible responses there are, and how many of each response there was.
            Visualize some of the Data
                We can start to build graphs from the data by first importing Matplotlib:
                    from matplotlib import pyplot as plt
                and then making some plots!
                For example, we were curious about the distribution of ages on the site, so we made a histogram of the age column:
                    plt.hist(df.age, bins=20)
                    plt.xlabel("Age")
                    plt.ylabel("Frequency")
                    plt.xlim(16, 80)
                    plt.show()
                Try this code in your own file and take a look at the histogram it produces!
            Formulate a Question
                As we started to look at this data, we started to get more and more curious about Zodiac signs. First, we looked at all of the possible values for Zodiac signs:
                df.sign.value_counts()
                We started to wonder if there was a way to predict a user’s Zodiac sign from the information in their profile. Thinking about the columns we had already explored, we thought that maybe we could classify Zodiac signs using drinking, smoking, drugs, and essays as our features.
            Augment your Data
                In order to answer the question you’ve formulated, you will probably need to create some new columns in the DataFrame. This is especially true because so much of our data here is categorical (i.e. diet consists of the options vegan, vegetarian, anything, etc. instead of numerical values).
                Categorical data is great to use as labels, but we want to create some numerical data as well to use for features.
                For our question about Zodiac signs, we wanted to transform the drinks column into numerical data. We used:
                    drink_mapping = {"not at all": 0, "rarely": 1, "socially": 2, "often": 3, "very often": 4, "desperately": 5}
                    all_data["drinks_code"] = all_data.drinks.map(drink_mapping)
                These lines of code created a new column called ‘drinks_code’ that mapped the following drinks values to these numbers:
                We did the same for smokes and drugs.
                We also wanted some numerical data about the short answer essays. We combined them all into one string, took out the NaNs, and then created a new column called essay_len:
                    essay_cols = ["essay0","essay1","essay2","essay3","essay4","essay5","essay6","essay7","essay8","essay9"]
                    # Removing the NaNs
                    all_essays = all_data[essay_cols].replace(np.nan, '', regex=True)
                    # Combining the essays
                    all_essays = all_essays[essay_cols].apply(lambda x: ' '.join(x), axis=1)
                    all_data["essay_len"] = all_essays.apply(lambda x: len(x)))
                We also created a column with average word length and a column with the frequency of the words “I” or “me” appearing in the essays.
            Normalize your Data
                In order to get accurate results, we should make sure our numerical data all has the same weight.
                For our Zodiac features, we used:
                    feature_data = all_data[['smokes_code', 'drinks_code', 'drugs_code', 'essay_len', 'avg_word_length']]
                    x = feature_data.values
                    min_max_scaler = preprocessing.MinMaxScaler()
                    x_scaled = min_max_scaler.fit_transform(x)
                    feature_data = pd.DataFrame(x_scaled, columns=feature_data.columns)
            Use Classification Techniques
                We have learned how to perform classification in a few different ways.
                    We learned about K-Nearest Neighbors by exploring IMDB ratings of popular movies
                    We learned about Support Vector Machines by exploring baseball statistics
                    We learned about Naive Bayes by exploring Amazon Reviews
                Some questions we used classification to tackle were:
                    Can we predict sex with education level and income??
                    Can we predict education level with essay text word counts?
            Use Regression Techniques
                We have learned how to perform Multiple Linear Regression by playing with StreetEasy apartment data. Is there a way we can apply the techniques we learned to this dataset?
                Some questions we used regression to tackle were:
                    Predict income with length of essays and average word length?
                    Predict age with the frequency of “I” or “me” in essays?
                We also learned about K-Nearest Neighbors Regression. Which form of regression works better to answer your question?
            Analyze the Accuracy, Precision and Recall
                After you have trained your model and run it, you will probably be curious about how well it did.
                Find the accuracy, precision, and recall of each model you used, and create graphs showing how they changed.
                For our question of classifying Zodiac signs, one graph we produced showed classification accuracy versus k (for K-Nearest Neighbors):
                The accuracy we would expect from predicting a Zodiac sign by randomly selecting one would be 1/12, or 0.0833. Our model did not significantly outperform this number. We were unimpressed.
            Create your Presentation
                - at least two graphs containing exploration of the dataset
                - a statement of your question (or questions!) and how you arrived there
                - the explanation of at least two new columns you created and how you did it
                - the comparison between two classification approaches, including a qualitative discussion of simplicity, time to run the model, and accuracy, precision, and/or recall
                - the comparison between two regression approaches, including a qualitative discussion of simplicity, time to run the model, and accuracy, precision, and/or recall
                - an overall conclusion, with a preliminary answer to your initial question(s), next steps, and what other data you would like to have in order to better answer your question(s)
************************************************************************************************
************************************************************************************************
************************************************************************************************

26. Natural Language Processing (NLP)
    > Getting Started with NLP
        Look at the technologies around us:
            Spellcheck and autocorrect                  // e.g. Grammarly
            Auto-generated video captions               // e.g. Youtube captions
            Virtual assistants like Amazon’s Alexa
            Autocomplete                                // e.g. Google search
            Your news site’s suggested articles
            What do they have in common?
        All of these handy technologies exist because of NLP.
        
        NLP -  the field is at the intersection of linguistics, artificial intelligence, and computer science. The goal? Enabling computers to interpret, analyze, and approximate the generation of human languages (like English or Spanish).
        Applications from detecting spam emails or bias in tweets to improving accessibility for people with disabilities all rely heavily on NLP techniques.
        NLP can be conducted in several programming languages. However, Python has some of the most extensive open-source NLP libraries, including the Natural Language Toolkit https://www.nltk.org/ or NLTK. 

        Text Preprocessing          // to have clean list of words
            Text preprocessing is usually the first step you’ll take when faced with an NLP task.
            Without preprocessing, your computer interprets "the", "The", and "<p>The" as entirely different words.
            Regex and NLTK will do most of it for you! Common tasks include:
                - Noise removal — stripping text of formatting (e.g., HTML tags).
                - Tokenization — breaking text into individual words.
                - Normalization — cleaning text data in any other way:
                    - Stemming is a blunt axe to chop off word prefixes and suffixes. “booing” and “booed” become “boo”, but “sing” may become “s” and “sung” would remain “sung.”
                    - Lemmatization is a scalpel to bring words down to their root forms. For example, NLTK’s savvy lemmatizer knows “am” and “are” are related to “be.”
                    - Other common tasks include lowercasing, stopwords(https://en.wikipedia.org/wiki/Stop_words) removal, spelling correction, etc.
            
            Example:
                # regex for removing punctuation!
                import re
                # nltk preprocessing magic
                import nltk
                from nltk.tokenize import word_tokenize
                from nltk.stem import PorterStemmer
                from nltk.stem import WordNetLemmatizer
                # grabbing a part of speech function:
                from part_of_speech import get_part_of_speech

                text = "So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed."

                cleaned = re.sub('\W+', ' ', text)
                tokenized = word_tokenize(cleaned)

                stemmer = PorterStemmer()
                stemmed = [stemmer.stem(token) for token in tokenized]

                lemmatizer = WordNetLemmatizer()
                lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]

                print("Stemmed text:")
                print(stemmed)
                print("\nLemmatized text:")
                print(lemmatized)
        
        Parsing Text
            Parsing is a stage of NLP concerned with segmenting text based on syntax.
            You probably do not want to be doing any parsing by hand and NLTK has:
                - Part-of-speech tagging (POS tagging) identifies parts of speech (verbs, nouns, adjectives, etc.). 
                - Named entity recognition (NER) helps identify the proper nouns (e.g., “Natalia” or “Berlin”) in a text.
                - Dependency grammar trees help you understand the relationship between the words in a sentence. It can be a tedious task for a human, so the Python library spaCy is at your service, even if it isn’t always perfect.
                - Regex parsing, using Python’s re library, allows for a bit more nuance. When coupled with POS tagging, you can identify specific phrase chunks. On its own, it can find you addresses, emails, and many other common patterns within large chunks of text.

            Example:
                import spacy
                from nltk import Tree
                from squids import squids_text

                dependency_parser = spacy.load('en')

                parsed_squids = dependency_parser(squids_text)

                my_sentence = "My sentence!"
                my_parsed_sentence = dependency_parser(my_sentence)

                def to_nltk_tree(node):
                    if node.n_lefts + node.n_rights > 0:
                        parsed_child_nodes = [to_nltk_tree(child) for child in node.children]
                        return Tree(node.orth_, parsed_child_nodes)
                    else:
                        return node.orth_

                for sent in parsed_squids.sents:
                    to_nltk_tree(sent.root).pretty_print()

                for sent in my_parsed_sentence.sents:
                    to_nltk_tree(sent.root).pretty_print()

        Language Models - Bag-of-Words Approach
            How can we help a machine make sense of a bunch of word tokens? We can help computers make predictions about language by training a language model on a corpus (a bunch of example text).
            Language models are probabilistic computer models of language. We build and use these models to figure out the likelihood that a given sound, letter, word, or phrase will be used. Once a model has been trained, it can be tested out on new texts.
            One of the most common language models is the unigram model, a statistical language model commonly known as bag-of-words. As its name suggests, bag-of-words does not have much order to its chaos! What it does have is a tally count of each instance for each word. 
            bag-of-words would result in a mapping like:
                {"the": 2, "squid": 1, "jump": 1, "out": 1, "of": 1, "suitcase": 1}

            Example:
                # importing regex and nltk
                import re, nltk
                from nltk.corpus import stopwords
                from nltk.tokenize import word_tokenize
                from nltk.stem import WordNetLemmatizer
                # importing Counter to get word counts for bag of words
                from collections import Counter
                # importing a passage from Through the Looking Glass
                from looking_glass import looking_glass_text
                # importing part-of-speech function for lemmatization
                from part_of_speech import get_part_of_speech

                # Change text to another string:
                text = looking_glass_text

                cleaned = re.sub('\W+', ' ', text).lower()
                tokenized = word_tokenize(cleaned)

                stop_words = stopwords.words('english')
                filtered = [word for word in tokenized if word not in stop_words]

                normalizer = WordNetLemmatizer()
                normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]
                # Comment out the print statement below
                # print(normalized)

                # Define bag_of_looking_glass_words & print:
                bag_of_looking_glass_words = Counter(normalized)
                print(bag_of_looking_glass_words)
        
        Language Models - N-Grams and NLM
            Unlike bag-of-words, the n-gram model considers a sequence of some number (n) units and calculates the probability of each unit in a body of language given the preceding sequence of length n. Because of this, n-gram probabilities with larger n values can be impressive at language prediction.
            A bigram model (where n is 2) might give us the following count frequencies:
                {('', 'the'): 2, ('the', 'squids'): 2, ('squids', 'jumped'): 1, ('jumped', 'out'): 1, ('out', 'of'): 1, ('of', 'the'): 1, ('the', 'suitcases'): 1, ('suitcases', ''): 1, ('squids', 'were'): 1, ('were', 'furious'): 1, ('furious', ''): 1}
            There are a couple problems with the n gram model:
                - How can your language model make sense of the sentence “The cat fell asleep in the mailbox” if it’s never seen the word “mailbox” before? During training, your model will probably come across test words that it has never encountered before (this issue also pertains to bag of words). A tactic known as language smoothing can help adjust probabilities for unknown words, but it isn’t always ideal.
                - For a model that more accurately predicts human language patterns, you want n (your sequence length) to be as large as possible. That way, you will have more natural sounding language, right? Well, as the sequence length grows, the number of examples of each sequence within your training corpus shrinks. With too few examples, you won’t have enough data to make many predictions.
            
            Example:
                import nltk, re
                from nltk.tokenize import word_tokenize
                # importing ngrams module from nltk
                from nltk.util import ngrams
                from collections import Counter
                from looking_glass import looking_glass_full_text

                cleaned = re.sub('\W+', ' ', looking_glass_full_text).lower()
                tokenized = word_tokenize(cleaned)

                # Change the n value to 2:
                looking_glass_bigrams = ngrams(tokenized, 2)
                looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)

                # Change the n value to 3:
                looking_glass_trigrams = ngrams(tokenized, 3)
                looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)

                # Change the n value to a number greater than 3:
                looking_glass_ngrams = ngrams(tokenized, 5)
                looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)

                print("Looking Glass Bigrams:")
                print(looking_glass_bigrams_frequency.most_common(10))

                print("\nLooking Glass Trigrams:")
                print(looking_glass_trigrams_frequency.most_common(10))

                print("\nLooking Glass n-grams:")
                print(looking_glass_ngrams_frequency.most_common(10))

        Topic Models
            Topic modeling is an area of NLP dedicated to uncovering latent, or hidden, topics within a body of language. For example, one Codecademy curriculum developer used topic modeling to discover patterns within Taylor Swift songs related to love and heartbreak over time.
            A common technique is to deprioritize the most common words and prioritize less frequently used terms as topics in a process known as term frequency-inverse document frequency (tf-idf). Say what?! This may sound counter-intuitive at first. Why would you want to give more priority to less-used words? Well, when you’re working with a lot of text, it makes a bit of sense if you don’t want your topics filled with words like “the” and “is.” The Python libraries gensim and sklearn have modules to handle tf-idf.
            Whether you use your plain bag of words (which will give you term frequency) or run it through tf-idf, the next step in your topic modeling journey is often 'latent Dirichlet allocation' (LDA). LDA is a statistical model that takes your documents and determines which words keep popping up together in the same contexts (i.e., documents). We’ll use sklearn to tackle this for us.
            If you have any interest in visualizing your newly minted topics, 'word2vec' is a great technique to have up your sleeve. word2vec can map out your topic model results spatially as vectors so that similarly used words are closer together. In the case of a language sample consisting of “The squids jumped out of the suitcases. The squids were furious. Why are your suitcases full of jumping squids?”, we might see that “suitcase”, “jump”, and “squid” were words used within similar contexts. This word-to-vector mapping is known as a word embedding.

            Example:
                import nltk, re
                from sherlock_holmes import bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3
                from preprocessing import preprocess_text
                from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
                from sklearn.decomposition import LatentDirichletAllocation
                
                # preparing the text
                corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]
                preprocessed_corpus = [preprocess_text(chapter) for chapter in corpus]
                
                # Update stop_list:
                stop_list = ["say", "see", "holmes", "shall", "say", "man", "upon", "know", "quite", "one"]
                # filtering topics for stop words
                def filter_out_stop_words(corpus):
                  no_stops_corpus = []
                  for chapter in corpus:
                    no_stops_chapter = " ".join([word for word in chapter.split(" ") if word not in stop_list])
                    no_stops_corpus.append(no_stops_chapter)
                  return no_stops_corpus
                filtered_for_stops = filter_out_stop_words(preprocessed_corpus)
                
                # creating the bag of words model
                bag_of_words_creator = CountVectorizer()
                bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)
                
                # creating the tf-idf model
                tfidf_creator = TfidfVectorizer(min_df = 0.2)
                tfidf = tfidf_creator.fit_transform(preprocessed_corpus)
                
                # creating the bag of words LDA model
                lda_bag_of_words_creator = LatentDirichletAllocation(learning_method='online', n_components=10)
                lda_bag_of_words = lda_bag_of_words_creator.fit_transform(bag_of_words)
                
                # creating the tf-idf LDA model
                lda_tfidf_creator = LatentDirichletAllocation(learning_method='online', n_components=10)
                lda_tfidf = lda_tfidf_creator.fit_transform(tfidf)
                
                print("~~~ Topics found by bag of words LDA ~~~")
                for topic_id, topic in enumerate(lda_bag_of_words_creator.components_):
                  message = "Topic #{}: ".format(topic_id + 1)
                  message += " ".join([bag_of_words_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])
                  print(message)
                
                print("\n\n~~~ Topics found by tf-idf LDA ~~~")
                for topic_id, topic in enumerate(lda_tfidf_creator.components_):
                  message = "Topic #{}: ".format(topic_id + 1)
                  message += " ".join([tfidf_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])
                  print(message)
    
        Text Similarity
            Addressing word similarity and misspelling for spellcheck or autocorrect often involves considering the Levenshtein distance or minimal edit distance between two words. The distance is calculated through the minimum number of insertions, deletions, and substitutions that would need to occur for one word to become another. For example, turning “bees” into “beans” would require one substitution (“a” for “e”) and one insertion (“n”), so the Levenshtein distance would be two.
            Phonetic similarity is also a major challenge within speech recognition. English-speaking humans can easily tell from context whether someone said “euthanasia” or “youth in Asia,” but it’s a far more challenging task for a machine! More advanced autocorrect and spelling correction technology additionally considers key distance on a keyboard and phonetic similarity (how much two words or phrases sound the same).
            It’s also helpful to find out if texts are the same to guard against plagiarism, which we can identify through lexical similarity (the degree to which texts use the same vocabulary and phrases). Meanwhile, semantic similarity (the degree to which documents contain similar meaning or topics) is useful when you want to find (or recommend) an article or book similar to one you recently finished.
        
            Example:
                import nltk
                # NLTK has a built-in function to check Levenshtein distance:
                from nltk.metrics import edit_distance
                
                def print_levenshtein(string1, string2):
                    print("The Levenshtein distance from '{0}' to '{1}' is {2}!".format(string1, string2, edit_distance(string1, string2)))
                
                # Check the distance between any two words here!
                print_levenshtein("fart", "target")
                
                # Assign passing strings here:
                three_away_from_code = "cloud"
                two_away_from_chunk = "dunk"
                
                print_levenshtein("code", three_away_from_code)
                print_levenshtein("chunk", two_away_from_chunk)

        Language Prediction & Text Generation
            How does your favorite search engine complete your search queries? How does your phone’s keyboard know what you want to type next? 'Language prediction' is an application of NLP concerned with predicting text given preceding text. Autosuggest, autocomplete, and suggested replies are common forms of language prediction.
            Your first step to language prediction is picking a language model. Bag of words alone is generally not a great model for language prediction; no matter what the preceding word was, you will just get one of the most commonly used words from your training corpus.
            If you go the n-gram route, you will most likely rely on 'Markov chains' to predict the statistical likelihood of each following word (or character) based on the training corpus. Markov chains are memory-less and make statistical predictions based entirely on the current n-gram on hand.
            For example, let’s take a sentence beginning, “I ate so many grilled cheese”. Using a trigram model (where n is 3), a Markov chain would predict the following word as “sandwiches” based on the number of times the sequence “grilled cheese sandwiches” has appeared in the training data out of all the times “grilled cheese” has appeared in the training data.
            A more advanced approach, using a neural language model, is the 'Long Short Term Memory' (LSTM) model. LSTM uses deep learning with a network of artificial “cells” that manage memory, making them better suited for text prediction than traditional neural networks.

            Example:
                import nltk, re, random
                from nltk.tokenize import word_tokenize
                from collections import defaultdict, deque
                from document1 import training_doc1         // training_doc1 = """Alice1 ..."""
                from document2 import training_doc2         // training_doc1 = """Alice2 ..."""
                from document3 import training_doc3         // training_doc1 = """Alice3 ..."""
                
                class MarkovChain:
                  def __init__(self):
                    self.lookup_dict = defaultdict(list)
                    self._seeded = False
                    self.__seed_me()
                
                  def __seed_me(self, rand_seed=None):
                    if self._seeded is not True:
                      try:
                        if rand_seed is not None:
                          random.seed(rand_seed)
                        else:
                          random.seed()
                        self._seeded = True
                      except NotImplementedError:
                        self._seeded = False
                    
                  def add_document(self, str):
                    preprocessed_list = self._preprocess(str)
                    pairs = self.__generate_tuple_keys(preprocessed_list)
                    for pair in pairs:
                      self.lookup_dict[pair[0]].append(pair[1])
                
                  def _preprocess(self, str):
                    cleaned = re.sub(r'\W+', ' ', str).lower()
                    tokenized = word_tokenize(cleaned)
                    return tokenized
                
                  def __generate_tuple_keys(self, data):
                    if len(data) < 1:
                      return
                
                    for i in range(len(data) - 1):
                      yield [ data[i], data[i + 1] ]
                      
                  def generate_text(self, max_length=50):
                    context = deque()
                    output = []
                    if len(self.lookup_dict) > 0:
                      self.__seed_me(rand_seed=len(self.lookup_dict))
                      chain_head = [list(self.lookup_dict)[0]]
                      context.extend(chain_head)
                      
                      while len(output) < (max_length - 1):
                        next_choices = self.lookup_dict[context[-1]]
                        if len(next_choices) > 0:
                          next_word = random.choice(next_choices)
                          context.append(next_word)
                          output.append(context.popleft())
                        else:
                          break
                      output.extend(list(context))
                    return " ".join(output)
                
                my_markov = MarkovChain()
                my_markov.add_document(training_doc1)
                my_markov.add_document(training_doc2)
                my_markov.add_document(training_doc3)
                generated_text = my_markov.generate_text()
                print(generated_text)
        
        Advanced NLP Topics
            There are a slew of advanced topics and applications of NLP, many of which rely on deep learning and neural networks.
                - 'Naive Bayes classifiers' are supervised machine learning algorithms that leverage a probabilistic theorem to make predictions and classifications. They are widely used for sentiment analysis (determining whether a given block of language expresses negative or positive feelings) and spam filtering.
                - We’ve made enormous gains in 'machine translation', but even the most advanced translation software using neural networks and LSTM still has far to go in accurately translating between languages.
                - Some of the most life-altering applications of NLP are focused on improving 'language accessibility' for people with disabilities. Text-to-speech functionality and speech recognition have improved rapidly thanks to neural language models, making digital spaces far more accessible places.
                - NLP can also be used to detect bias in writing and speech. Feel like a political candidate, book, or news source is biased but can’t put your finger on exactly how? Natural language processing can help you identify the language at issue.
            
            Example:
                from reviews import counter, training_counts
                from sklearn.feature_extraction.text import CountVectorizer
                from sklearn.naive_bayes import MultinomialNB

                # Add your review:
                review = "Super!"
                review_counts = counter.transform([review])

                classifier = MultinomialNB()
                training_labels = [0] * 1000 + [1] * 1000

                classifier.fit(training_counts, training_labels)

                neg = (classifier.predict_proba(review_counts)[0][0] * 100).round()
                pos = (classifier.predict_proba(review_counts)[0][1] * 100).round()

                if pos > 50:
                  print("Thank you for your positive review!")
                elif neg > 50:
                  print("We're sorry this hasn't been the best possible lesson for you! We're always looking to improve.")
                else:
                  print("Naive Bayes cannot determine if this is negative or positive. Thank you or we're sorry?")

                print("\nAccording to our trained Naive Bayes classifier, the probability that your review was negative was {0}% and the probability it was positive was {1}%.".format(neg, pos))

        Challenges and Considerations
            When working with NLP, we have several important considerations to take into account:
                - Different NLP tasks may be more or less difficult in different languages. Because so many NLP tools are built by and for English speakers, these tools may lag behind in processing other languages. The tools may also be programmed with cultural and linguistic biases specific to English speakers.
                - What if your Amazon Alexa could only understand wealthy men from coastal areas of the United States? English itself is not a homogeneous body. English varies by person, by dialect, and by many sociolinguistic factors. When we build and train NLP tools, are we only building them for one type of English speaker?
                - You can have the best intentions and still inadvertently program a bigoted tool. While NLP can limit bias, it can also propagate bias. As an NLP developer, it’s important to consider biases, both within your code and within the training corpus. A machine will learn the same biases you teach it, whether intentionally or unintentionally.
                - As you become someone who builds tools with natural language processing, it’s vital to take into account your users’ privacy. There are many powerful NLP tools that come head-to-head with privacy concerns. Who is collecting your data? How much data is being collected and what do those companies plan to do with your data?
        
    > Parsing with Regular Expressions
        By using Python’s regular expression modulere and the Natural Language Toolkit, known as NLTK, you can find keywords of interest, discover where and how often they are used, and discern the parts-of-speech patterns in which they appear to understand the sometimes hidden meaning in a piece of writing.

        Example:
            from nltk import RegexpParser
            from pos_tagged_oz import pos_tagged_oz
            from np_chunk_counter import np_chunk_counter

            # define noun-phrase chunk grammar here
            chunk_grammar = "NP: {<DT>?<JJ>*<NN>}"

            # create RegexpParser object here
            chunk_parser = RegexpParser(chunk_grammar)

            # create a list to hold noun-phrase chunked sentences
            np_chunked_oz = list()

            # create a for-loop through each pos-tagged sentence in pos_tagged_oz here
            for pos_tagged_sentence in pos_tagged_oz:
              # chunk each sentence and append to np_chunked_oz here
              np_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))

            # store and print the most common np-chunks here
            most_common_np_chunks = np_chunk_counter(np_chunked_oz)
            print(most_common_np_chunks)
    
        Compiling and Matching
            The first method you will explore is .compile(). This method takes a regular expression pattern as an argument and compiles the pattern into a regular expression object, which you can later use to find matching text. The regular expression object below will exactly match 4 upper or lower case characters.
                e.g. regular_expression_object = re.compile("[A-Za-z]{4}")
            Regular expression objects have a .match() method that takes a string of text as an argument and looks for a single match to the regular expression that starts at the beginning of the string.
                e.g. result = regular_expression_object.match("Toto")
                    result = re.match("[A-Za-z]{4}","Toto")

                Example:
                    import re

                    # characters are defined
                    character_1 = "Dorothy"
                    character_2 = "Henry"

                    # compile your regular expression here
                    regular_expression = re.compile("\w{7}")

                    # check for a match to character_1 here
                    result_1 = regular_expression.match(character_1)
                    print(result_1)

                    # store and print the matched text here
                    match_1 = result_1.group(0)
                    print(match_1)

                    # compile a regular expression to match a 7 character string of word characters and check for a match to character_2 here
                    result_2 = re.match("\w{7}",character_2)
                    print(result_2)
        
        Searching and Finding
            e.g. result = re.search("\w{8}","Are you a Munchkin?")
            list_of_matches = re.findall("\w{8}",text)      // text = "Everything is green here"

            Example:    
                import re

                # import L. Frank Baum's The Wonderful Wizard of Oz
                oz_text = open("the_wizard_of_oz_text.txt",encoding='utf-8').read().lower()

                # search oz_text for an occurrence of 'wizard' here
                found_wizard = re.search("wizard",oz_text)
                print(found_wizard)

                # find all the occurrences of 'lion' in oz_text here
                all_lions = re.findall("lion",oz_text)
                print(all_lions)

                # store and print the length of all_lions here
                number_lions = len(all_lions)
                print(number_lions)
        
        Part-of-Speech Tagging
            While it is useful to match and search for patterns of individual characters in a text, you can often find more meaning by analyzing text on a word-by-word basis, focusing on the part of speech of each word in a sentence. This process of identifying and labeling the part of speech of words is known as part-of-speech tagging!
            You can automate the part-of-speech tagging process with nltk‘s pos_tag() function! The function takes one argument, a list of words in the order they appear in a sentence, and returns a list of tuples, where the first entry in the tuple is a word and the second is the part-of-speech tag.
            https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html

            Example:
                import nltk
                from nltk import pos_tag
                from word_tokenized_oz import word_tokenized_oz

                # save and print the sentence stored at index 100 in word_tokenized_oz here
                witches_fate = word_tokenized_oz[100]
                print(witches_fate)

                # create a list to hold part-of-speech tagged sentences here
                pos_tagged_oz = list()

                # create a for loop through each word tokenized sentence in word_tokenized_oz here
                for word_tokenized_sentence in word_tokenized_oz:
                  # part-of-speech tag each sentence and append to pos_tagged_oz here
                  pos_tagged_oz.append(pos_tag(word_tokenized_sentence))

                # store and print the part-of-speech tagged sentence at index 100 in witches_fate_pos here
                witches_fate_pos = pos_tagged_oz[100]
                print(witches_fate_pos)
        
        Introduction to Chunking
            Given your part-of-speech tagged text, you can now use regular expressions to find patterns in sentence structure that give insight into the meaning of a text. This technique of grouping words by their part-of-speech tag is called chunking.
            With chunking in nltk, you can define a pattern of parts-of-speech tags using a modified notation of regular expressions. You can then find non-overlapping matches, or chunks of words, in the part-of-speech tagged sentences of a text.
            The regular expression you build to find chunks is called chunk grammar. A piece of chunk grammar can be written as follows:
                chunk_grammar = "AN: {<JJ><NN>}"
                    AN is a user-defined name for the kind of chunk you are searching for. You can use whatever name makes sense given your chunk grammar. In this case AN stands for adjective-noun
                    A pair of curly braces {} surround the actual chunk grammar
                    <JJ> operates similarly to a regex character class, matching any adjective
                    <NN> matches any noun, singular or plural
                    The chunk grammar above will thus match any adjective that is followed by a noun.
            To use the chunk grammar defined, you must create a nltk RegexpParser object and give it a piece of chunk grammar as an argument.
                chunk_parser = RegexpParser(chunk_grammar)
            You can then use the RegexpParser object’s .parse() method, which takes a list of part-of-speech tagged words as an argument, and identifies where such chunks occur in the sentence!
            Consider the part-of-speech tagged sentence below:
                pos_tagged_sentence = [('where', 'WRB'), ('is', 'VBZ'), ('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN'), ('?', '.')]
            You can chunk the sentence to find any adjectives followed by a noun with the following:
                chunked = chunk_parser.parse(pos_tagged_sentence)
            
            Example:
                from nltk import RegexpParser, Tree
                from pos_tagged_oz import pos_tagged_oz

                # define adjective-noun chunk grammar here
                chunk_grammar = "AN:{<JJ><NN>}"

                # create RegexpParser object here
                chunk_parser = RegexpParser(chunk_grammar)

                # chunk the pos-tagged sentence at index 282 in pos_tagged_oz here
                scaredy_cat = chunk_parser.parse(pos_tagged_oz[282])
                print(scaredy_cat)

                # pretty_print the chunked sentence here
                Tree.fromstring(str(scaredy_cat)).pretty_print()
        
        Chunking Noun Phrases  
            A noun phrase is a phrase that contains a noun and operates, as a unit, as a noun.
            A popular form of noun phrase begins with a determiner DT, which specifies the noun being referenced, followed by any number of adjectives JJ, which describe the noun, and ends with a noun NN.
            Consider the part-of-speech tagged sentence below:
                [('we', 'PRP'), ('are', 'VBP'), ('so', 'RB'), ('grateful', 'JJ'), ('to', 'TO'), ('you', 'PRP'), ('for', 'IN'), ('having', 'VBG'), ('killed', 'VBN'), ('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'), ('of', 'IN'), ('the', 'DT'), ('east', 'NN'), (',', ','), ('and', 'CC'), ('for', 'IN'), ('setting', 'VBG'), ('our', 'PRP$'), ('people', 'NNS'), ('free', 'VBP'), ('from', 'IN'), ('bondage', 'NN'), ('.', '.')]
            Can you spot the three noun phrases of the form described above? They are:
                (('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'))
                (('the', 'DT'), ('east', 'NN'))
                (('bondage', 'NN'))
            With the help of a regular expression defined chunk grammar, you can easily find all the non-overlapping noun phrases in a piece of text! Just like in normal regular expressions, you can use quantifiers to indicate how many of each part of speech you want to match.
            The chunk grammar for a noun phrase can be written as follows:
                chunk_grammar = "NP: {<DT>?<JJ>*<NN>}"
                    NP is the user-defined name of the chunk you are searching for. In this case NP stands for noun phrase
                    <DT> matches any determiner
                    ? is an optional quantifier, matching either 0 or 1 determiners
                    <JJ> matches any adjective
                    * is the Kleene star quantifier, matching 0 or more occurrences of an adjective
                    <NN> matches any noun, singular or plural
            By finding all the NP-chunks in a text, you can perform a frequency analysis and identify important, recurring noun phrases. You can also use these NP-chunks as pseudo-topics and tag articles and documents by their highest count NP-chunks! Or perhaps your analysis has you looking at the adjective choices an author makes for different nouns.
            It is ultimately up to you, with your knowledge of the text you are working with, to interpret the meaning and use-case of the NP-chunks and their frequency of occurrence.

            Example:
                from nltk import RegexpParser
                from pos_tagged_oz import pos_tagged_oz
                from np_chunk_counter import np_chunk_counter

                # define noun-phrase chunk grammar here
                chunk_grammar = "NP: {<DT>?<JJ>*<NN>}"

                # create RegexpParser object here
                chunk_parser = RegexpParser(chunk_grammar)

                # create a list to hold noun-phrase chunked sentences
                np_chunked_oz = list()

                # create a for loop through each pos-tagged sentence in pos_tagged_oz here
                for pos_tagged_sentence in pos_tagged_oz:
                  # chunk each sentence and append to np_chunked_oz here
                  np_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))

                # store and print the most common np-chunks here
                most_common_np_chunks = np_chunk_counter(np_chunked_oz)
                print(most_common_np_chunks)
        
        Chunking Verb Phrases
            A verb phrase is a phrase that contains a verb and its complements, objects, or modifiers.
            Verb phrases can take a variety of structures, and here you will consider two. The first structure begins with a verb VB of any tense, followed by a noun phrase, and ends with an optional adverb RB of any form. The second structure switches the order of the verb and the noun phrase, but also ends with an optional adverb.
            Both structures are considered because verb phrases of each form are essentially the same in meaning. For example, consider the part-of-speech tagged verb phrases given below:
                (('said', 'VBD'), ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN'))
                ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), (('said', 'VBD'),
            The chunk grammar to find the first form of verb phrase is given below:
                chunk_grammar = "VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}"
                    VP is the user-defined name of the chunk you are searching for. In this case VP stands for verb phrase
                    <VB.*> matches any verb using the . as a wildcard and the * quantifier to match 0 or more occurrences of any character. This ensures matching verbs of any tense (ex. VB for present tense, VBD for past tense, or VBN for past participle)
                    <DT>?<JJ>*<NN> matches any noun phrase
                    <RB.?> matches any adverb using the . as a wildcard and the optional quantifier to match 0 or 1 occurrence of any character. This ensures matching any form of adverb (regular RB, comparative RBR, or superlative RBS)
                    ? is an optional quantifier, matching either 0 or 1 adverbs
            The chunk grammar for the second form of verb phrase is given below:
                chunk_grammar = "VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}"
            Just like with NP-chunks, you can find all the VP-chunks in a text and perform a frequency analysis to identify important, recurring verb phrases. These verb phrases can give insight into what kind of action different characters take or how the actions that characters take are described by the author.
            Once again, this is the part of the analysis where you get to be creative and use your own knowledge about the text you are working with to find interesting insights!

            Example:
                from nltk import RegexpParser
                from pos_tagged_oz import pos_tagged_oz
                from vp_chunk_counter import vp_chunk_counter

                # define verb phrase chunk grammar here
                chunk_grammar = "VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}"
                #chunk_grammar = "VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}"

                # create RegexpParser object here
                chunk_parser = RegexpParser(chunk_grammar)

                # create a list to hold verb-phrase chunked sentences
                vp_chunked_oz = list()

                # create for loop through each pos-tagged sentence in pos_tagged_oz here
                for pos_tagged_sentence in pos_tagged_oz:
                  # chunk each sentence and append to vp_chunked_oz here
                  vp_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))

                # store and print the most common vp-chunks here
                most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)
                print(most_common_vp_chunks)
        
        Chunk Filtering
            Chunk filtering lets you define what parts of speech you do not want in a chunk and remove them.
            A popular method for performing chunk filtering is to chunk an entire sentence together and then indicate which parts of speech are to be filtered out. If the filtered parts of speech are in the middle of a chunk, it will split the chunk into two separate chunks! The chunk grammar you can use to perform chunk filtering is given below:
                chunk_grammar = """NP: {<.*>+}
                                        }<VB.?|IN>+{"""
                    NP is the user-defined name of the chunk you are searching for. In this case NP stands for noun phrase
                    The brackets {} indicate what parts of speech you are chunking. <.*>+ matches every part of speech in the sentence
                    The inverted brackets }{ indicate which parts of speech you want to filter from the chunk. <VB.?|IN>+ will filter out any verbs or prepositions
            Chunk filtering provides an alternate way for you to search through a text and find the chunks of information useful for your analysis!

            Example:
                from nltk import RegexpParser, Tree
                from pos_tagged_oz import pos_tagged_oz

                # define chunk grammar to chunk an entire sentence together
                grammar = "Chunk: {<.*>+}"

                # create RegexpParser object
                parser = RegexpParser(grammar)

                # chunk the pos-tagged sentence at index 230 in pos_tagged_oz
                chunked_dancers = parser.parse(pos_tagged_oz[230])
                print(chunked_dancers)

                # define noun phrase chunk grammar using chunk filtering here
                chunk_grammar = """NP: {<.*>+}
                                       }<VB.?|IN>+{"""

                # create RegexpParser object here
                chunk_parser = RegexpParser(chunk_grammar)

                # chunk and filter the pos-tagged sentence at index 230 in pos_tagged_oz here
                filtered_dancers = chunk_parser.parse(pos_tagged_oz[230])
                print(filtered_dancers)

                # pretty_print the chunked and filtered sentence here
                Tree.fromstring(str(filtered_dancers)).pretty_print()

    > Bag-of-Words Language Model
        The bag-of-words language model is a simple-yet-powerful tool to have up your sleeve when working on natural language processing (NLP). The model has many, many use cases including:
            - determining topics in a song
            - filtering spam from your inbox
            - finding out if a tweet has positive or negative sentiment
            - creating word clouds
        
        Example (a spam filter using bag-of-words): 
            from spam_data import training_spam_docs, training_doc_tokens, training_labels
            from sklearn.naive_bayes import MultinomialNB
            from preprocessing import preprocess_text

            # Add your email text to test_text between the triple quotes:
            test_text = """
            Your email text here.       // Buy a brand new Rolex dive watch for $1,000
            """
            test_tokens = preprocess_text(test_text)

            def create_features_dictionary(document_tokens):
              features_dictionary = {}
              index = 0
              for token in document_tokens:
                if token not in features_dictionary:
                  features_dictionary[token] = index
                  index += 1
              return features_dictionary

            def tokens_to_bow_vector(document_tokens, features_dictionary):
              bow_vector = [0] * len(features_dictionary)
              for token in document_tokens:
                if token in features_dictionary:
                  feature_index = features_dictionary[token]
                  bow_vector[feature_index] += 1
              return bow_vector

            bow_sms_dictionary = create_features_dictionary(training_doc_tokens)
            training_vectors = [tokens_to_bow_vector(training_doc, bow_sms_dictionary) for training_doc in training_spam_docs]
            test_vectors = [tokens_to_bow_vector(test_tokens, bow_sms_dictionary)]

            spam_classifier = MultinomialNB()
            spam_classifier.fit(training_vectors, training_labels)

            predictions = spam_classifier.predict(test_vectors)

            print("Looks like a normal email!" if predictions[0] == 0 else "You've got spam!")

        Bag-of-words (BoW) is a statistical language model based on word count. 
        Statistical language model is a way for computers to make sense of language based on probability. For example, let’s say we have the text:
        “Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish?”
        A statistical language model focused on the starting letter for words might take this text and predict that words are most likely to start with the letter “f” because 11 out of 15 words begin that way. A different statistical model that pays attention to word order might tell us that the word “fish” tends to follow the word “fantastic.”
        Bag-of-words does not give a flying fish about word starts or word order though; its sole concern is word count — how many times each word appears in a document.
        If you’re already familiar with statistical language models, you may also have heard BoW referred to as the unigram model. It’s technically a special case of another statistical model, the n-gram model, with n (the number of words in a sequence) set to 1.

        BoW Dictionaries
            One of the most common ways to implement the BoW model in Python is as a dictionary with each key set to a word and each value set to the number of times that word appears. 
            The words from the sentence go into the bag-of-words and come out as a dictionary of words with their corresponding counts. For statistical models, we call the text that we use to build the model our training data. Usually, we need to prepare our text data by breaking it up into documents (shorter strings of text, generally sentences).
            Let’s build a function that converts a given training text into a bag-of-words!
                from preprocessing import preprocess_text
                # Define text_to_bow() below:
                def text_to_bow(some_text):
                  bow_dictionary = {}
                  tokens = preprocess_text(some_text)
                  for token in tokens:
                    if token in bow_dictionary:
                      bow_dictionary[token] += 1
                    else:
                      bow_dictionary[token] = 1
                  return bow_dictionary
                print(text_to_bow("I love fantastic flying fish. These flying fish are just ok, so maybe I will find another few fantastic fish..."))
            
        Introducing BoW Vectors
            A feature vector is a numeric representation of an item’s important features. Each feature has its own column. If the feature exists for the item, you could represent that with a 1. If the feature does not exist for that item, you could represent that with a 0. 
            Turning text into a BoW vector is known as feature extraction or vectorization.
            But how do we know which vector index corresponds to which word? When building BoW vectors, we generally create a features dictionary of all vocabulary in our training data (usually several documents) mapped to indices.
            For example, with “Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish?” our dictionary might be:
                {'five': 0, 'fantastic': 1, 'fish': 2, 'fly': 3, 'off': 4, 'to': 5, 'find': 6, 'faraway': 7, 'function': 8, 'maybe': 9, 'another': 10}
            Using this dictionary, we can convert new documents into vectors using a vectorization function. For example, we can take a brand new sentence “Another five fish find another faraway fish.” — test data — and convert it to a vector that looks like:
                [1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2]
            The word ‘another’ appeared twice in the test data. If we look at the feature dictionary for ‘another’, we find that its index is 10. So when we go back and look at our vector, we’d expect the number at index 10 to be 2.
        
        Building a Features Dictionary
            from preprocessing import preprocess_text
            # Define create_features_dictionary() below:
            def create_features_dictionary(documents):
              features_dictionary = {}
              merged = " ".join(documents)
              tokens = preprocess_text(merged)
              index = 0
              for token in tokens:
                if token not in features_dictionary:
                  features_dictionary[token] = index
                  index += 1
              return features_dictionary, tokens

            training_documents = ["Five fantastic fish flew off to find faraway functions.", "Maybe find another five fantastic fish?", "Find my fish with a function please!"]

            print(create_features_dictionary(training_documents)[0])

        Building a BoW Vector
            from preprocessing import preprocess_text
            # Define text_to_bow_vector() below:
            def text_to_bow_vector(some_text, features_dictionary):
              bow_vector = [0] * len(features_dictionary)
              tokens = preprocess_text(some_text)
              for token in tokens:
                feature_index = features_dictionary[token]
                bow_vector[feature_index] += 1
              return bow_vector, tokens

            features_dictionary = {'function': 8, 'please': 14, 'find': 6, 'five': 0, 'with': 12, 'fantastic': 1, 'my': 11, 'another': 10, 'a': 13, 'maybe': 9, 'to': 5, 'off': 4, 'faraway': 7, 'fish': 2, 'fly': 3}

            text = "Another five fish find another faraway fish."
            print(text_to_bow_vector(text, features_dictionary)[0])

        Example (It’s time to put create_features_dictionary() and tokens_to_bow_vector() together and use them in a spam filter we created that uses a Naive Bayes classifier.):
            from spam_data import training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, training_docs, test_docs
            from sklearn.naive_bayes import MultinomialNB

            def create_features_dictionary(document_tokens):
              features_dictionary = {}
              index = 0
              for token in document_tokens:
                if token not in features_dictionary:
                  features_dictionary[token] = index
                  index += 1
              return features_dictionary

            def tokens_to_bow_vector(document_tokens, features_dictionary):
              bow_vector = [0] * len(features_dictionary)
              for token in document_tokens:
                if token in features_dictionary:
                  feature_index = features_dictionary[token]
                  bow_vector[feature_index] += 1
              return bow_vector

            # Define bow_sms_dictionary:
            bow_sms_dictionary = create_features_dictionary(training_doc_tokens)

            # Define training_vectors:
            training_vectors = [tokens_to_bow_vector(training_doc, bow_sms_dictionary) for training_doc in training_spam_docs]

            # Define test_vectors:
            test_vectors = [tokens_to_bow_vector(test_doc, bow_sms_dictionary) for test_doc in test_spam_docs]


            spam_classifier = MultinomialNB()

            def spam_or_not(label):
              return "spam" if label else "not spam"

            # Uncomment the code below when you're done:
            spam_classifier.fit(training_vectors, training_labels)

            predictions = spam_classifier.score(test_vectors, test_labels)

            print("The predictions for the test data were {0}% accurate.\n\nFor example, '{1}' was classified as {2}.\n\nMeanwhile, '{3}' was classified as {4}.".format(predictions * 100, test_docs[0], spam_or_not(test_labels[0]), test_docs[10], spam_or_not(test_labels[10])))

        Spam A Lot No More
            For text_to_bow(), you can approximate the functionality with the collections module’s Counter() function:
                from collections import Counter
                tokens = ['another', 'five', 'fish', 'find', 'another', 'faraway', 'fish']
                print(Counter(tokens))
                # Counter({'fish': 2, 'another': 2, 'find': 1, 'five': 1, 'faraway': 1})
            For vectorization, you can use CountVectorizer from the machine learning library scikit-learn. You can use fit() to train the features dictionary and then transform() to transform text into a vector:
                from sklearn.feature_extraction.text import CountVectorizer
                training_documents = ["Five fantastic fish flew off to find faraway functions.", "Maybe find another five fantastic fish?", "Find my fish with a function please!"]
                test_text = ["Another five fish find another faraway fish."]
                bow_vectorizer = CountVectorizer()
                bow_vectorizer.fit(training_documents)
                bow_vector = bow_vectorizer.transform(test_text)
                print(bow_vector.toarray())
                # [[2 0 1 1 2 1 0 0 0 0 0 0 0 0 0]]
        
        BoW Wow
            Overfitting (adapting a model too strongly to training data, akin to our highly tailored shirt) is a common problem for statistical language models. While BoW still suffers from overfitting in terms of vocabulary, it overfits less than other statistical models, allowing for more flexibility in grammar and word choice.
            The combination of low data sparsity and less overfitting makes the bag-of-words model more reliable with smaller training data sets than other statistical models.
        
        BoW Ow
            BoW is NOT a great primary model for text prediction. If that sort of “sentence” isn’t your bag, it’s because bag-of-words has high perplexity, meaning that it’s not a very accurate model for language prediction. The probability of the following word is always just the most frequently used words.
            If your BoW model finds “good” frequently occurring in a text sample, you might assume there’s a positive sentiment being communicated in that text… but if you look at the original text you may find that in fact every “good” was preceded by a “not.”
            There are several ways that NLP developers have tackled this issue. A common approach is through language smoothing in which some probability is siphoned from the known words and given to unknown words.
        
            Example:
                import nltk, re, random
                from nltk.tokenize import word_tokenize
                from collections import defaultdict, deque, Counter
                from document import oscar_wilde_thoughts

                # Change sequence_length:
                sequence_length = 1

                class MarkovChain:
                  def __init__(self):
                    self.lookup_dict = defaultdict(list)
                    self.most_common = []
                    self._seeded = False
                    self.__seed_me()

                  def __seed_me(self, rand_seed=None):
                    if self._seeded is not True:
                      try:
                        if rand_seed is not None:
                          random.seed(rand_seed)
                        else:
                          random.seed()
                        self._seeded = True
                      except NotImplementedError:
                        self._seeded = False

                  def add_document(self, str):
                    preprocessed_list = self._preprocess(str)
                    self.most_common = Counter(preprocessed_list).most_common(20)
                    pairs = self.__generate_tuple_keys(preprocessed_list)
                    for pair in pairs:
                      self.lookup_dict[pair[0]].append(pair[1])

                  def _preprocess(self, str):
                    cleaned = re.sub(r'\W+', ' ', str).lower()
                    tokenized = word_tokenize(cleaned)
                    return tokenized

                  def __generate_tuple_keys(self, data):
                    if len(data) < sequence_length:
                      return

                    for i in range(len(data) - 1):
                      yield [ data[i], data[i + 1] ]

                  def generate_text(self, max_length=50):
                    context = deque()
                    output = []
                    if len(self.lookup_dict) > 0:
                      self.__seed_me(rand_seed=len(self.lookup_dict))
                      chain_head = [list(self.lookup_dict)[0]]
                      context.extend(chain_head)
                      if sequence_length > 1:
                        while len(output) < (max_length - 1):
                          next_choices = self.lookup_dict[context[-1]]
                          if len(next_choices) > 0:
                            next_word = random.choice(next_choices)
                            context.append(next_word)
                            output.append(context.popleft())
                          else:
                            break
                        output.extend(list(context))
                      else:
                        while len(output) < (max_length - 1):
                          next_choices = [word[0] for word in self.most_common]
                          next_word = random.choice(next_choices)
                          output.append(next_word)
                    return " ".join(output)

                my_markov = MarkovChain()
                my_markov.add_document(oscar_wilde_thoughts)
                random_oscar_wilde = my_markov.generate_text()
                print(random_oscar_wilde)
        
        
************************************************************************************************
************************************************************************************************
************************************************************************************************

